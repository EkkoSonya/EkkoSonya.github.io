const L=Object.entries,st=Object.fromEntries,nt="ENTRIES",T="KEYS",R="VALUES",_="";class k{set;_type;_path;constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===_)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==_).join("")}value(){return E(this._path).node.get(_)}result(){switch(this._type){case R:return this.value();case T:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],ot=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return W(e,t,s,n,i,1,o,""),n},W=(e,t,s,n,o,u,i,r)=>{const d=u*i;t:for(const c of e.keys())if(c===_){const a=o[d-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let h=0;h<c.length;++h,++a){const g=c[h],m=i*a,p=m-i;let l=o[m];const f=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let F=f;F<y;++F){const D=g!==t[F],w=o[p+F]+ +D,A=o[p+F+1]+1,z=o[m+F]+1,V=o[m+F+1]=Math.min(w,A,z);V<l&&(l=V)}if(l>s)continue t}W(e.get(c),t,s,n,o,a,i,r+c)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=M(n);for(const i of o.keys())if(i!==_&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ut(this._tree,t)}entries(){return new k(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ot(this._tree,t,s)}get(t){const s=I(this._tree,t);return s!==void 0?s.get(_):void 0}has(t){const s=I(this._tree,t);return s!==void 0&&s.has(_)}keys(){return new k(this,T)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,O(this._tree,t).set(_,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);return n.set(_,s(n.get(_))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);let o=n.get(_);return o===void 0&&n.set(_,o=s()),o}values(){return new k(this,R)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==_&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},I=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==_&&t.startsWith(s))return I(e.get(s),t.slice(s.length))},O=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==_&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const d=e.get(u);if(r===u.length)e=d;else{const c=new Map;c.set(u.slice(r),d),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ut=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(_),s.size===0)q(n);else if(s.size===1){const[o,u]=s.entries().next().value;$(n,o,u)}}},q=e=>{if(e.length===0)return;const[t,s]=M(e);if(t.delete(s),t.size===0)q(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==_&&$(e.slice(0,-1),n,o)}},$=(e,t,s)=>{if(e.length===0)return;const[n,o]=M(e);n.set(o+t,s),n.delete(o)},M=e=>e[e.length-1],it=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},rt=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,S="or",N="and",ct="and_not",lt=(e,t)=>{e.includes(t)||e.push(t)},P=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},G=({score:e},{score:t})=>t-e,ht=()=>new Map,b=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},H=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,dt={[S]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),P(n.terms,u)}}return e},[N]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);P(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[ct]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},at=(e,t,s,n,o,u)=>{const{k:i,b:r,d}=u;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/o)))},ft=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},J=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},gt=(e,t,s,n)=>{if(!e._index.has(n)){J(e,s,t,n);return}const o=e._index.fetch(n,ht),u=o.get(t);u==null||u.get(s)==null?J(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},mt={k:1.2,b:.7,d:.5},pt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(rt),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},U={combineWith:S,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:mt},Ft={combineWith:N,prefix:(e,t,s)=>t===s.length-1},_t={batchSize:1e3,batchWait:10},K={minDirtFactor:.1,minDirtCount:20},yt={..._t,...K},X=Symbol("*"),At=(e,t)=>{const s=new Map,n={...e._options.searchOptions,...t};for(const[o,u]of e._documentIds){const i=n.boostDocument?n.boostDocument(u,"",e._storedFields.get(o)):1;s.set(o,{score:i,terms:[],match:{}})}return s},Y=(e,t=S)=>{if(e.length===0)return new Map;const s=t.toLowerCase(),n=dt[s];if(!n)throw new Error(`Invalid combination operator: ${t}`);return e.reduce(n)||new Map},B=(e,t,s,n,o,u,i,r,d=new Map)=>{if(o==null)return d;for(const c of Object.keys(u)){const a=u[c],h=e._fieldIds[c],g=o.get(h);if(g==null)continue;let m=g.size;const p=e._avgFieldLength[h];for(const l of g.keys()){if(!e._documentIds.has(l)){gt(e,h,l,s),m-=1;continue}const f=i?i(e._documentIds.get(l),s,e._storedFields.get(l)):1;if(!f)continue;const y=g.get(l),F=e._fieldLength.get(l)[h],D=at(y,m,e._documentCount,F,p,r),w=n*a*f*D,A=d.get(l);if(A){A.score+=w,lt(A.terms,t);const z=H(A.match,s);z?z.push(c):A.match[s]=[c]}else d.set(l,{score:w,terms:[t],match:{[s]:[c]}})}}return d},Ct=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((l,f)=>({...l,[f]:H(n.boost,f)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:c,prefix:a}={...U.weights,...i},h=e._index.get(t.term),g=B(e,t.term,t.term,1,h,o,u,d);let m,p;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const l=t.fuzzy===!0?.2:t.fuzzy,f=l<1?Math.min(r,Math.round(t.term.length*l)):l;f&&(p=e._index.fuzzyGet(t.term,f))}if(m)for(const[l,f]of m){const y=l.length-t.term.length;if(!y)continue;p?.delete(l);const F=a*l.length/(l.length+.3*y);B(e,t.term,l,F,f,o,u,d,g)}if(p)for(const l of p.keys()){const[f,y]=p.get(l);if(!y)continue;const F=c*l.length/(l.length+y);B(e,t.term,l,F,f,o,u,d,g)}return g},Q=(e,t,s={})=>{if(t===X)return At(e,s);if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(g=>Q(e,g,a));return Y(h,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:d}=i,c=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(ft(i)).map(a=>Ct(e,a,i));return Y(c,i.combineWith)},Z=(e,t,s={})=>{const n=Q(e,t,s),o=[];for(const[u,{score:i,terms:r,match:d}]of n){const c=r.length||1,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(d),queryTerms:r,match:d};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return t===X&&s.boostDocument==null&&e._options.searchOptions.boostDocument==null||o.sort(G),o},Et=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Z(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=u,d.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:d}]of n)o.push({suggestion:u,terms:r,score:i/d});return o.sort(G),o};class wt{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?yt:t.autoVacuum;this._options={...pt,...t,autoVacuum:s,searchOptions:{...U,...t.searchOptions||{}},autoSuggestOptions:{...Ft,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=K,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new wt(a);h._documentCount=t,h._nextId=s,h._documentIds=b(n),h._idToShortId=new Map,h._fieldIds=o,h._fieldLength=b(u),h._avgFieldLength=i,h._storedFields=b(r),h._dirtCount=d||0,h._index=new C;for(const[g,m]of h._documentIds)h._idToShortId.set(m,g);for(const[g,m]of e){const p=new Map;for(const l of Object.keys(m)){let f=m[l];c===1&&(f=f.ds),p.set(parseInt(l,10),b(f))}h._index.set(g,p)}return h},j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let h="";i===0?h=c.length>20?`… ${c.slice(-20)}`:c:a?h=c.length+i>100?`${c.slice(0,100-i)}… `:c:h=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,h&&o.push(h),i+=h.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let d=s.indexOf(n,u);if(d===-1)return null;for(;d>=0;){const c=d+n.length;if(r(e.slice(u,d)),u=c,i>100)break;d=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},xt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),bt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),tt=(e,t,s={})=>{const n={};return Z(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...s}).forEach(o=>{const{id:u,terms:i,score:r}=o,d=u.includes("@"),c=u.includes("#"),[a,h]=u.split(/[#@]/),g=Number(a),m=i.sort((l,f)=>l.length-f.length).filter((l,f)=>i.slice(f+1).every(y=>!y.includes(l))),{contents:p}=n[g]??={title:"",contents:[]};if(d)p.push([{type:"customField",id:g,index:h,display:m.map(l=>o.c.map(f=>j(f,l))).flat().filter(l=>l!==null)},r]);else{const l=m.map(f=>j(o.h,f)).filter(f=>f!==null);if(l.length&&p.push([{type:c?"heading":"title",id:g,...c&&{anchor:h},display:l},r]),"t"in o)for(const f of o.t){const y=m.map(F=>j(f,F)).filter(F=>F!==null);y.length&&p.push([{type:"text",id:g,...c&&{anchor:h},display:y},r])}}}),L(n).sort(([,o],[,u])=>"max"==="total"?xt(o,u):bt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=it(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},et=(e,t,s={})=>Et(t,e,{fuzzy:.2,...s}).map(({suggestion:n})=>n),v=st(L(JSON.parse("{\"/\":{\"documentCount\":109,\"nextId\":109,\"documentIds\":{\"0\":\"2\",\"1\":\"2@0\",\"2\":\"2@1\",\"3\":\"3\",\"4\":\"3#主要动机\",\"5\":\"3#主要贡献\",\"6\":\"3#主要内容\",\"7\":\"3#系统结构\",\"8\":\"3#基本设置\",\"9\":\"3#信号模型\",\"10\":\"3#quality-of-experience-model\",\"11\":\"3#优化问题建立\",\"12\":\"3#解决方案\",\"13\":\"3#无人机的3d部署\",\"14\":\"3#无人机的动态移动设计\",\"15\":\"3@0\",\"16\":\"3@1\",\"17\":\"4\",\"18\":\"4#强化学习框架图\",\"19\":\"4#_1-基本概念\",\"20\":\"4#_2-markov-decision-process-mdp\",\"21\":\"4@0\",\"22\":\"4@1\",\"23\":\"5\",\"24\":\"5#核心内容\",\"25\":\"5#_1-state-value\",\"26\":\"5#_1-1\",\"27\":\"5#_1-2-state-value\",\"28\":\"5#_1-3-state-value-与-return-的区别\",\"29\":\"5#_2-bellman-equation\",\"30\":\"5#_2-1-the-mean-of-immediate-rewards\",\"31\":\"5#_2-2-the-mean-of-future-rewards\",\"32\":\"5#_2-3-bellman-equation\",\"33\":\"5#_2-4-bellman-equation-matrix-vector-form\",\"34\":\"5#_3-why-to-slove-state-value\",\"35\":\"5#_4-action-value\",\"36\":\"5#_5-总结\",\"37\":\"5@0\",\"38\":\"5@1\",\"39\":\"6\",\"40\":\"6#_1-optimal-policy\",\"41\":\"6#_2-bellman-optimality-equation-boe\",\"42\":\"6#_2-1-基本形式\",\"43\":\"6#_2-2-如何求解\",\"44\":\"6#_2-2-1-如何处理等式右边的-最优策略\",\"45\":\"6#_2-求解-state-value\",\"46\":\"6@0\",\"47\":\"6@1\",\"48\":\"7\",\"49\":\"7#_1-value-iteration-algorithm\",\"50\":\"7#_1-1-具体步骤\",\"51\":\"7#_1-2-伪代码\",\"52\":\"7#_2-policy-iteration-algorithm\",\"53\":\"7#_2-1-算法描述\",\"54\":\"7#_2-2-伪代码\",\"55\":\"7#_2-3-一些问题\",\"56\":\"7#_3-truncated-policy-iteration-algorithm\",\"57\":\"7#_3-1-value-iteration-与-policy-iteration-算法比较\",\"58\":\"7#_3-2-truncated-policy-iteration-algorithm\",\"59\":\"7#truncated-policy-iteration-algorithm-是否是收敛的\",\"60\":\"7@0\",\"61\":\"7@1\",\"62\":\"8\",\"63\":\"8#_1-mc-basic\",\"64\":\"8#_1-1-算法思路\",\"65\":\"8#_1-2-如何估计\",\"66\":\"8#_1-3-具体算法\",\"67\":\"8#_2-mc-exploring-starts\",\"68\":\"8#_2-1-episode-的高效利用\",\"69\":\"8#_2-2-高效地更新-policy\",\"70\":\"8#_2-3-mc-exploring-starts\",\"71\":\"8#_2-4-exploring-statrts的解释\",\"72\":\"8#_3-mc-eplison-greedy\",\"73\":\"8#_3-1-soft-policy\",\"74\":\"8#_3-2-greedy-policy\",\"75\":\"8#_3-3-greedy-policy-引入-mc-based-算法中\",\"76\":\"8#_3-3-算法流程\",\"77\":\"8@0\",\"78\":\"8@1\",\"79\":\"9\",\"80\":\"9#_1-引言\",\"81\":\"9#_1-1-求均值的方法\",\"82\":\"9#_2-robbins-monto-rm-algorithm\",\"83\":\"9#_2-1-问题引入\",\"84\":\"9#_2-2-算法介绍\",\"85\":\"9#_2-3-收敛性分析\",\"86\":\"9#_2-4-应用于-mean-estimation-中\",\"87\":\"9#_3-stochastic-gradient-descent\",\"88\":\"9#_3-1-问题引入\",\"89\":\"9#_3-2-sgd-分析\",\"90\":\"9#mean-estimation-问题转化\",\"91\":\"9#sgd-正确性和收敛性分析\",\"92\":\"9#_3-3-sgd-另一种问题描述方法-deterministic-formulation\",\"93\":\"9#_3-4-bgd-mbgd-sgdw\",\"94\":\"9@0\",\"95\":\"9@1\",\"96\":\"10\",\"97\":\"10@0\",\"98\":\"10@1\",\"99\":\"11\",\"100\":\"11@0\",\"101\":\"11@1\",\"102\":\"12\",\"103\":\"13\",\"104\":\"14\",\"105\":\"15\",\"106\":\"16\",\"107\":\"17\",\"108\":\"18\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,1],\"1\":[null,null,1],\"2\":[null,null,1],\"3\":[10,6],\"4\":[1,45],\"5\":[1,29],\"6\":[1],\"7\":[1],\"8\":[1,44],\"9\":[1,105],\"10\":[4,82],\"11\":[1,34],\"12\":[1],\"13\":[1,157],\"14\":[1,67],\"15\":[null,null,1],\"16\":[null,null,6],\"17\":[2],\"18\":[1,1],\"19\":[2,118],\"20\":[6,54],\"21\":[null,null,1],\"22\":[null,null,1],\"23\":[2],\"24\":[1,5],\"25\":[3],\"26\":[1,16],\"27\":[4,27],\"28\":[7,36],\"29\":[3,35],\"30\":[8,10],\"31\":[7,23],\"32\":[4,47],\"33\":[8,34],\"34\":[6,34],\"35\":[3,72],\"36\":[2,29],\"37\":[null,null,1],\"38\":[null,null,1],\"39\":[2,16],\"40\":[3,36],\"41\":[6],\"42\":[3,28],\"43\":[2,8],\"44\":[5,21],\"45\":[4,75],\"46\":[null,null,1],\"47\":[null,null,1],\"48\":[4,6],\"49\":[4,18],\"50\":[2,53],\"51\":[3,1],\"52\":[4,3],\"53\":[3,70],\"54\":[2,1],\"55\":[3,44],\"56\":[5,6],\"57\":[7,71],\"58\":[6,23],\"59\":[5,1],\"60\":[null,null,1],\"61\":[null,null,1],\"62\":[6,51],\"63\":[3,16],\"64\":[2,61],\"65\":[3,33],\"66\":[3,49],\"67\":[4,10],\"68\":[4,63],\"69\":[3,52],\"70\":[5,1],\"71\":[4,33],\"72\":[4,9],\"73\":[4,25],\"74\":[4,39],\"75\":[7,12],\"76\":[2,1],\"77\":[null,null,1],\"78\":[null,null,1],\"79\":[2,33],\"80\":[2],\"81\":[2,32],\"82\":[5],\"83\":[3,17],\"84\":[2,31],\"85\":[3,44],\"86\":[6,41],\"87\":[4],\"88\":[3,57],\"89\":[4],\"90\":[3,8],\"91\":[2,67],\"92\":[6,11],\"93\":[5,1],\"94\":[null,null,1],\"95\":[null,null,1],\"96\":[4],\"97\":[null,null,1],\"98\":[null,null,1],\"99\":[1,1],\"100\":[null,null,1],\"101\":[null,null,1],\"102\":[1,3],\"103\":[1],\"104\":[1],\"105\":[1],\"106\":[1],\"107\":[1],\"108\":[1]},\"averageFieldLength\":[3.1547944664290037,33.70694654898481,1],\"storedFields\":{\"0\":{\"h\":\"daily1\",\"t\":[\"a+b=c\"]},\"1\":{\"c\":[\"daily\"]},\"2\":{\"c\":[\"d1\"]},\"3\":{\"h\":\"Reinforcement Learning in Multiple-UAV Networks:Deployment and Movement Design\",\"t\":[\"2019 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY\"]},\"4\":{\"h\":\"主要动机\",\"t\":[\"A novel framework is proposed for quality of experience driven deployment and dynamic movement of multiple unmanned aerial vehicles (UAVs).\",\"过去研究大多没有基于用户的移动(movement of users)来考虑无人机的机动性，更多地是考虑多架无人机的二维部署或单架无人机在地面用户保持静止情况下的部署。\",\"考虑QoE, 而不是仅考虑吞吐量(throughput)，即需要考虑地面不同用户的具体需求。(QoE is invoked for demonstrating the users’ satisfaction, and it is supposed to be considered in UAV-assisted wireless networks)\",\"该文设计的是3D部署，过去研究主要考虑的是2D部署。\"]},\"5\":{\"h\":\"主要贡献\",\"t\":[\"提出了一个理想的由QoE驱动的多无人机协助通信框架。该框架将无人机部署在三维空间内，以 mean opinion score(MOS) 为指标。通过优化无人机的部署和动态移动来解决总用户MOS最大化问题。\",\"提出解决总用户MOS最大化问题的三步骤: \",\"通过GAK-mean算法获得初始单元划分。\",\"设计一种基于 q-learning 的部署方法，在初始时间假设用户处于静止下不断调整 UAVs 3D位置进行优化处理。\",\"设计一种基于 q-learning 的无人机3D动态运动设计算法。\",\"该文基于q-learning的方案来解决无人机的NP-hard 3D部署和移动问题，并与传统的基于遗传的学习算法进行对比。\",\"该文提出的算法具较快的收敛性，与K-means和IGK算法比具有较低的复杂度。\"]},\"6\":{\"h\":\"主要内容\"},\"7\":{\"h\":\"系统结构\"},\"8\":{\"h\":\"基本设置\",\"t\":[\"考虑无人机辅助无线网络的下行链路传输(down-link transmission)，即无人机作为空中基站。\",\"对于指定区域，会将其划分为N个簇，其中用户表示为K=K1​,…,KN​，其中KN​表示划分到集群N的用户，N∈1,2,…,N。\",\"每个用户只能属于一个集群，Kn​∩Kn′​=ϕ,n′=n,\",\"在任意时刻t，同一无人机通过FDMA同时为同一集群中的多个用户提供服务\",\"对于用户kn​∈Kn​，其坐标表示为wkn​​=[xkn​​(t),ykn​​(t)]T∈R2×1\",\"对于无人机n(飞行速度恒定)，其垂直高度表示为hn​(t)∈[hmin​,hmax​],0≤t≤Ts​，其水平坐标表示为qn​(t)=[xn​(t),yn​(t)]T∈R2×1,0≤t≤Ts​\",\"无人机n与用户kn​在时间t的距离表示为:\",\"dkn​​=hn2​(t)+[xn​(t)−xkn​​(t)]2+[yn​(t)−ykn​​(t)]2​\"]},\"9\":{\"h\":\"信号模型\",\"t\":[\"无人机往往有更高的LoS链接概率，该文中表示为:\",\"PLoS​(θkn​​)=b1​(π180​θkn​​−ζ)b2​PNLoS​=1−PLoS​\",\"其中θkn​​(t)=sin−1[dkn​(t)​hn​(t)​]，表示无人机与用户之间的仰角。b1​,b2​,ζ是由环境决定的常数。在实际应用中，为了在LoS信道概率和路径损耗之间取得平衡，需要合理选择无人机n的垂直高度hn​(t)。\",\"在时间t，从无人机n到用户kn​的信道功率增益(the channel power gain)为:\",\"gkn​​(t)=K0​−1dkn​​−α[t](PLos​μLoS​+PNLos​μNLoS​)−1\",\"其中K0​=(c4πfc​​)2，α是表示路径损耗指数(常数)，μLoS​,μNLoS​是表示LoS和NLoS链路的衰减因子，fc​是载波频率，c是光速。\",\"对于无人机n，其可用带宽为Bn​，将其平均分配给其∣Kn​∣个关联用户，其每个用户带宽表示为: Bkn​​=Bn​/Kn​. 该文中不同集群所利用的频谱是不同的，且无人机向关联用户的发射功率是恒定的。 同样，对于无人机的总发射功率也均匀地分配给每个用户，pkn​​=Pmax​/Kn​\",\"由于不同集群的频谱不同，可以减轻无人机对用户接收到的干扰。因此，在时刻t关联到无人机n的地面用户kn​的接受到的信噪比表示为:\",\"Γkn​​(t)=σ2pkn​​gkn​​(t)​\",\"其中σ2=Bkn​​N0​, N0​为用户所在位置的加性高斯白噪声(AWGN)的功率谱密度。\",\"为了满足不同用户传输速率要求，对于用户kn​存在特定的信噪比目标γkn​​, 即Γ≥γkn​​.\",\"由此，存在Lemma1： 为了保证所有用户都能连接到网络，我们对无人机的发射功率有一个约束，可以表示为\",\"Pmax​≥γσ2K0​dkn​​α(t)μNLoS​\",\"根据香农定理: 信道容量C=B∗log(1+NS​)，且传输率永远都不可能超过信道容量C。 因此对于用户kn​的在时刻t的传输速率rkn​​(t)，表示为rkn​​(t)=Bkn​​log2​[1+σ2pkn​​gkn​​(t)​].\",\"Proposition1: 无人机n的高度需满足:\",\"dkn​​(t)sin[180π​(ζ+eM(t))]≤hn​(t)≤(γK0​σ2μLoS​Pmax​​)\",\"其中\",\"M(t)=b2​ln(b1​(μLoS​−μNLoS​)S(t)​−μLoS​−μNLoS​μNLoS​​​S(t)=γK0​σ2dkn​​α(t)Pm​ax​\",\"Proposition1展示了无人机为相关用户提供可靠服务所需的高度的必要条件。 可知，其高度的下界是距离dkn​​(t)的函数；高度的上界是最大发射功率Pmax​的函数。 因此，随着无人机与用户之间距离和发射功率的变化，需要调整相应无人机的高度，以向用户提供可靠的服务。\"]},\"10\":{\"h\":\"Quality-of-Experience Model\",\"t\":[\"由于不同用户对于传输速率的需求是不同的，所以在无人机辅助通信网络中我们需要考虑QoE模型。\",\"在该文中，采用MOS作为用户QoS衡量的标准，具体如下:\",\"MOSkn​​(t)=ζ1​MOSkn​​delay(t)+ζ2​MOSkn​​rate(t)\",\"其中，ζ1​,ζ2​是系数，且ζ1​+ζ2​=1。\",\"根据MOS数值，共划分5个等级: excellent(4.5) very good(2~3.5) fair(1~2) poor(1)。\",\"在该文中考虑的是网页浏览应用传输情况，因此MOSkn​​delay(t)可以忽略，因此，此时的MOS模型定义如下:\",\"MOSkn​​(t)=−C1​ln[d(rkn​​(t))]+C2​\",\"d(rkn​​(t))是与传输速率有关的延迟时间，MOSkn​​(t)为t时刻的MOS评分，取值范围从1−4.5。C1​和C2​是通过分析web浏览应用程序的实验结果确定的常数，分别设为1.120和4.6746。\",\"d(rkn​​(t))=3RTT+rkn​​(t)FS​+L(rkn​​MSS​)+RTT−rkn​​(t)2MSS(2L−1)​\",\"其中，RTT[s]表示round trip time(数据包从发送端-接收端-发送端的时间)，FS[bit]是网页大小，MSS[bit]是最大报文长度，L=min[L1​,L2​]表示 the number of slow start cycles with idle periods。\",\"L1​=log2​(MSSrkn​​RTT​+1)−1,L2​=log2​(2MSSFS​+1)−1.\",\"用户rkn​​在一段时间Ts​内的MOS总和为:\",\"MOSrkn​​​=t=0∑Ts​​MOSkn​​(t)\"]},\"11\":{\"h\":\"优化问题建立\",\"t\":[\"假设功率Q=qn​(t),0≤t≤Ts​, 高度H=hn​(t),0≤t≤Ts​\",\"本文目的是优化无人机在每个时隙的位置，从而最大化所有用户的总MOS值。具体表述如下:\",\"C,Q,Hmax​MOStotal​=∑n=1N​∑kn​=1Kn​​∑t=0Ts​​MOSkn​​(t)s.t.Kn​∩Kn′​=ϕ,n′=n,∀n,hmin​≤hn​(t)≤hmax​,∀t,∀n,Γkn​(t)​≥γkn​​,∀t,∀kn​,∑kn​=1Kn​​pkn​​(t)≤Pmax​,∀t,∀kn​,pkn​(t)​≥0,∀kn​,∀t,​\",\"该优化问题是一个non-convex问题，因为目标函数对于无人机的3D坐标是非凸的。\",\"总用户的MOS取决于无人机的发射功率、数量和位置(水平位置和高度)。\"]},\"12\":{\"h\":\"解决方案\"},\"13\":{\"h\":\"无人机的3D部署\",\"t\":[\"考虑以下场景，将上述优化问题简化:\",\"无人机n以可变高度悬停在用户上方，用户是保持静态的。 每架无人机的带宽和发射功率都均匀分配给每个用户。 因此我们将优化问题简化为区域分割问题。\",\"描述如下: 但即使仅考虑用户聚类，该问题依然是NP-hard问题\",\"C,Q,Hmax​MOStotal​=∑n=1N​∑kn​=1Kn​​MOSkn​​(t)s.t.Kn​∩Kn′​=ϕ,n′=n,∀n,hmin​≤hn​(t)≤hmax​,∀t,∀n,Γkn​(t)​≥γkn​​,∀t,∀kn​,∑kn​=1Kn​​pkn​​(t)≤Pmax​,∀t,∀kn​,pkn​(t)​≥0,∀kn​,∀t,​\",\"无人机-用户关联策略(用户区域划分算法)\",\"采用基于遗传算法的GAK-means算法 由于特定用户的MOS与该用户与无人机之间的距离有关，因此GAK-means可以视为获得无人机部署的低复杂度方案。\",\"根据N个用户，根据遗传算法找到CN​个最优个体作为簇的中心。\",\"将无人机部署在每个中心内，再将用户划分给距离最近的无人机\",\"重复步骤，再找到新的簇的各中心，再根据欧几里得距离重新划分，直到各个簇的成员没有太大变化，划分完毕。\",\"无人机3D部署算法\",\"根据所给定的用户划分情况，目标是获得无人机的最佳3D位置，来最大化MOS总和。 由于GAK-means的优化目标是最小化无人机与对应集群用户的欧氏距离，MOS主要是有关传输速率rkn​​的函数，因此MOS不仅与欧氏距离有关，还与LoS的概率有关。\",\"采用Q-learning算法\",\"智能体(agent): UAVn,n∈N={1,2,…,N}\",\"状态(state): 对于每个智能体，其状态为其3D坐标，定义为ξ=(xUAV​,yUAV​,hUAV​)\",\"状态空间(state space S): 这里采用离散化空间坐标，即xUAV​:{0,1,…,Xd​},yUAV​:0,1,…,Yd​,hUAV​:{hmin​,…,hmax​}，所以状态其实共有(XD​+1)×(Yd​+1)×(hmax​−hmin​+1)个\",\"动作空间(action space): 每次无人机会根据当前状态st​∈S，按照所给定策略J来执行一个动作at​∈A从而获得奖励rt​以及下一个状态st+1​ 该论文中在精度和模型复杂型上作出平衡，共考虑7个方向。 (1,0,0)：右转 (−1,0,0)：左转 (0,1,0)：前进 (0,−1,0)：后退 (0,0,1)：上行 (0,0,−1)：下行 (0,0,0)：静止\",\"状态转换模型: 当执行动作at​时，从状态st​到st+1​，并获得奖励rt​的这一过程可以用条件转移概率p(st+1​,rt​∣st​,at​)来表示。 Q-learning的优化目标是最大化长期收益\",\"Gt​=E[n=0∑∞​βnrt+n​]\",\"奖励(reward): 如果agent在当前时刻t所执行的动作能够提高总MOS，则无人机将获得正奖励。否则，agent将获得负奖励。\",\"xt​=⎩⎨⎧​1,−0.1,−1,​ifMOSnew​>MOSold​ifMOSnew​=MOSold​ifMOSnew​<MOSold​​\",\"具体代码：（策略为贪心策略）\",\"算法1\",\"个人理解：\",\"通过K-means来划分各个无人机所管理的用户簇。无人机的位置初始化也是随机部署的\",\"但每个无人机所管理的用户不同，其目标也应该不一样，不能用同一个Q-table管理，这里是每个无人机都有一张自己的Q-table，来进行迭代？ 还是同一张Q-table，只不过根据区域划分，不同的无人机agent的Q(s,a)的s是有范围的？(个人感觉是这个)\",\"最终输出的结果，应该是无人机最终停的位置即是部署的最佳位置(因为q-learning是优化长期目标)，发现在该位置静止是最优的，表示是最佳部署位置。\",\"最终输出结果，是根据Q-table来找出对应q(s,a)当a为静止时，最大的q(s,a)值，对应s就是UAV的部署位置\"]},\"14\":{\"h\":\"无人机的动态移动设计\",\"t\":[\"考虑用户在每个时隙移动的情况，由于用户在每个时隙都处于漫游状态，因此随着用户位置的变化，每个集群中无人机的最优位置也会发生变化，无人机需要进行移动。\",\"在本文中不考虑用户移动到其他集群的情况 因为在不考虑用户自由穿梭集群的情况，对于动作空间而言，仅需要考虑无人机的7个移动方向即可；但若考虑集群情况，动作空间包含两个部分：选择移动方向和选择关联用户。设无人机总数为N，∣Kn​∣为第n个簇的用户总数，则用户的关联动作数为2N∑n=1N​∣Kn​∣，∑n=1N​∣Kn​∣是总用户数，每个用户都需要判断是否与每个无人机关联，因此是2N 则总动作空间的大小为7+2N∑n=1N​∣Kn​∣会导致动作空间过大，Q-table过大。\",\"1.用户漫游模型 在设计无人机的移动之前，需考虑用户的移动性，这里有多种mobility modles可选择，如a deterministic approach, a hybrid approach, and a random walk model. 在本文中，采用的是the random walk model(Markovian mobility model) 每个用户的移动方向均匀分布在左、右、前、后四个方向。 用户的速度设为[0,cmax​]，其中cmax​表示用户的最大速度。\",\"2.基于q-learning的移动算法 与基于q-learning的部署算法不同的是，在此情况下，状态除了要考虑无人机的3D位置外，还需要考虑所有用户的2D位置。即ξ={xUAV​,yUAV​,hUAV​,xuser​,yuser​}(xuser​,yuser​)由用户的初始位置和运动模型决定，(xUAV​,yUAV​,hUAV​)由无人机的位置和它们在最后时隙采取的动作决定.\",\"训练阶段: \",\"测试阶段:\\n\"]},\"15\":{\"c\":[\"academic\"]},\"16\":{\"c\":[\"UAV\",\"IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY\"]},\"17\":{\"h\":\"RL1 - 基本概念\"},\"18\":{\"h\":\"强化学习框架图\",\"t\":[\"主要框架\"]},\"19\":{\"h\":\"1. 基本概念\",\"t\":[\"State(状态)：The status of the agent with respect to the environment.\",\"State Space(状态空间): 所有状态的集合。S={si​}i=1n​。\",\"Action(动作): 对于每一个状态，都有可选择的动作。\",\"Action space of a state: 对应状态中所有可选择的动作集合。A(si​)={ai​}i=1n​\",\"State transition(状态转换): s1​→a1​s2​。定义了agent与环境的交互行为。\",\"State transition probability: p(s2​∣s1​,a1​)，即状态s1​采用动作a1​转到状态s2​的概率。\",\"Policy π: 指导agent在当前状态下选择哪个动作。\",\"Reward(奖励): 在执行一个动作后获得的一个常数(依赖于当前状态和所采取的动作)。同样可以用条件概率的形式进行描述，如p(r=1∣s1​,a1​)，即在状态s1​下采用动作a1​获得的奖励r=1的概率。\",\"Trajectory：a state-action-reward chain.(可以有限，也可以是无限长的trajectory) s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​. 个人理解，trajectory是在策略给定下，agent可能走出的全部轨迹，并非只是一个单一的轨迹。\",\"Return of a trajectory：将对应的轨迹所获得的所有reward的总和，可以粗步衡量一个策略的好坏。\",\"Discounted return(of a trajectory)：为了应对具有无限步的trajectory的return=∞的情况。 s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​r=1→​a2​​s9​r=1→​a2​​s9​…. 此时该trajectory的return=0+0+0+1+1+⋯=∞。 引入discount rate, γ∈[0,1). 此时对应的discountedrate=0+γ0+γ20+γ31+γ41+⋯=γ31−γ1​ 显然，如果γ接近0，即此时的discounted return越短视，注重近期的reward；γ接近1，更远视，更注重长远的reward。\",\"Episode(trial)：When interacting with the environment following a policy, the agent may stop at some terminal states. The resulting trajectory is called an episode(or a trial)/ 即表示具有终止状态terminal states的trajectory，通常是具有有限步长的trajectory. 同理，这样的任务称为episodic tasks。\",\"continuing tasks：即不具备terminal states的任务，会与环境一直交互下去。 可以通过设置将episodic tasks转换成continuing tasks，如可以在target states中限制action space，控制其一直待在target states中。 Deterministic — Stochastic\"]},\"20\":{\"h\":\"2.Markov decision process(MDP)\",\"t\":[\"关键元素：\",\"Sets： \",\"State：the set of states S\",\"Action：the set of actions A(s) is associate for state s∈S\",\"Reward：the set of rewards R(s,a).\",\"Probability distribution： \",\"State transition probability p(s′∣s,a): 表示在状态s下采取动作a，转换到状态s′的概率。\",\"Reward probability p(r∣s,a): 表示在状态s下采取动作a，获得reward r 的概率。\",\"Policy：at state s, the probability to choose action a is π(a∣s). 表示在各状态执行各动作的概率。\",\"Markov property：即无记忆的特性。 p(st+1​∣at+1​,st​,…,a1​,s0​)=p(st+1​∣at+1​,st​)r(st+1​∣at+1​,st​,…,a1​,s0​)=p(rt+1​∣at+1​,st​)\",\"Markov process：在policy是确定的情况下，MDP就变为MP。\"]},\"21\":{\"c\":[\"academic\"]},\"22\":{\"c\":[\"强化学习\"]},\"23\":{\"h\":\"RL2 - 贝尔曼公式\"},\"24\":{\"h\":\"核心内容\",\"t\":[\"state value\",\"the Bellman equation\"]},\"25\":{\"h\":\"1.State value\"},\"26\":{\"h\":\"1.1\",\"t\":[\"引入随机变量后对应的discounted return的描述。 即一个trajectory下的discounted return。 由此可以推导出一个多步的trajectory:\",\"St​→At​Rt+1​,St+1​→At+1​Rt+2​,St+2​→At+2​Rt+3​,St+2​→At+3​…\",\"对应的discounted return为：Gt​=Rt+1​+γRt+2​+γ2Rt+3​+…\",\"γ 为discounted rate\",\"Gt​也是一个随机变量\"]},\"27\":{\"h\":\"1.2 State value\",\"t\":[\"State value 是 Gt​ 的期望, 也称为 state value function 表示为 The expection(expected value or mean) of Gt​:\",\"vπ​(s)=E[Gt​∣St​=s]\",\"是一个有关状态s的函数.\",\"vπ​(s) 是基于一个给定策略 π , 对于不同的策略，所得到的 state value 是不同的.\",\"state value 可以用来衡量一个状态的价值.\"]},\"28\":{\"h\":\"1.3 State value 与 return 的区别\",\"t\":[\"Return 是针对一条trajectory所求的，而 State value 则是对多个 trajectory 求 return 再求平均值。 The state value is the mean of all possible returns that can be obtained starting from a state. 只有当所有东西都是确定性的(π(a∣s),p(r∣s,a),p(s′∣s,a))，state value 与 return 是一致的.\"]},\"29\":{\"h\":\"2. Bellman equation\",\"t\":[\"用来描述所有状态的state value的关系. 根据一个 random trajectory:\",\"St​→At​Rt+1​,St+1​→At+1​Rt+2​,St+2​→At+2​Rt+3​,St+2​→At+3​…\",\"对应的 discounted return Gt​ 为:\",\"Gt​​=Rt+1​+γRt+2​+γ2Rt+3​+…=Rt+1​+γ(Rt+2​+γRt+3​+…)=Rt+1​+γGt+1​​\",\"因此，对应的 state value 为:\",\"vπ​(s)​=E[Gt​∣St​=s]=E[Rt+1​+γGt+1​∣St​=s]=E[Rt+1​∣St​=s]+γE[Gt+1​∣St​=s]​\",\"需要推导E[Rt+1​∣St​=s]和E[Gt+1​∣St​=s]的计算即可。\"]},\"30\":{\"h\":\"2.1 The mean of immediate rewards:\",\"t\":[\"E[Rt+1​∣St​=s]​=a∑​π(a∣s)E[Rt+1​∣St​=s,At​=a]=a∑​π(a∣s)r∑​p(r∣s,a)r​\"]},\"31\":{\"h\":\"2.2 The mean of future rewards:\",\"t\":[\"E[Gt+1​∣St​=s]​=s′∑​E[Gt+1​∣St​=s,St+1​=s′]=s′∑​E[Gt+1​∣St+1​=s′](无记忆性)=s′∑​vπ​(s′)p(s′∣s)=s′∑​vπ​(s′)a∑​p(s′∣s,a)π(a∣s)​\",\"个人推导：\",\"E[Gt+1​∣St​=s]​=a∑​π(a∣s)E[Gt+1​∣St​=s,At​=a]=a∑​π(a∣s)s′∑​E[Gt+1​∣St​=s,At​=a,St+1​=s′]=a∑​π(a∣s)s′∑​p(s′∣s,a)E[Gt+1​∣St+1​=s′]=a∑​π(a∣s)s′∑​p(s′∣s,a)vπ​(s′)​\"]},\"32\":{\"h\":\"2.3 Bellman equation\",\"t\":[\"vπ​(s)​=E[Rt+1​∣St​=s]+γE[Gt+1​∣St​=s],=mean of immediate rewards a∑​π(a∣s)r∑​p(r∣s,a)r​​+mean of future rewards γa∑​π(a∣s)s′∑​p(s′∣s,a)vπ​(s′)​​,=a∑​π(a∣s)[r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)],∀s∈S.​\",\"该式子针对状态空间中的所有状态均成立.\",\"通过 Bootstrapping , 可以求解 state value.\",\"π(a∣s) 表示一个给定的策略. 求解Bellman equation 称为策略评估(Policy evaluation).\",\"p(r∣s,a),p(s′∣s,a) 是由环境决定的(dynamic model|environment model). 后续可能是未知的(model-free)，需要通过采样解决.\"]},\"33\":{\"h\":\"2.4 Bellman equation (Matrix-vector form)\",\"t\":[\" 此时,对于所有状态s，对应的 Bellman equation 为\",\"vπ​(s)=rπ​(s)+γs′∑​pπ​(s′∣s)vπ​(s′)​\",\"将所有状态的 Bellman equation 整合，重新修改为 matrix-vector form.\",\"vπ​=rπ​+γPπ​vπ​​\",\"其中,\",\"vπ​=[vπ​(s1​),…,vπ​(sn​)]T∈Rn\",\"rπ​=[rπ​(s1​),…,rπ​(sn​)]T∈Rn\",\"Pπ​∈Rn×n, where [Pπ​]ij​=pπ​(sj​∣si​), 表示状态转移矩阵.\"]},\"34\":{\"h\":\"3. Why to slove state value\",\"t\":[\"为了进行 Policy evaluation, 即对于给定策略，求出其对应状态的 state value 的过程。\",\"通过 Bellman euqation 进行求解。\",\"The closed-form solution(不常用):\",\"vπ​=(I−γpπ​)−1rπ​​\",\"An iterative solution(一种迭代策略):\",\"vk+1​=rπ​+γPπ​vk​​\",\"可以最开始均初始化为 0 , 然后进行不断迭代，可以得到一个序列v0​,v1​,v2​,…. 最终可以证明：vk​→vπ​=(I−γpπ​)−1rπ​,k→∞\"]},\"35\":{\"h\":\"4. Action value\",\"t\":[\"State value: agent从一个状态出发可以得到的平均return. the average return the agent can get starting from a state\",\"Action value: agent从一个状态出发，采取一个指定的action可以得到的平均return。 the average return the agent can get starting from a state and taking an action.\",\"通过求解 action value 我们可以分析出在该状态下采取哪个 action 收益最大. Action value 定义:\",\"qπ​(s,a)=E[Gt​∣St​=s,At​=a]​\",\"同样地，qπ​(s,a)是依赖于策略π的，并且与状态 s 和动作 a 有关.\",\"vπ​(s)E[Gt​∣St​=s]​​=a∑​qπ​(s,a)E[Gt​∣St​=s,At​=a]​​π(a∣s)\",\"因此，vπ​(s)=∑a​qπ​(s,a)π(a∣s) 由于,\",\"vπ​(s)=a∑​π(a∣s)[r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)]​\",\"所以，qπ​(s,a)=∑r​p(r∣s,a)r+γ∑s′​p(s′∣s,a)vπ​(s′)\",\"实际意义是：在当前状态s下采取动作 a 所获得的均值，加上 γ 的转到下一个状态的 state value 加权均值。\",\"引入 action value 后，对于 state value 实际意义的解释：在当前状态s下，根据策略π, 所有可能动作的 action value 的加权均值。\",\"state value 和 action value 可以互相转化。\"]},\"36\":{\"h\":\"5. 总结\",\"t\":[\"State value: vπ​(s)=E[Gt​∣St​=s]\",\"Action value: qπ​(s,a)=E[Gt​∣St​=s,At​=a]\",\"State value 是 action value 的根据策略π加权平均，即vπ​(s)=∑a​π(a∣s)q(s,a)\",\"The Bellman equation (elementwise form and matrix-vector form)\",\"求解 the Bellman equation (2种方法)\"]},\"37\":{\"c\":[\"academic\"]},\"38\":{\"c\":[\"强化学习\"]},\"39\":{\"h\":\"RL3 - 贝尔曼最优公式\",\"t\":[\"Core concepts: optimal state value and optimal policy\",\"A fundamental tool: the Bellman optimality equation (BOE)\"]},\"40\":{\"h\":\"1. Optimal policy\",\"t\":[\"最优策略的定义: A policy π∗ is optimal if π∗(s)≥vπ​(s) for all s and for any other policy π. 需要确定几件事:\",\"最优策略是否存在 存在，根据 the contraction mapping Theorem.\",\"最优策略是否唯一 唯一，根据 the contraction mapping Theorem.\",\"最优策略是 stochastic 还是 deterministic deterministic 且 greedy\",\"如何得到最优策略 选取状态中最大的 action value 作为下一步的 action\"]},\"41\":{\"h\":\"2. Bellman optimality equation (BOE)\"},\"42\":{\"h\":\"2.1 基本形式\",\"t\":[\"对于贝尔曼最优公式而言，其策略π表示的是最优策略，除了需要求解 state value 外，还需要求解最优策略π.elementwise form:\",\"vπ​(s)​=πmax​a∑​π(a∣s)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)),∀s∈S=πmax​a∑​π(a∣s)q(a,s),∀s∈S​\",\"matrix-vector foem:\",\"v=πmax​(rπ​+γPπ​v)​\"]},\"43\":{\"h\":\"2.2 如何求解\",\"t\":[\"对于贝尔曼最优公式而言，区别于贝尔曼公式，只是求解各状态的 state value, 我们还需要理解其所描述的最优策略π∗ 具体分两步:\"]},\"44\":{\"h\":\"2.2.1 如何处理等式右边的 (最优策略)\",\"t\":[\"vπ​(s)=maxπ​∑a​π(a∣s)q(s,a), 为了让右边取到最大值的情况，我们只需要在当前状态下，保证选取最大的 action value 即可，对应策略表示为:\",\"π(a∣s)={10​a=a∗a=a∗​\",\"其中a∗表示在该状态下计算出来的最大 action value 对应的动作，即a∗=argmaxa​q(s∣a)\"]},\"45\":{\"h\":\"2. 求解 state value\",\"t\":[\"将 BOE 转换为 v=f(v) 的形式，其中f(v):=maxπ​(rπ​+γPπ​v)f(v)对应一个向量, [f(v)]s​=maxπ​∑a​π(a∣s)q(s∣a),∀s∈S\",\"求解方法：\",\"Fix point: f(x)=x\",\"Contraction mapping(contractive function): ∣∣f(x1​)−f(x2​)∣∣≤γ∣∣x1​−x2​∣∣\",\"由此可以根据Contraction Mapping Theorem: For any equation that has the form of x=f(x), if f is a contraction mapping, then\",\"Existence: 存在不动点x∗，满足f(x∗)=x∗\",\"Uniqueness: 不动点x∗是唯一的\",\"Algorithm: Consider a sequence xk​ where xk+1​=f(xk​), then xk​→x∗ as k→∞. Moreover, the convergence rate is exponentially fast.\",\"因此，可以通过Contraction Mapping Theorem来求解贝尔曼最优公式，因为其满足该理论，即f(v)是一个contraction mapping。\"]},\"46\":{\"c\":[\"academic\"]},\"47\":{\"c\":[\"强化学习\"]},\"48\":{\"h\":\"RL4 - 值迭代和策略迭代(动态规划)\",\"t\":[\"贝尔曼最优公式:\",\"v=f(v)=πmax​(rπ​+γPπ​v)\"]},\"49\":{\"h\":\"1. Value iteration algorithm\",\"t\":[\"根据 chapter 3 中涉及的 contraction mapping theorem, 我们可以通过对应的迭代算法来求解贝尔曼最优公式\",\"vk+1​=f(vk​)=πmax​(rπ​+γPπ​vk​),k=1,2,3…\",\"这种迭代算法称为 value iteration.\"]},\"50\":{\"h\":\"1.1 具体步骤\",\"t\":[\"共分为 2 步：\",\"Policy update 这步是更新策略π，即求解右边的式子，πk+1​=argmaxπ​(rπ​+γPπ​vk​), 其中vk​是给定的。 其对应的 elementwise form:\",\"πk+1​(s)=πargmax​a∑​π(a∣s)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)v(s′)),s∈S 由于 p(s′∣s,a),p(r∣s,a),v(s′) 是已知的，显然，这里的最优策略πk+1​是一个 greedy policy，我们只需要挑选在当前迭代下最大的 action value 就好了, 即:\",\"πk+1​(a∣s)={10​a=ak∗​(s)a=ak∗​(s)​ 其中ak∗​(s)=argmaxa​qk​(a,s).\",\"value update 根据 Policy update 的策略πk+1​, 求解下一步的vk+1​, 即\",\"vk+1​=rπk+1​​+γPπk+1​​vk​这里的vk​并不是 state value 由于πk+1​是 greedy 的，对应的vk+1​(s)=maxa​qk​(a,s)\"]},\"51\":{\"h\":\"1.2 伪代码\",\"t\":[\"20240810190018\"]},\"52\":{\"h\":\"2. Policy iteration algorithm\",\"t\":[\"算法迭代示意图:\",\"π0​PE​vπ0​​PI​π1​PE​vπ1​​PI​π2​PE​vπ2​​PI​…\"]},\"53\":{\"h\":\"2.1 算法描述\",\"t\":[\"首先随机设计一个初始的策略π0​\",\"Step 1: policy evaluation (PE) 策略评估 该步骤是用来计算当前策略 πk​ 的 state value. 可以通过 Bellman equation 进行求解，即:\",\"vπk​​=rπk​​+γPπk​​vπk​​\",\"根据对应的 Elementwise form:\",\"vπk​(j+1)​(s)=a∑​πk​(a∣s)(r∑​p(r∣s,a)r+s′∑​p(s′∣s,a)vπk​(j)​(s′)),s∈S\",\"由此进行迭代，直到设置的收敛条件为止，即j→∞ 或者 ∣∣vπk+1​(j+1)​(s)−vπk​(j)​(s)∣∣≤δ.\",\"Step 2: policy improvement (PI) 策略提升 该步骤是根据 PE 所求出的 state value, 根据 action value，来提升当前策略 πk​\",\"πk+1​=πargmax​(rπ​+γPπ​vπk​​)\",\"对应的 Elementwise form:\",\"πk+1​(s)=πargmax​a∑​πk​(a∣s)qπk​​(s,a)(r∑​p(r∣s,a)r+s′∑​p(s′∣s,a)vπk​​(s′))​​,s∈S\",\"这里，显然是可以通过一个 greedy 的策略来进行选择，即:\",\"πk+1​(a∣s)={10​a=ak∗​(s),a=ak∗​(s).​\",\"其中 aK∗​(s)=argmaxa​qπk​​(s,a).\"]},\"54\":{\"h\":\"2.2 伪代码\",\"t\":[\"20240811002219\"]},\"55\":{\"h\":\"2.3 一些问题\",\"t\":[\"在 PE 步骤中，如何通过 Bellman equation 得到 state value vπk​​. 根据 chapter 2 中求解 Bellman equation 的方法 一种是可以直接通过矩阵求逆进行求解，即 vπk​​=(I−γPπk​​)−1rπk​​，实际不常用. 一种是通过迭代算法来求解\",\"vπk​(j+1)​=rπk​​+γPπk​​vπk​(j)​\",\"在 PI 步骤中，如何确保策略 πk+1​ 是优于 πk​的.\",\"为什么这个迭代算法最终可以找到最优策略 每次迭代都会使得策略进行提升，那么\",\"vπ0​​≤vπ1​​≤vπ2​​⋯≤vπk​​≤⋯≤v∗\",\"我们需要保证策略是不断提升，且最终会收敛到最优策略v∗\",\"policy iteration algorithm 与 value iteration algorithm 之间存在什么关系.\"]},\"56\":{\"h\":\"3. Truncated policy iteration algorithm\",\"t\":[\"该算法是 value iteration 以及 policy iteration 一般化的推广\"]},\"57\":{\"h\":\"3.1 value iteration 与 policy iteration 算法比较\",\"t\":[\"Policy iteration: 需要初始化策略π0​, 之后进行迭代\",\"Policy evaluation (PE): 通过 Bellman equation 求解当前策略的 state value.\",\"vπk​​=rπk​​+γPπk​​vπk​​\",\"内嵌迭代算法求解.\",\"Policy improvement (PI): 考虑 greedy 策略求解, 选取当前状态下最大的 action value.\",\"πk+1​=πargmax​(rπ​+γPπ​vπk​​)\",\"Value iteration: 需要初始化猜测的 state value v0​\",\"Policy update (PU): 考虑 greedy 策略求解, 选取当前状态下最大的 action value.\",\"πk+1​=πargmax​(rπ​+γPπ​vk​)\",\"Value update (VU): 进行迭代\",\"vk+1​=rπk+1​​+γPπk+1​​vk​\",\"两个算法迭代过程十分类似: Policy iteration:\",\"π0​PE​vπ0​​PI​π1​PE​vπ1​​PI​π2​PE​vπ2​​PI​…\",\"Value iteration:\",\"u0​PU​π1′​VU​u1​PU​π2′​VU​u2​PU​…\",\"Policy iteration algorithm\",\"Value iteration algorithm\",\"Comments\",\"1) Policy:\",\"π0​\",\"N/A\",\"2) Value:\",\"vπ0​​=rπ0​​+γPπ0​​vπ0​​\",\"v0​:=vπ0​​\",\"对于 policy iteration，vπ0​​是通过迭代算法来求的; 而 value iteration 我们这里强行初始化为vπ0​​，方便后续比较\",\"3) Policy:\",\"π1​=argmaxπ​(rπ​+γPπ​vπ0​​)\",\"π1​=argmaxπ​(rπ​+γPπ​vπ0​​)\",\"在策略更新上，这两个算法是一致的。\",\"4) Value:\",\"vπ1​​=rπ1​​+γPπ1​​vπ1​​\",\"v1​=rπ1​​+γPπ1​​v0​\",\"对于 Policy iteration 而言, 这里需要通过迭代算法来精确求出 vπ1​​; 对于 Value iteration，则只是进行一次带入求解。\",\"5) Policy:\",\"π2​=argmaxπ​(rπ​+γPπ​vπ1​​)\",\"π2′​=argmaxπ​(rπ​+γPπ​v1​)\",\"⋮\",\"⋮\",\"⋮\",\"⋮\"]},\"58\":{\"h\":\"3.2 Truncated policy iteration algorithm\",\"t\":[\"20240811010933\",\"显然，在求解 Bellman equation 中，Value iteration 只是进行了一步求解，而 Policy iteration 进行了无穷多步来进行了真实的求解 state value，显然在现实运行算法中是无法做到的。 因此 Truncated policy iteration algorithm 就是进行迭代 n 步来求解。\"]},\"59\":{\"h\":\"truncated policy iteration algorithm 是否是收敛的\",\"t\":[\"20240811011334\"]},\"60\":{\"c\":[\"academic\"]},\"61\":{\"c\":[\"强化学习\"]},\"62\":{\"h\":\"RL5 - 蒙特卡洛方法 (Monte Carlo) model-free\",\"t\":[\"如何在没有模型 (即p(r∣s,a),p(s′∣s,a)等均未知) 的情况下进行估计 通过 Monte Carlo estimation. 其核心思想是： 若有一系列(i.i.d)样本采样，得到一个样本序列x1​,x2​,…,xN​ 那么对于随机变量X的估计可以为：\",\"E[x]≈xˉ=N1​j=1∑N​xj​\",\"该方法成立的数学依据是 大数定理 (Law of Large Numbers)样本必须是独立同分布(iid, independent and identically distributed)\",\"为什么考虑 mean estimation. 因为无论是 state value 还是 action value 其原始定义都是从期望出发的。\",\"vπ​(s)=E[Gt​∣St​=s];qπ​(s,a)=E[Gt​∣St​=s,At​=a]\"]},\"63\":{\"h\":\"1. MC Basic\",\"t\":[\"最简单的示例算法，用于解释 MC 的原理，但现实场景中不太经常使用，效率过低。\",\"核心思想：如何将 Policy iteration algorithm 转换为 model-free 的情况。\"]},\"64\":{\"h\":\"1.1 算法思路\",\"t\":[\"Policy iteration 算法的核心是 先根据当前策略计算出各个状态的 state value， 再将 state value 转换为 action value，更新策略的步骤就是选择此时 action value 最大的 action.\",\"{Policyevaluation:vπk​​=rπk​​+γPπk​​vπk​​Policyimprovement:πk+1​=argmaxπ​(rπ​+γPπ​vπk​​)​\",\"显然其核心关键就是在 PE 中 通过迭代算法求解 Bellman equation 的 state value后：\",\"对于 model-based 的情况, 因为 p(r∣s,a),p(s′∣s,a) 已知，我们可以很轻松的求出各个情况下的q(s,a)，从而选择每个状态下最大的 action value 即可。\",\"qπk​​(s,a)=r∑​p(r∣s,a)+γs′∑​p(s′∣s,a)vπk​​(s)\",\"对于 model-free 的情况，此时 p(r∣s,a),p(s′∣s,a) 未知，我们不能通过之前的方法来求出q(s,a)，需要从 action value 的定义出发，即：\",\"qπk​​(s,a)=E[Gt​∣St​=s,At​=a]\",\"从此可以发现，我们可以通过前面所引入的 mean estimation 方法，来进行求解 q(s,a).\"]},\"65\":{\"h\":\"1.2 如何估计\",\"t\":[\"从指定的 (s,a) 出发，根据策略 πk​, 我们可以生成一个 episode.\",\"这个 episode 的 return 为 g(s,a).\",\"显然，g(s,a) 就是前面 Gt​ 的一个 sample.\",\"假设我们有了一系列 从状态 s 出发, 采取动作 a 的 episodes, 即 g(j)(s,a). 那么我们可以对 qπk​​(s,a) 进行估计，即\",\"qπk​​(s,a)=E[Gt​∣St​=s,At​=a]≈N1​i=1∑N​g(i)(s,a).\"]},\"66\":{\"h\":\"1.3 具体算法\",\"t\":[\"与 Policy iteration algorithm 步骤类似 首先初始化一个随机的策略π0​，然后进行迭代，对于 kth 迭代，有：\",\"Step 1: Policy evaluation. 求在策略πk​下所有的 action value, q(s,a). 具体求解方法，如 1.2 节所述，只不过我们此时需要遍历所有的 action-state pair. 为什么不去求 state value，因为最终策略更新的核心仍然是 action value, 即使先估计了 state value, 我们仍需要估计 action value.\",\"Step 2: Policy improvement. 这是来求解 πk+1​(s)=argmaxπ​∑a​π(a∣s)qπk​​(s,a),foralls∈S 这个仍然与之前一致，采用 greedy policy，即对于每个状态，我们选取其 action value 最大的 action.πk+1​(ak∗​∣s)=1，其中ak∗​=argmaxa​qπk​​(s,a)\",\"20240811233346\"]},\"67\":{\"h\":\"2. MC Exploring Starts\",\"t\":[\"MC Exploring Starts 是针对 MC Basic 的一些改进，即对于数据(experience)更加高效利用。\"]},\"68\":{\"h\":\"2.1 Episode 的高效利用\",\"t\":[\"Visit: every time a state-action pair appears in the episode, it is called a visit of that state-action pair.\",\"考虑一个 episode, 跟随策略π,\",\"s1​a2​​s2​a4​​s1​a2​​s2​a3​​s5​a1​​…\",\"对于 MC-Basic, 这一条 episode 仅用作估计 state-action pair (s1​,a2​) 的 action value q(s1​,a2​)，但存在一定的浪费, 对于一个 episode, 可以拆分为多个 episode, 从而进行多次利用.\",\"s1​a2​​s2​a4​​s1​a2​​s2​a3​​s5​a1​​…s2​a4​​s1​a2​​s2​a3​​s5​a1​​…s1​a2​​s2​a3​​s5​a1​​…s2​a3​​s5​a1​​…s5​a1​​…​[originalepisode][episodestartingfrom(s2​,a4​)][episodestartingfrom(s1​,a2​)][episodestartingfrom(s2​,a3​)][episodestartingfrom(s5​,a1​)]​\",\"这样，我们不仅可以用来估计q(s1​,a2​), 还可以估计q(s2​,a4​),q(s2​,a3​)…\",\"Data-efficient methods:\",\"first-visit method 记录在 episode 中第一次出现的 state-action pair, 如果该 state-action pair 再次出现, 不记录 action value 估计中.\",\"every-visit method 对于每个 state-action pair, 都记录 action value 估计中.\"]},\"69\":{\"h\":\"2.2 高效地更新 Policy\",\"t\":[\"什么时候更新策略也是一个影响效率的因素。\",\"方法1：如 MC Based 一样，在收集到了足够多的 从给定的 state-action pair 出发的 episodes 后, 通过 mean estimation 估计了q(s,a)后, 才进行更新。 缺点，等候时间过长，只有当所有 episodes 均收集完，才能进行 策略更新。\",\"方法2：直接 uses the return of a single episode to approximate the action value. 这类算法统称为：Generalized policy iteration (GPI). 它会在 Policy-evaluation 和 policy-improvement 中不断切换，即不需要完全精确地求出 action value，就直接去更新策略。\"]},\"70\":{\"h\":\"2.3 MC Exploring Starts\",\"t\":[\"20240812004534\"]},\"71\":{\"h\":\"2.4 Exploring Statrts的解释\",\"t\":[\"Exploring 表示对于每一个 action-state pair (s,a), 都需要有多个 episodes, 这样才能去估计相应的qπ​(s,a). 如果存在一个 action value 未能访问，就不能确保所选择的 action 是最优的。\",\"Starts 表示对于对应 action-state pair (s,a) 的 episodes，每次都是从对应的状态 s 出发，选择对应的动作 a 进行的采样。 如果从其他状态出发，得到的 episode，如果经过了 (s,a)，那么这称为 visit , 但目前无法保证 visit 一定可以遍历所给定的 (s,a).\",\"据目前而言，Exploring Starts 是一个必要条件.\"]},\"72\":{\"h\":\"3. MC Eplison-Greedy\",\"t\":[\"将 Exploring Starts 条件转换掉，通过采取 Soft Policies 的方法。\"]},\"73\":{\"h\":\"3.1 Soft Policy\",\"t\":[\"A policy is called soft if the probability to take any action is positive. 显然 soft policy 是 stochastic 的，并且如果按照这样一个策略，在 episode 足够长的情况下，我们可以确保其可以遍历所有的 state-action pair.\"]},\"74\":{\"h\":\"3.2 -greedy policy\",\"t\":[\"在这里，我们采用的是 ϵ-greedy policies, 其属于 soft policies.\",\"π(a∣s)={1−∣A(s)∣ϵ​(∣A(s)∣−1),∣A(s)∣ϵ​,​forthegreedyaction,fortheother∣A(s)∣−1action,​\",\"其中 ϵ∈[0,1] 且 ∣A(s)∣ 为状态 s 的动作数量.ϵ-greedy policy 可以平衡 exploitation 和 exploration. 显然ϵ=0, policy 就是 greedy 的; 如果ϵ=1, 此时就是随机策略，其探索性就很强.\"]},\"75\":{\"h\":\"3.3 -greedy policy 引入 MC-based 算法中\",\"t\":[\"对于 MC Basic 以及 MC Exploring 中的 policy improvement 中，找的是在所有可能策略中的最优策略，因此是一个确定的贪心策略。\",\"20240812011140\"]},\"76\":{\"h\":\"3.3 算法流程\",\"t\":[\"20240812010538\"]},\"77\":{\"c\":[\"academic\"]},\"78\":{\"c\":[\"强化学习\"]},\"79\":{\"h\":\"RL6 - 随机近似理论与随机梯度下降算法\",\"t\":[\"针对 mean estimation 问题进行研究，因为在 RL 中 无论是 state value 还是 action value 其定义都是一个均值 (means)\",\"Stochastic approximation(SA): SA refers to a broad class of stochastic iterative algorithms soloving root finding or optimization problems.\"]},\"80\":{\"h\":\"1. 引言\"},\"81\":{\"h\":\"1.1 求均值的方法\",\"t\":[\"第一种：直接通过 E[x]≈xˉ:=N1​∑i=1N​xi​，进行估计，只有当样本全部收集完才能估计.\",\"第二种: 增量式的迭代算法. 假设:\",\"wk+1​=k1​i=1∑k​xi​,k=1,2,…\",\"对应的\",\"wk​=k−11​i=1∑k−1​xi​,k=2,3,…\",\"那么，wk+1​可以由wk​推导出来，即\",\"wk+1​​=k1​∑i=1k​xi​​=k1​(∑i=1k−1​xi​+xk​)=k1​((k−1)wk​+xk​)​=wk​−k1​(wk​−xk​)​\",\"因此，wk+1​=wk​−k1​(wk​−xk​)\"]},\"82\":{\"h\":\"2. Robbins-Monto(RM) algorithm\"},\"83\":{\"h\":\"2.1 问题引入\",\"t\":[\"假设我们需要求解如下方程:\",\"g(w)=0\",\"其中, w∈R 且需要被求解出来，g:R→R 为一个函数方程. 显然，如果对于 g(w) 已知的情况，我们可以通过一些特定的算法进行求解。 如果 g(w) 未知，就需要新的算法进行解决。\"]},\"84\":{\"h\":\"2.2 算法介绍\",\"t\":[\"RM 算法就可以用来求解当 g(w) 未知时的情况，即函数 g(w) 是一个黑盒，我们只能通过 输入序列: wk​, 得到含有噪音的观测值序列: g​(wk​,ηk​) 具体解决如下:\",\"wk+1​=wk​−ak​g​(wk​,ηk​),k=1,2,3,…\",\"其中:\",\"wk​ 是第 k 次方程根的估计.\",\"g​(wk​,ηk​)=g(wk​)+ηk​ 是第 k 次的观测值(含噪音).\",\"ak​ 是一个 positive coefficient.\"]},\"85\":{\"h\":\"2.3 收敛性分析\",\"t\":[\"Robbins-Monro Theorem In the Robbins-Monro algorithm, if\",\"0<c1​≤▽w​g(w)≤c2​,forallw; 要求g(w)必须是递增的，确保根是存在且唯一的。\",\"∑k=1∞​ak​=∞ 且 ∑k=1∞​ak2​<∞;∑k=1∞​ak2​=∞ 保证 ak​→0,k→0∑k=1∞​ak​=∞ 保证 ak​→0不要过快.\",\"E[ηk​∣Hk​]=0 且 E[ηk2​∣Hk​]<∞; 其中Hk​=wk​,wk−1​,…, 那么 wk​ converges with probability 1 (w.p.1) to the root w∗ satisfying g(w∗)=0.\",\"ak​=k1​是满足上面三个条件的. 但实际上我们往往是选择一个非常小的常数。\"]},\"86\":{\"h\":\"2.4 应用于 mean estimation 中\",\"t\":[\"比如我们要估计某个随机变量X的 E[X] 我们可以设计如下方程:\",\"g(w)≐w−E[X].\",\"那么只要求解 g(w)=0, 我们就可以得到 E[X] 的值。 同样，我们不能直接得到随机变量的值，而是对应的样本 x，sample of X. 即，我们得到的观测值是:\",\"g​(w,x)≐w−x\",\"我们可以修改为噪音 η 的形式，\",\"g​(w,η)​=w−x​=w−x+E[X]−E[X]=(w−E[X])+(E[X]−x)​≐g(w)+η​\",\"因此我们可以通过 RM 算法来进行求解\",\"wk+1​=wk​−αk​g​(wk​,ηk​)=wk​−αk​(wk​−xk​)\"]},\"87\":{\"h\":\"3. Stochastic gradient descent\"},\"88\":{\"h\":\"3.1 问题引入\",\"t\":[\"需要求解一个优化问题:\",\"wargmin​J(w)=E[f(w,X)]\",\"其中，\",\"w 是需要被优化的参数\",\"X 是一个随机变量\",\"w 和 X 可以是标量，也可以是向量. 对于函数 f(⋅) 输出为标量.\",\"对于这个问题，我们有以下几种方法:\",\"Method 1: 梯度下降法 (gradient descent, GD)\",\"wk+1​=wk​−αk​▽w​E[f(wk​,X)]=wk​−αk​E[▽w​f(wk​,X)]\",\"但由于 j(w) 是一个期望值，我们很难直接获得.\",\"Method 2: batch gradient descent (BGD) 借用 MC 的思想，我们可以将:\",\"E[▽w​f(wk​,X)]≈n1​i=1∑n​▽w​f(wk​,xi​).\",\"因此\",\"wk+1​=wk​−αk​n1​i=1∑n​▽w​f(wk​,xi​)\",\"但需要大量的 samples 收集完毕才能进行一次迭代.\",\"Method 3: 随机梯度下降(SGD) 考虑能否仅用一次 sample 进行迭代.\",\"wk+1​=wk​−αk​▽w​f(wk​,xk​)\",\"但能否保证其精确度，以及是否可以到最后优化的成果。\"]},\"89\":{\"h\":\"3.2 SGD 分析\"},\"90\":{\"h\":\"mean estimation 问题转化\",\"t\":[\"我们可以将 均值估计 问题 转化为 一个 优化问题 进行求解：\",\"20240814014058\"]},\"91\":{\"h\":\"SGD 正确性和收敛性分析\",\"t\":[\"从 GD 到 SGD:\",\"wk+1​=wk​−αk​E[▽w​f(wk​,X)]⇓wk+1​=wk​−αk​▽w​f(wk​,x)​\",\"显然我们可以将 ▽w​f(wk​,x) 视为 E[▽w​f(wk​,x)] 的一个观测值(含噪声):\",\"▽w​f(wk​,x)=E[▽w​f(wk​,x)]+η▽w​f(wk​,x)−E[▽w​f(wk​,x)]​​\",\"因为\",\"▽w​f(wk​,x)=E[▽w​f(wk​,x)]\",\"因此，我们需要思考使用 SGD 时wk​→w∗ as k→∞ 是否成立。\",\"我们可以将 SGD 视为一个特殊情况下的 RM 算法 SGD的目标是 minimize\",\"J(w)=E[f(w,X)]\",\"而最小值问题，往往可以转化为导数为 0 的情况,\",\"▽w​J(w)=E[▽w​f(w,X)]=0\",\"显然，可以参考 RM 算法, 让\",\"g(w)=▽w​J(w)=E[▽w​f(w,X)]\",\"从而转换为一个 root-finding 问题. 相应的，对于观测值g​(w,η),\",\"g~​(w,η)​=∇w​f(w,x)=g(w)E[∇w​f(w,X)]​​+η∇w​f(w,x)−E[∇w​f(w,X)]​​.​\",\"因此，我们就可以通过 RM 算法进行求解g(w)=0,\",\"wk+1​=wk​−αk​g​(wk​,ηk​)=wk​−αk​▽w​f(wk​,xk​)\",\"对应收敛性证明\"]},\"92\":{\"h\":\"3.3 SGD 另一种问题描述方法 (deterministic formulation)\",\"t\":[\"在之前关于使用 SGD 算法的问题描述中，我们是引入了 随机变量 和 期望的情况. 我们可以将这个问题可以转化为一个随机变量的方法，从而引入 SGD 算法.\"]},\"93\":{\"h\":\"3.4 BGD MBGD SGDw\",\"t\":[\"20240814230747\"]},\"94\":{\"c\":[\"academic\"]},\"95\":{\"c\":[\"强化学习\"]},\"96\":{\"h\":\"RL7 - Temporal-Difference Learning\"},\"97\":{\"c\":[\"academic\"]},\"98\":{\"c\":[\"强化学习\"]},\"99\":{\"h\":\"Java1\",\"t\":[\"a+b=c\"]},\"100\":{\"c\":[\"code\"]},\"101\":{\"c\":[\"java\"]},\"102\":{\"h\":\"\",\"t\":[\"404 Not Found\"]},\"103\":{\"h\":\"Daily\"},\"104\":{\"h\":\"UAV\"},\"105\":{\"h\":\"Academic\"},\"106\":{\"h\":\"强化学习\"},\"107\":{\"h\":\"Java\"},\"108\":{\"h\":\"Code\"}},\"dirtCount\":0,\"index\":[[\"期望的情况\",{\"1\":{\"92\":1}}],[\"另一种问题描述方法\",{\"0\":{\"92\":1}}],[\"∇w​f\",{\"1\":{\"91\":2}}],[\"相应的\",{\"1\":{\"91\":1}}],[\"让\",{\"1\":{\"91\":1}}],[\"▽w​j\",{\"1\":{\"91\":1}}],[\"▽w​f\",{\"1\":{\"88\":2,\"91\":10}}],[\"往往可以转化为导数为\",{\"1\":{\"91\":1}}],[\"时wk​→w∗\",{\"1\":{\"91\":1}}],[\"=e\",{\"1\":{\"91\":1}}],[\"=n\",{\"1\":{\"8\":1,\"11\":1,\"13\":1}}],[\"含噪声\",{\"1\":{\"91\":1}}],[\"含噪音\",{\"1\":{\"84\":1}}],[\"视为一个特殊情况下的\",{\"1\":{\"91\":1}}],[\"视为\",{\"1\":{\"91\":1}}],[\"⇓wk+1​=wk​−αk​▽w​f\",{\"1\":{\"91\":1}}],[\"到\",{\"1\":{\"91\":1}}],[\"正确性和收敛性分析\",{\"0\":{\"91\":1}}],[\"优化问题\",{\"1\":{\"90\":1}}],[\"优化问题建立\",{\"0\":{\"11\":1}}],[\"转化为\",{\"1\":{\"90\":1}}],[\"转换为\",{\"1\":{\"45\":1,\"63\":1,\"64\":1}}],[\"转换到状态s\",{\"1\":{\"20\":1}}],[\"均值估计\",{\"1\":{\"90\":1}}],[\"均收集完\",{\"1\":{\"69\":1}}],[\"分析\",{\"0\":{\"89\":1}}],[\"分别设为1\",{\"1\":{\"10\":1}}],[\"借用\",{\"1\":{\"88\":1}}],[\"梯度下降法\",{\"1\":{\"88\":1}}],[\"输出为标量\",{\"1\":{\"88\":1}}],[\"输入序列\",{\"1\":{\"84\":1}}],[\"⋅\",{\"1\":{\"88\":1}}],[\"η\",{\"1\":{\"86\":2,\"91\":2}}],[\"ηk2​∣hk​\",{\"1\":{\"85\":1}}],[\"ηk​∣hk​\",{\"1\":{\"85\":1}}],[\"ηk​\",{\"1\":{\"84\":3,\"86\":1,\"91\":1}}],[\"≐w−x\",{\"1\":{\"86\":1}}],[\"≐w−e\",{\"1\":{\"86\":1}}],[\"比如我们要估计某个随机变量x的\",{\"1\":{\"86\":1}}],[\"应用于\",{\"0\":{\"86\":1}}],[\"应该是无人机最终停的位置即是部署的最佳位置\",{\"1\":{\"13\":1}}],[\"<∞\",{\"1\":{\"85\":1}}],[\"保证\",{\"1\":{\"85\":2}}],[\"保证选取最大的\",{\"1\":{\"44\":1}}],[\"确保根是存在且唯一的\",{\"1\":{\"85\":1}}],[\"必须是递增的\",{\"1\":{\"85\":1}}],[\"要求g\",{\"1\":{\"85\":1}}],[\"收集完毕才能进行一次迭代\",{\"1\":{\"88\":1}}],[\"收敛性分析\",{\"0\":{\"85\":1}}],[\"收益最大\",{\"1\":{\"35\":1}}],[\"次的观测值\",{\"1\":{\"84\":1}}],[\"次方程根的估计\",{\"1\":{\"84\":1}}],[\"问题\",{\"1\":{\"90\":1,\"91\":1}}],[\"问题转化\",{\"0\":{\"90\":1}}],[\"问题引入\",{\"0\":{\"83\":1,\"88\":1}}],[\"问题进行研究\",{\"1\":{\"79\":1}}],[\"增量式的迭代算法\",{\"1\":{\"81\":1}}],[\"第二种\",{\"1\":{\"81\":1}}],[\"第一种\",{\"1\":{\"81\":1}}],[\"引言\",{\"0\":{\"80\":1}}],[\"引入\",{\"0\":{\"75\":1},\"1\":{\"35\":1}}],[\"引入随机变量后对应的discounted\",{\"1\":{\"26\":1}}],[\"引入discount\",{\"1\":{\"19\":1}}],[\"针对\",{\"1\":{\"79\":1}}],[\"随机变量\",{\"1\":{\"92\":1}}],[\"随机梯度下降\",{\"1\":{\"88\":1}}],[\"随机近似理论与随机梯度下降算法\",{\"0\":{\"79\":1}}],[\"随着无人机与用户之间距离和发射功率的变化\",{\"1\":{\"9\":1}}],[\"找的是在所有可能策略中的最优策略\",{\"1\":{\"75\":1}}],[\"ϵ∈\",{\"1\":{\"74\":1}}],[\"ϵ\",{\"1\":{\"74\":2}}],[\"足够长的情况下\",{\"1\":{\"73\":1}}],[\"条件转换掉\",{\"1\":{\"72\":1}}],[\"据目前而言\",{\"1\":{\"71\":1}}],[\"未能访问\",{\"1\":{\"71\":1}}],[\"未知时的情况\",{\"1\":{\"84\":1}}],[\"未知\",{\"1\":{\"64\":1,\"83\":1}}],[\"它会在\",{\"1\":{\"69\":1}}],[\"直接通过\",{\"1\":{\"81\":1}}],[\"直接\",{\"1\":{\"69\":1}}],[\"直到设置的收敛条件为止\",{\"1\":{\"53\":1}}],[\"直到各个簇的成员没有太大变化\",{\"1\":{\"13\":1}}],[\"才能进行\",{\"1\":{\"69\":1}}],[\"才进行更新\",{\"1\":{\"69\":1}}],[\"等候时间过长\",{\"1\":{\"69\":1}}],[\"等均未知\",{\"1\":{\"62\":1}}],[\"缺点\",{\"1\":{\"69\":1}}],[\"估计了q\",{\"1\":{\"69\":1}}],[\"估计中\",{\"1\":{\"68\":2}}],[\"什么时候更新策略也是一个影响效率的因素\",{\"1\":{\"69\":1}}],[\"高效地更新\",{\"0\":{\"69\":1}}],[\"高度h=hn​\",{\"1\":{\"11\":1}}],[\"高度的上界是最大发射功率pmax​的函数\",{\"1\":{\"9\":1}}],[\"都需要有多个\",{\"1\":{\"71\":1}}],[\"都记录\",{\"1\":{\"68\":1}}],[\"都有可选择的动作\",{\"1\":{\"19\":1}}],[\"记录在\",{\"1\":{\"68\":1}}],[\"仅用作估计\",{\"1\":{\"68\":1}}],[\"仅需要考虑无人机的7个移动方向即可\",{\"1\":{\"14\":1}}],[\"跟随策略π\",{\"1\":{\"68\":1}}],[\"节所述\",{\"1\":{\"66\":1}}],[\"有\",{\"1\":{\"66\":1}}],[\"有关\",{\"1\":{\"35\":1}}],[\"迭代\",{\"1\":{\"66\":1}}],[\"然后进行迭代\",{\"1\":{\"66\":1}}],[\"然后进行不断迭代\",{\"1\":{\"34\":1}}],[\"首先初始化一个随机的策略π0​\",{\"1\":{\"66\":1}}],[\"首先随机设计一个初始的策略π0​\",{\"1\":{\"53\":1}}],[\"≈n1​i=1∑n​▽w​f\",{\"1\":{\"88\":1}}],[\"≈n1​i=1∑n​g\",{\"1\":{\"65\":1}}],[\"≈xˉ\",{\"1\":{\"81\":1}}],[\"≈xˉ=n1​j=1∑n​xj​\",{\"1\":{\"62\":1}}],[\"假设我们需要求解如下方程\",{\"1\":{\"83\":1}}],[\"假设我们有了一系列\",{\"1\":{\"65\":1}}],[\"假设\",{\"1\":{\"81\":1}}],[\"假设功率q=qn​\",{\"1\":{\"11\":1}}],[\"出发的\",{\"1\":{\"69\":1}}],[\"出发\",{\"1\":{\"65\":2,\"71\":1}}],[\"方法2\",{\"1\":{\"69\":1}}],[\"方法1\",{\"1\":{\"69\":1}}],[\"方法\",{\"1\":{\"64\":1}}],[\"方便后续比较\",{\"1\":{\"57\":1}}],[\"已知的情况\",{\"1\":{\"83\":1}}],[\"已知\",{\"1\":{\"64\":1}}],[\"先根据当前策略计算出各个状态的\",{\"1\":{\"64\":1}}],[\"核心思想\",{\"1\":{\"63\":1}}],[\"核心内容\",{\"0\":{\"24\":1}}],[\"效率过低\",{\"1\":{\"63\":1}}],[\"样本必须是独立同分布\",{\"1\":{\"62\":1}}],[\"样本采样\",{\"1\":{\"62\":1}}],[\"大数定理\",{\"1\":{\"62\":1}}],[\"若有一系列\",{\"1\":{\"62\":1}}],[\"蒙特卡洛方法\",{\"0\":{\"62\":1}}],[\"就需要新的算法进行解决\",{\"1\":{\"83\":1}}],[\"就不能确保所选择的\",{\"1\":{\"71\":1}}],[\"就直接去更新策略\",{\"1\":{\"69\":1}}],[\"就是\",{\"1\":{\"74\":1}}],[\"就是前面\",{\"1\":{\"65\":1}}],[\"就是进行迭代\",{\"1\":{\"58\":1}}],[\"就好了\",{\"1\":{\"50\":1}}],[\"⋮\",{\"1\":{\"57\":4}}],[\"两个算法迭代过程十分类似\",{\"1\":{\"57\":1}}],[\"进行的采样\",{\"1\":{\"71\":1}}],[\"进行估计\",{\"1\":{\"65\":1,\"81\":1}}],[\"进行了无穷多步来进行了真实的求解\",{\"1\":{\"58\":1}}],[\"进行迭代\",{\"1\":{\"57\":1,\"88\":1}}],[\"进行求解\",{\"1\":{\"34\":1,\"53\":1,\"90\":1}}],[\"内嵌迭代算法求解\",{\"1\":{\"57\":1}}],[\"之后进行迭代\",{\"1\":{\"57\":1}}],[\"之间存在什么关系\",{\"1\":{\"55\":1}}],[\"那么只要求解\",{\"1\":{\"86\":1}}],[\"那么这称为\",{\"1\":{\"71\":1}}],[\"那么我们可以对\",{\"1\":{\"65\":1}}],[\"那么对于随机变量x的估计可以为\",{\"1\":{\"62\":1}}],[\"那么\",{\"1\":{\"55\":1,\"81\":1,\"85\":1}}],[\"实际不常用\",{\"1\":{\"55\":1}}],[\"实际意义的解释\",{\"1\":{\"35\":1}}],[\"实际意义是\",{\"1\":{\"35\":1}}],[\"中的\",{\"1\":{\"75\":1}}],[\"中不断切换\",{\"1\":{\"69\":1}}],[\"中第一次出现的\",{\"1\":{\"68\":1}}],[\"中\",{\"0\":{\"86\":1},\"1\":{\"58\":1,\"64\":1,\"75\":1,\"79\":1}}],[\"中求解\",{\"1\":{\"55\":1}}],[\"中涉及的\",{\"1\":{\"49\":1}}],[\"得到含有噪音的观测值序列\",{\"1\":{\"84\":1}}],[\"得到的\",{\"1\":{\"71\":1}}],[\"得到一个样本序列x1​\",{\"1\":{\"62\":1}}],[\"得到\",{\"1\":{\"55\":1}}],[\"一个\",{\"1\":{\"90\":1}}],[\"一定可以遍历所给定的\",{\"1\":{\"71\":1}}],[\"一样\",{\"1\":{\"69\":1}}],[\"一般化的推广\",{\"1\":{\"56\":1}}],[\"一种是通过迭代算法来求解\",{\"1\":{\"55\":1}}],[\"一种是可以直接通过矩阵求逆进行求解\",{\"1\":{\"55\":1}}],[\"一种迭代策略\",{\"1\":{\"34\":1}}],[\"一些问题\",{\"0\":{\"55\":1}}],[\"或者\",{\"1\":{\"53\":1}}],[\"java\",{\"0\":{\"107\":1},\"2\":{\"101\":1}}],[\"java1\",{\"0\":{\"99\":1}}],[\"j\",{\"1\":{\"53\":2,\"55\":1,\"65\":1,\"88\":1,\"91\":1}}],[\"j+1\",{\"1\":{\"53\":2,\"55\":1}}],[\"策略更新\",{\"1\":{\"69\":1}}],[\"策略求解\",{\"1\":{\"57\":2}}],[\"策略提升\",{\"1\":{\"53\":1}}],[\"策略评估\",{\"1\":{\"53\":1}}],[\"策略为贪心策略\",{\"1\":{\"13\":1}}],[\"算法的问题描述中\",{\"1\":{\"92\":1}}],[\"算法的核心是\",{\"1\":{\"64\":1}}],[\"算法进行求解g\",{\"1\":{\"91\":1}}],[\"算法\",{\"1\":{\"91\":2,\"92\":1}}],[\"算法来进行求解\",{\"1\":{\"86\":1}}],[\"算法就可以用来求解当\",{\"1\":{\"84\":1}}],[\"算法介绍\",{\"0\":{\"84\":1}}],[\"算法流程\",{\"0\":{\"76\":1}}],[\"算法中\",{\"0\":{\"75\":1}}],[\"算法思路\",{\"0\":{\"64\":1}}],[\"算法比较\",{\"0\":{\"57\":1}}],[\"算法描述\",{\"0\":{\"53\":1}}],[\"算法迭代示意图\",{\"1\":{\"52\":1}}],[\"算法1\",{\"1\":{\"13\":1}}],[\"伪代码\",{\"0\":{\"51\":1,\"54\":1}}],[\"步骤类似\",{\"1\":{\"66\":1}}],[\"步骤中\",{\"1\":{\"55\":2}}],[\"步来求解\",{\"1\":{\"58\":1}}],[\"步\",{\"1\":{\"50\":1}}],[\"动态规划\",{\"0\":{\"48\":1}}],[\"动作\",{\"1\":{\"19\":1}}],[\"动作空间包含两个部分\",{\"1\":{\"14\":1}}],[\"动作空间\",{\"1\":{\"13\":1}}],[\"满足f\",{\"1\":{\"45\":1}}],[\"区别于贝尔曼公式\",{\"1\":{\"43\":1}}],[\"外\",{\"1\":{\"42\":1}}],[\"除了需要求解\",{\"1\":{\"42\":1}}],[\"作为下一步的\",{\"1\":{\"40\":1}}],[\"选择对应的动作\",{\"1\":{\"71\":1}}],[\"选择移动方向和选择关联用户\",{\"1\":{\"14\":1}}],[\"选取当前状态下最大的\",{\"1\":{\"57\":2}}],[\"选取状态中最大的\",{\"1\":{\"40\":1}}],[\"唯一\",{\"1\":{\"40\":1}}],[\"存在不动点x∗\",{\"1\":{\"45\":1}}],[\"存在\",{\"1\":{\"40\":1}}],[\"存在lemma1\",{\"1\":{\"9\":1}}],[\"≥vπ​\",{\"1\":{\"40\":1}}],[\"贝尔曼最优公式\",{\"0\":{\"39\":1},\"1\":{\"48\":1}}],[\"贝尔曼公式\",{\"0\":{\"23\":1}}],[\"总结\",{\"0\":{\"36\":1}}],[\"总用户的mos取决于无人机的发射功率\",{\"1\":{\"11\":1}}],[\"加权均值\",{\"1\":{\"35\":1}}],[\"加上\",{\"1\":{\"35\":1}}],[\"和\",{\"1\":{\"35\":1,\"69\":1,\"74\":1,\"88\":1,\"92\":1}}],[\"和动作\",{\"1\":{\"35\":1}}],[\"和e\",{\"1\":{\"29\":1}}],[\"我们是引入了\",{\"1\":{\"92\":1}}],[\"我们就可以通过\",{\"1\":{\"91\":1}}],[\"我们就可以得到\",{\"1\":{\"86\":1}}],[\"我们需要思考使用\",{\"1\":{\"91\":1}}],[\"我们需要保证策略是不断提升\",{\"1\":{\"55\":1}}],[\"我们很难直接获得\",{\"1\":{\"88\":1}}],[\"我们有以下几种方法\",{\"1\":{\"88\":1}}],[\"我们得到的观测值是\",{\"1\":{\"86\":1}}],[\"我们只能通过\",{\"1\":{\"84\":1}}],[\"我们只需要挑选在当前迭代下最大的\",{\"1\":{\"50\":1}}],[\"我们只需要在当前状态下\",{\"1\":{\"44\":1}}],[\"我们采用的是\",{\"1\":{\"74\":1}}],[\"我们不能直接得到随机变量的值\",{\"1\":{\"86\":1}}],[\"我们不能通过之前的方法来求出q\",{\"1\":{\"64\":1}}],[\"我们不仅可以用来估计q\",{\"1\":{\"68\":1}}],[\"我们选取其\",{\"1\":{\"66\":1}}],[\"我们仍需要估计\",{\"1\":{\"66\":1}}],[\"我们这里强行初始化为vπ0​​\",{\"1\":{\"57\":1}}],[\"我们可以将这个问题可以转化为一个随机变量的方法\",{\"1\":{\"92\":1}}],[\"我们可以将\",{\"1\":{\"88\":1,\"90\":1,\"91\":1}}],[\"我们可以修改为噪音\",{\"1\":{\"86\":1}}],[\"我们可以设计如下方程\",{\"1\":{\"86\":1}}],[\"我们可以确保其可以遍历所有的\",{\"1\":{\"73\":1}}],[\"我们可以生成一个\",{\"1\":{\"65\":1}}],[\"我们可以通过一些特定的算法进行求解\",{\"1\":{\"83\":1}}],[\"我们可以通过前面所引入的\",{\"1\":{\"64\":1}}],[\"我们可以通过对应的迭代算法来求解贝尔曼最优公式\",{\"1\":{\"49\":1}}],[\"我们可以很轻松的求出各个情况下的q\",{\"1\":{\"64\":1}}],[\"我们可以分析出在该状态下采取哪个\",{\"1\":{\"35\":1}}],[\"我们还需要理解其所描述的最优策略π∗\",{\"1\":{\"43\":1}}],[\"我们对无人机的发射功率有一个约束\",{\"1\":{\"9\":1}}],[\"采取动作\",{\"1\":{\"65\":1}}],[\"采取一个指定的action可以得到的平均return\",{\"1\":{\"35\":1}}],[\"采用\",{\"1\":{\"66\":1}}],[\"采用的是the\",{\"1\":{\"14\":1}}],[\"采用q\",{\"1\":{\"13\":1}}],[\"采用基于遗传算法的gak\",{\"1\":{\"13\":1}}],[\"采用mos作为用户qos衡量的标准\",{\"1\":{\"10\":1}}],[\"k→0∑k=1∞​ak​=∞\",{\"1\":{\"85\":1}}],[\"k→∞\",{\"1\":{\"34\":1,\"45\":1,\"91\":1}}],[\"k\",{\"1\":{\"84\":2}}],[\"k−1\",{\"1\":{\"81\":1}}],[\"k=2\",{\"1\":{\"81\":1}}],[\"k=1\",{\"1\":{\"49\":1,\"81\":1,\"84\":1}}],[\"kth\",{\"1\":{\"66\":1}}],[\"kn​∩kn\",{\"1\":{\"8\":1,\"11\":1,\"13\":1}}],[\"kn​\",{\"1\":{\"8\":1,\"9\":2}}],[\"重新修改为\",{\"1\":{\"33\":1}}],[\"重复步骤\",{\"1\":{\"13\":1}}],[\"整合\",{\"1\":{\"33\":1}}],[\"称为策略评估\",{\"1\":{\"32\":1}}],[\"无论是\",{\"1\":{\"79\":1}}],[\"无记忆性\",{\"1\":{\"31\":1}}],[\"无人机需要进行移动\",{\"1\":{\"14\":1}}],[\"无人机的动态移动设计\",{\"0\":{\"14\":1}}],[\"无人机的位置初始化也是随机部署的\",{\"1\":{\"13\":1}}],[\"无人机的3d部署\",{\"0\":{\"13\":1}}],[\"无人机3d部署算法\",{\"1\":{\"13\":1}}],[\"无人机\",{\"1\":{\"13\":1}}],[\"无人机n以可变高度悬停在用户上方\",{\"1\":{\"13\":1}}],[\"无人机n的高度需满足\",{\"1\":{\"9\":1}}],[\"无人机n与用户kn​在时间t的距离表示为\",{\"1\":{\"8\":1}}],[\"无人机往往有更高的los链接概率\",{\"1\":{\"9\":1}}],[\"用于解释\",{\"1\":{\"63\":1}}],[\"用来描述所有状态的state\",{\"1\":{\"29\":1}}],[\"用户的速度设为\",{\"1\":{\"14\":1}}],[\"用户漫游模型\",{\"1\":{\"14\":1}}],[\"用户区域划分算法\",{\"1\":{\"13\":1}}],[\"用户关联策略\",{\"1\":{\"13\":1}}],[\"用户是保持静态的\",{\"1\":{\"13\":1}}],[\"用户rkn​​在一段时间ts​内的mos总和为\",{\"1\":{\"10\":1}}],[\"只有当样本全部收集完才能估计\",{\"1\":{\"81\":1}}],[\"只有当所有\",{\"1\":{\"69\":1}}],[\"只有当所有东西都是确定性的\",{\"1\":{\"28\":1}}],[\"只不过我们此时需要遍历所有的\",{\"1\":{\"66\":1}}],[\"只不过根据区域划分\",{\"1\":{\"13\":1}}],[\"只是进行了一步求解\",{\"1\":{\"58\":1}}],[\"只是求解各状态的\",{\"1\":{\"43\":1}}],[\"求均值的方法\",{\"0\":{\"81\":1}}],[\"求在策略πk​下所有的\",{\"1\":{\"66\":1}}],[\"求解当前策略的\",{\"1\":{\"57\":1}}],[\"求解下一步的vk+1​\",{\"1\":{\"50\":1}}],[\"求解方法\",{\"1\":{\"45\":1}}],[\"求解\",{\"0\":{\"45\":1},\"1\":{\"36\":1}}],[\"求解bellman\",{\"1\":{\"32\":1}}],[\"求出其对应状态的\",{\"1\":{\"34\":1}}],[\"求\",{\"1\":{\"28\":1}}],[\"而最小值问题\",{\"1\":{\"91\":1}}],[\"而是对应的样本\",{\"1\":{\"86\":1}}],[\"而言\",{\"1\":{\"57\":1}}],[\"而\",{\"1\":{\"28\":1,\"57\":1,\"58\":1}}],[\"而不是仅考虑吞吐量\",{\"1\":{\"4\":1}}],[\"3\",{\"0\":{\"28\":1,\"32\":1,\"34\":1,\"55\":1,\"56\":1,\"57\":1,\"58\":1,\"66\":1,\"70\":1,\"72\":1,\"73\":1,\"74\":1,\"75\":2,\"76\":2,\"85\":1,\"87\":1,\"88\":1,\"89\":1,\"92\":2,\"93\":1},\"1\":{\"49\":2,\"57\":1,\"81\":1,\"84\":1,\"88\":1}}],[\"3d部署和移动问题\",{\"1\":{\"5\":1}}],[\"3d位置进行优化处理\",{\"1\":{\"5\":1}}],[\"也可以是向量\",{\"1\":{\"88\":1}}],[\"也可以是无限长的trajectory\",{\"1\":{\"19\":1}}],[\"也称为\",{\"1\":{\"27\":1}}],[\"visit\",{\"1\":{\"68\":4,\"71\":2}}],[\"vu\",{\"1\":{\"57\":1}}],[\"v0​\",{\"1\":{\"57\":2}}],[\"vπ1​​\",{\"1\":{\"57\":1}}],[\"vπ1​​=rπ1​​+γpπ1​​vπ1​​\",{\"1\":{\"57\":1}}],[\"vπ0​​是通过迭代算法来求的\",{\"1\":{\"57\":1}}],[\"vπ0​​=rπ0​​+γpπ0​​vπ0​​\",{\"1\":{\"57\":1}}],[\"vπ0​​≤vπ1​​≤vπ2​​⋯≤vπk​​≤⋯≤v∗\",{\"1\":{\"55\":1}}],[\"vπk​​=\",{\"1\":{\"55\":1}}],[\"vπk​​=rπk​​+γpπk​​vπk​​policyimprovement\",{\"1\":{\"64\":1}}],[\"vπk​​=rπk​​+γpπk​​vπk​​\",{\"1\":{\"53\":1,\"57\":1}}],[\"vπk​​\",{\"1\":{\"53\":1,\"55\":1,\"64\":1}}],[\"vπk​\",{\"1\":{\"53\":2,\"55\":1}}],[\"vπ​=\",{\"1\":{\"33\":1,\"34\":1}}],[\"vπ​=rπ​+γpπ​vπ​​\",{\"1\":{\"33\":1}}],[\"vπ​\",{\"1\":{\"27\":2,\"29\":1,\"31\":1,\"32\":3,\"33\":4,\"35\":5,\"36\":1,\"42\":2,\"44\":1,\"62\":1}}],[\"v\",{\"1\":{\"45\":5,\"48\":1,\"50\":2}}],[\"v=f\",{\"1\":{\"45\":1,\"48\":1}}],[\"v=πmax​\",{\"1\":{\"42\":1}}],[\"vk​\",{\"1\":{\"49\":1}}],[\"vk​→vπ​=\",{\"1\":{\"34\":1}}],[\"vk+1​=rπk+1​​+γpπk+1​​vk​\",{\"1\":{\"57\":1}}],[\"vk+1​=rπk+1​​+γpπk+1​​vk​这里的vk​并不是\",{\"1\":{\"50\":1}}],[\"vk+1​=rπ​+γpπ​vk​​\",{\"1\":{\"34\":1}}],[\"vk+1​=f\",{\"1\":{\"49\":1}}],[\"v2​\",{\"1\":{\"34\":1}}],[\"v1​=rπ1​​+γpπ1​​v0​\",{\"1\":{\"57\":1}}],[\"v1​\",{\"1\":{\"34\":1}}],[\"value后\",{\"1\":{\"64\":1}}],[\"value的关系\",{\"1\":{\"29\":1}}],[\"value\",{\"0\":{\"25\":1,\"27\":1,\"28\":1,\"34\":1,\"35\":1,\"45\":1,\"49\":1,\"57\":1},\"1\":{\"24\":1,\"27\":5,\"28\":3,\"29\":1,\"32\":1,\"34\":1,\"35\":10,\"36\":4,\"39\":1,\"40\":1,\"42\":1,\"43\":1,\"44\":2,\"49\":1,\"50\":3,\"53\":3,\"55\":2,\"56\":1,\"57\":12,\"58\":2,\"62\":2,\"64\":6,\"66\":6,\"68\":3,\"69\":2,\"71\":1,\"79\":2}}],[\"vector\",{\"0\":{\"33\":1},\"1\":{\"33\":1,\"36\":1,\"42\":1}}],[\"very\",{\"1\":{\"10\":1}}],[\"vehicles\",{\"1\":{\"4\":1}}],[\"vehicular\",{\"1\":{\"3\":1},\"2\":{\"16\":1}}],[\"强化学习\",{\"0\":{\"106\":1},\"2\":{\"22\":1,\"38\":1,\"47\":1,\"61\":1,\"78\":1,\"95\":1,\"98\":1}}],[\"强化学习框架图\",{\"0\":{\"18\":1}}],[\"获得reward\",{\"1\":{\"20\":1}}],[\"∣\",{\"1\":{\"74\":1}}],[\"∣−1action\",{\"1\":{\"74\":1}}],[\"∣−1\",{\"1\":{\"74\":1}}],[\"∣a\",{\"1\":{\"74\":3}}],[\"∣ϵ​\",{\"1\":{\"74\":2}}],[\"∣∣≤δ\",{\"1\":{\"53\":1}}],[\"∣∣≤γ∣∣x1​−x2​∣∣\",{\"1\":{\"45\":1}}],[\"∣∣vπk+1​\",{\"1\":{\"53\":1}}],[\"∣∣f\",{\"1\":{\"45\":1}}],[\"∣s\",{\"1\":{\"20\":1,\"28\":1,\"31\":4,\"32\":3,\"33\":1,\"35\":2,\"42\":1,\"50\":2,\"53\":2,\"62\":1,\"64\":3}}],[\"∣kn​∣为第n个簇的用户总数\",{\"1\":{\"14\":1}}],[\"关键元素\",{\"1\":{\"20\":1}}],[\"控制其一直待在target\",{\"1\":{\"19\":1}}],[\"会与环境一直交互下去\",{\"1\":{\"19\":1}}],[\"会将其划分为n个簇\",{\"1\":{\"8\":1}}],[\"这类算法统称为\",{\"1\":{\"69\":1}}],[\"这样才能去估计相应的qπ​\",{\"1\":{\"71\":1}}],[\"这样\",{\"1\":{\"68\":1}}],[\"这样的任务称为episodic\",{\"1\":{\"19\":1}}],[\"这一条\",{\"1\":{\"68\":1}}],[\"这是来求解\",{\"1\":{\"66\":1}}],[\"这个仍然与之前一致\",{\"1\":{\"66\":1}}],[\"这个\",{\"1\":{\"65\":1}}],[\"这两个算法是一致的\",{\"1\":{\"57\":1}}],[\"这步是更新策略π\",{\"1\":{\"50\":1}}],[\"这种迭代算法称为\",{\"1\":{\"49\":1}}],[\"这里需要通过迭代算法来精确求出\",{\"1\":{\"57\":1}}],[\"这里\",{\"1\":{\"53\":1}}],[\"这里的最优策略πk+1​是一个\",{\"1\":{\"50\":1}}],[\"这里有多种mobility\",{\"1\":{\"14\":1}}],[\"这里是每个无人机都有一张自己的q\",{\"1\":{\"13\":1}}],[\"这里采用离散化空间坐标\",{\"1\":{\"13\":1}}],[\"通常是具有有限步长的trajectory\",{\"1\":{\"19\":1}}],[\"通过采取\",{\"1\":{\"72\":1}}],[\"通过迭代算法求解\",{\"1\":{\"64\":1}}],[\"通过求解\",{\"1\":{\"35\":1}}],[\"通过\",{\"1\":{\"32\":1,\"34\":1,\"57\":1,\"62\":1,\"69\":1}}],[\"通过k\",{\"1\":{\"13\":1}}],[\"通过gak\",{\"1\":{\"5\":1}}],[\"通过优化无人机的部署和动态移动来解决总用户mos最大化问题\",{\"1\":{\"5\":1}}],[\"更加高效利用\",{\"1\":{\"67\":1}}],[\"更新策略的步骤就是选择此时\",{\"1\":{\"64\":1}}],[\"更注重长远的reward\",{\"1\":{\"19\":1}}],[\"更远视\",{\"1\":{\"19\":1}}],[\"更多地是考虑多架无人机的二维部署或单架无人机在地面用户保持静止情况下的部署\",{\"1\":{\"4\":1}}],[\"注重近期的reward\",{\"1\":{\"19\":1}}],[\"显然我们可以将\",{\"1\":{\"91\":1}}],[\"显然ϵ=0\",{\"1\":{\"74\":1}}],[\"显然其核心关键就是在\",{\"1\":{\"64\":1}}],[\"显然在现实运行算法中是无法做到的\",{\"1\":{\"58\":1}}],[\"显然是可以通过一个\",{\"1\":{\"53\":1}}],[\"显然\",{\"1\":{\"19\":1,\"50\":1,\"58\":1,\"65\":1,\"73\":1,\"83\":1,\"91\":1}}],[\"γa∑​π\",{\"1\":{\"32\":1}}],[\"γ\",{\"1\":{\"26\":1,\"35\":1}}],[\"γ接近1\",{\"1\":{\"19\":1}}],[\"γ∈\",{\"1\":{\"19\":1}}],[\"γkn​\",{\"1\":{\"11\":1,\"13\":1}}],[\"γkn​​\",{\"1\":{\"9\":1}}],[\"γk0​σ2μlos​pmax​​\",{\"1\":{\"9\":1}}],[\"此时就是随机策略\",{\"1\":{\"74\":1}}],[\"此时\",{\"1\":{\"33\":1,\"64\":1}}],[\"此时对应的discountedrate=0+γ0+γ20+γ31+γ41+⋯=γ31−γ1​\",{\"1\":{\"19\":1}}],[\"此时该trajectory的return=0+0+0+1+1+⋯=∞\",{\"1\":{\"19\":1}}],[\"此时的mos模型定义如下\",{\"1\":{\"10\":1}}],[\"依赖于当前状态和所采取的动作\",{\"1\":{\"19\":1}}],[\"指导agent在当前状态下选择哪个动作\",{\"1\":{\"19\":1}}],[\"π2\",{\"1\":{\"57\":1}}],[\"π2​=argmaxπ​\",{\"1\":{\"57\":1}}],[\"π1​=argmaxπ​\",{\"1\":{\"57\":2}}],[\"π180​θkn​​−ζ\",{\"1\":{\"9\":1}}],[\"π0​\",{\"1\":{\"57\":1}}],[\"π0​pe​vπ0​​pi​π1​pe​vπ1​​pi​π2​pe​vπ2​​pi​\",{\"1\":{\"52\":1,\"57\":1}}],[\"πk​的\",{\"1\":{\"55\":1}}],[\"πk​\",{\"1\":{\"53\":2,\"65\":1}}],[\"πk+1​=πargmax​\",{\"1\":{\"53\":1,\"57\":2}}],[\"πk+1​=argmaxπ​\",{\"1\":{\"50\":1,\"64\":1}}],[\"πk+1​\",{\"1\":{\"50\":2,\"53\":2,\"55\":1,\"66\":2}}],[\"π∗\",{\"1\":{\"40\":2}}],[\"π\",{\"1\":{\"19\":1,\"20\":1,\"27\":1,\"28\":1,\"31\":1,\"32\":1,\"35\":1,\"40\":1,\"44\":1,\"74\":1}}],[\"定义\",{\"1\":{\"35\":1}}],[\"定义了agent与环境的交互行为\",{\"1\":{\"19\":1}}],[\"定义为ξ=\",{\"1\":{\"13\":1}}],[\"所求出的\",{\"1\":{\"53\":1}}],[\"所有可能动作的\",{\"1\":{\"35\":1}}],[\"所有状态的集合\",{\"1\":{\"19\":1}}],[\"所获得的均值\",{\"1\":{\"35\":1}}],[\"所得到的\",{\"1\":{\"27\":1}}],[\"所以\",{\"1\":{\"35\":1}}],[\"所以状态其实共有\",{\"1\":{\"13\":1}}],[\"所以在无人机辅助通信网络中我们需要考虑qoe模型\",{\"1\":{\"10\":1}}],[\"eplison\",{\"0\":{\"72\":1}}],[\"episodestartingfrom\",{\"1\":{\"68\":4}}],[\"episodes\",{\"1\":{\"65\":1,\"69\":2,\"71\":2}}],[\"episode\",{\"0\":{\"68\":1},\"1\":{\"19\":2,\"65\":2,\"68\":6,\"69\":1,\"71\":1,\"73\":1}}],[\"efficient\",{\"1\":{\"68\":1}}],[\"every\",{\"1\":{\"68\":2}}],[\"evaluation\",{\"1\":{\"32\":1,\"34\":1,\"53\":1,\"57\":1,\"66\":1,\"69\":1}}],[\"estimation\",{\"0\":{\"86\":1,\"90\":1},\"1\":{\"62\":2,\"64\":1,\"69\":1,\"79\":1}}],[\"elementwise\",{\"1\":{\"36\":1,\"42\":1,\"50\":1,\"53\":2}}],[\"euqation\",{\"1\":{\"34\":1}}],[\"e\",{\"1\":{\"30\":2,\"31\":4,\"35\":2,\"62\":1,\"81\":1,\"85\":2,\"86\":3,\"88\":1,\"91\":2}}],[\"equation\",{\"0\":{\"29\":1,\"32\":1,\"33\":1,\"41\":1},\"1\":{\"24\":1,\"32\":1,\"33\":2,\"36\":2,\"39\":1,\"45\":1,\"53\":1,\"55\":2,\"57\":1,\"58\":1,\"64\":1}}],[\"environment\",{\"1\":{\"19\":2}}],[\"exploration\",{\"1\":{\"74\":1}}],[\"exploring\",{\"0\":{\"67\":1,\"70\":1,\"71\":1},\"1\":{\"67\":1,\"71\":2,\"72\":1,\"75\":1}}],[\"exploitation\",{\"1\":{\"74\":1}}],[\"exponentially\",{\"1\":{\"45\":1}}],[\"expected\",{\"1\":{\"27\":1}}],[\"expection\",{\"1\":{\"27\":1}}],[\"experience\",{\"0\":{\"10\":1},\"1\":{\"4\":1,\"67\":1}}],[\"existence\",{\"1\":{\"45\":1}}],[\"excellent\",{\"1\":{\"10\":1}}],[\"测试阶段\",{\"1\":{\"14\":1}}],[\"训练阶段\",{\"1\":{\"14\":1}}],[\"与\",{\"0\":{\"28\":1,\"57\":1},\"1\":{\"28\":1,\"55\":1,\"66\":1}}],[\"与基于q\",{\"1\":{\"14\":1}}],[\"与k\",{\"1\":{\"5\":1}}],[\"基本形式\",{\"0\":{\"42\":1}}],[\"基本概念\",{\"0\":{\"17\":1,\"19\":1}}],[\"基本设置\",{\"0\":{\"8\":1}}],[\"基于q\",{\"1\":{\"14\":1}}],[\"后\",{\"1\":{\"35\":1,\"69\":2}}],[\"后续可能是未知的\",{\"1\":{\"32\":1}}],[\"后四个方向\",{\"1\":{\"14\":1}}],[\"后退\",{\"1\":{\"13\":1}}],[\"前\",{\"1\":{\"14\":1}}],[\"前进\",{\"1\":{\"13\":1}}],[\"右\",{\"1\":{\"14\":1}}],[\"右转\",{\"1\":{\"13\":1}}],[\"wargmin​j\",{\"1\":{\"88\":1}}],[\"walk\",{\"1\":{\"14\":2}}],[\"w−e\",{\"1\":{\"86\":1}}],[\"w∗\",{\"1\":{\"85\":2}}],[\"w∈r\",{\"1\":{\"83\":1}}],[\"w\",{\"1\":{\"83\":3,\"84\":2,\"85\":3,\"86\":5,\"88\":5,\"91\":15}}],[\"wk−1​\",{\"1\":{\"85\":1}}],[\"wk​\",{\"1\":{\"84\":6,\"85\":1,\"86\":1,\"88\":6,\"91\":12}}],[\"wk​−xk​\",{\"1\":{\"81\":2,\"86\":1}}],[\"wk​+xk​\",{\"1\":{\"81\":1}}],[\"wk​=k−11​i=1∑k−1​xi​\",{\"1\":{\"81\":1}}],[\"wk+1​=wk​−αk​e\",{\"1\":{\"91\":1}}],[\"wk+1​=wk​−αk​▽w​f\",{\"1\":{\"88\":1}}],[\"wk+1​=wk​−αk​▽w​e\",{\"1\":{\"88\":1}}],[\"wk+1​=wk​−αk​n1​i=1∑n​▽w​f\",{\"1\":{\"88\":1}}],[\"wk+1​=wk​−αk​g​\",{\"1\":{\"86\":1,\"91\":1}}],[\"wk+1​=wk​−ak​g​\",{\"1\":{\"84\":1}}],[\"wk+1​=wk​−k1​\",{\"1\":{\"81\":1}}],[\"wk+1​=k1​i=1∑k​xi​\",{\"1\":{\"81\":1}}],[\"wk+1​​=k1​∑i=1k​xi​​=k1​\",{\"1\":{\"81\":1}}],[\"wk+1​可以由wk​推导出来\",{\"1\":{\"81\":1}}],[\"why\",{\"0\":{\"34\":1}}],[\"where\",{\"1\":{\"33\":1,\"45\":1}}],[\"when\",{\"1\":{\"19\":1}}],[\"with\",{\"1\":{\"10\":1,\"19\":2,\"85\":1}}],[\"wireless\",{\"1\":{\"4\":1}}],[\"如\",{\"1\":{\"66\":1,\"69\":1}}],[\"如何估计\",{\"0\":{\"65\":1}}],[\"如何将\",{\"1\":{\"63\":1}}],[\"如何在没有模型\",{\"1\":{\"62\":1}}],[\"如何确保策略\",{\"1\":{\"55\":1}}],[\"如何通过\",{\"1\":{\"55\":1}}],[\"如何处理等式右边的\",{\"0\":{\"44\":1}}],[\"如何求解\",{\"0\":{\"43\":1}}],[\"如何得到最优策略\",{\"1\":{\"40\":1}}],[\"如可以在target\",{\"1\":{\"19\":1}}],[\"如果\",{\"1\":{\"83\":1}}],[\"如果对于\",{\"1\":{\"83\":1}}],[\"如果ϵ=1\",{\"1\":{\"74\":1}}],[\"如果经过了\",{\"1\":{\"71\":1}}],[\"如果从其他状态出发\",{\"1\":{\"71\":1}}],[\"如果存在一个\",{\"1\":{\"71\":1}}],[\"如果该\",{\"1\":{\"68\":1}}],[\"如果γ接近0\",{\"1\":{\"19\":1}}],[\"如果agent在当前时刻t所执行的动作能够提高总mos\",{\"1\":{\"13\":1}}],[\"如p\",{\"1\":{\"19\":1}}],[\"如a\",{\"1\":{\"14\":1}}],[\"需考虑用户的移动性\",{\"1\":{\"14\":1}}],[\"需要求解一个优化问题\",{\"1\":{\"88\":1}}],[\"需要从\",{\"1\":{\"64\":1}}],[\"需要初始化猜测的\",{\"1\":{\"57\":1}}],[\"需要初始化策略π0​\",{\"1\":{\"57\":1}}],[\"需要确定几件事\",{\"1\":{\"40\":1}}],[\"需要通过采样解决\",{\"1\":{\"32\":1}}],[\"需要推导e\",{\"1\":{\"29\":1}}],[\"需要调整相应无人机的高度\",{\"1\":{\"9\":1}}],[\"需要合理选择无人机n的垂直高度hn​\",{\"1\":{\"9\":1}}],[\"∑k=1∞​ak2​=∞\",{\"1\":{\"85\":1}}],[\"∑k=1∞​ak2​<∞\",{\"1\":{\"85\":1}}],[\"∑k=1∞​ak​=∞\",{\"1\":{\"85\":1}}],[\"∑kn​=1kn​​pkn​​\",{\"1\":{\"11\":1,\"13\":1}}],[\"∑i=1k−1​xi​+xk​\",{\"1\":{\"81\":1}}],[\"∑​pπ​\",{\"1\":{\"33\":1}}],[\"∑​p\",{\"1\":{\"31\":2,\"32\":2,\"35\":1,\"42\":1,\"50\":1,\"53\":2,\"64\":1}}],[\"∑​vπ​\",{\"1\":{\"31\":2}}],[\"∑​e\",{\"1\":{\"31\":3}}],[\"∑n=1n​∣kn​∣是总用户数\",{\"1\":{\"14\":1}}],[\"则只是进行一次带入求解\",{\"1\":{\"57\":1}}],[\"则是对多个\",{\"1\":{\"28\":1}}],[\"则总动作空间的大小为7+2n∑n=1n​∣kn​∣会导致动作空间过大\",{\"1\":{\"14\":1}}],[\"则用户的关联动作数为2n∑n=1n​∣kn​∣\",{\"1\":{\"14\":1}}],[\"则无人机将获得正奖励\",{\"1\":{\"13\":1}}],[\"设无人机总数为n\",{\"1\":{\"14\":1}}],[\"设计一种基于\",{\"1\":{\"5\":2}}],[\"对应收敛性证明\",{\"1\":{\"91\":1}}],[\"对应一个向量\",{\"1\":{\"45\":1}}],[\"对应策略表示为\",{\"1\":{\"44\":1}}],[\"对应的vk+1​\",{\"1\":{\"50\":1}}],[\"对应的动作\",{\"1\":{\"44\":1}}],[\"对应的\",{\"1\":{\"29\":2,\"33\":1,\"53\":1,\"81\":1}}],[\"对应的discounted\",{\"1\":{\"26\":1}}],[\"对应状态中所有可选择的动作集合\",{\"1\":{\"19\":1}}],[\"对应s就是uav的部署位置\",{\"1\":{\"13\":1}}],[\"对于观测值g​\",{\"1\":{\"91\":1}}],[\"对于这个问题\",{\"1\":{\"88\":1}}],[\"对于函数\",{\"1\":{\"88\":1}}],[\"对于一个\",{\"1\":{\"68\":1}}],[\"对于贝尔曼最优公式而言\",{\"1\":{\"42\":1,\"43\":1}}],[\"对于\",{\"1\":{\"35\":1,\"57\":3,\"64\":2,\"66\":1,\"68\":1,\"75\":1}}],[\"对于所有状态s\",{\"1\":{\"33\":1}}],[\"对于不同的策略\",{\"1\":{\"27\":1}}],[\"对于每个\",{\"1\":{\"68\":1}}],[\"对于每个智能体\",{\"1\":{\"13\":1}}],[\"对于每一个状态\",{\"1\":{\"19\":1}}],[\"对于动作空间而言\",{\"1\":{\"14\":1}}],[\"对于用户kn​存在特定的信噪比目标γkn​​\",{\"1\":{\"9\":1}}],[\"对于用户kn​∈kn​\",{\"1\":{\"8\":1}}],[\"对于无人机的总发射功率也均匀地分配给每个用户\",{\"1\":{\"9\":1}}],[\"对于无人机n\",{\"1\":{\"8\":1,\"9\":1}}],[\"对于指定区域\",{\"1\":{\"8\":1}}],[\"值迭代和策略迭代\",{\"0\":{\"48\":1}}],[\"值\",{\"1\":{\"13\":1}}],[\"最大的\",{\"1\":{\"64\":1,\"66\":1}}],[\"最大的q\",{\"1\":{\"13\":1}}],[\"最简单的示例算法\",{\"1\":{\"63\":1}}],[\"最优策略\",{\"0\":{\"44\":1}}],[\"最优策略是\",{\"1\":{\"40\":1}}],[\"最优策略是否唯一\",{\"1\":{\"40\":1}}],[\"最优策略是否存在\",{\"1\":{\"40\":1}}],[\"最优策略的定义\",{\"1\":{\"40\":1}}],[\"最终可以证明\",{\"1\":{\"34\":1}}],[\"最终输出结果\",{\"1\":{\"13\":1}}],[\"最终输出的结果\",{\"1\":{\"13\":1}}],[\"当a为静止时\",{\"1\":{\"13\":1}}],[\"当执行动作at​时\",{\"1\":{\"13\":1}}],[\"发现在该位置静止是最优的\",{\"1\":{\"13\":1}}],[\"发送端的时间\",{\"1\":{\"10\":1}}],[\"不记录\",{\"1\":{\"68\":1}}],[\"不动点x∗是唯一的\",{\"1\":{\"45\":1}}],[\"不常用\",{\"1\":{\"34\":1}}],[\"不同的无人机agent的q\",{\"1\":{\"13\":1}}],[\"不能用同一个q\",{\"1\":{\"13\":1}}],[\"还可以估计q\",{\"1\":{\"68\":1}}],[\"还需要求解最优策略π\",{\"1\":{\"42\":1}}],[\"还需要考虑所有用户的2d位置\",{\"1\":{\"14\":1}}],[\"还是\",{\"1\":{\"40\":1,\"62\":1,\"79\":1}}],[\"还是同一张q\",{\"1\":{\"13\":1}}],[\"还与los的概率有关\",{\"1\":{\"13\":1}}],[\"但能否保证其精确度\",{\"1\":{\"88\":1}}],[\"但需要大量的\",{\"1\":{\"88\":1}}],[\"但由于\",{\"1\":{\"88\":1}}],[\"但实际上我们往往是选择一个非常小的常数\",{\"1\":{\"85\":1}}],[\"但目前无法保证\",{\"1\":{\"71\":1}}],[\"但存在一定的浪费\",{\"1\":{\"68\":1}}],[\"但现实场景中不太经常使用\",{\"1\":{\"63\":1}}],[\"但若考虑集群情况\",{\"1\":{\"14\":1}}],[\"但每个无人机所管理的用户不同\",{\"1\":{\"13\":1}}],[\"但即使仅考虑用户聚类\",{\"1\":{\"13\":1}}],[\"否则\",{\"1\":{\"13\":1}}],[\"奖励\",{\"1\":{\"13\":1,\"19\":1}}],[\"并且如果按照这样一个策略\",{\"1\":{\"73\":1}}],[\"并且与状态\",{\"1\":{\"35\":1}}],[\"并非只是一个单一的轨迹\",{\"1\":{\"19\":1}}],[\"并获得奖励rt​的这一过程可以用条件转移概率p\",{\"1\":{\"13\":1}}],[\"并与传统的基于遗传的学习算法进行对比\",{\"1\":{\"5\":1}}],[\"静止\",{\"1\":{\"13\":1}}],[\"下行\",{\"1\":{\"13\":1}}],[\"上行\",{\"1\":{\"13\":1}}],[\"左转\",{\"1\":{\"13\":1}}],[\"共分为\",{\"1\":{\"50\":1}}],[\"共考虑7个方向\",{\"1\":{\"13\":1}}],[\"共划分5个等级\",{\"1\":{\"10\":1}}],[\"按照所给定策略j来执行一个动作at​∈a从而获得奖励rt​以及下一个状态st+1​\",{\"1\":{\"13\":1}}],[\"个人推导\",{\"1\":{\"31\":1}}],[\"个人感觉是这个\",{\"1\":{\"13\":1}}],[\"个人理解\",{\"1\":{\"13\":1,\"19\":1}}],[\"个\",{\"1\":{\"13\":1}}],[\"×\",{\"1\":{\"13\":2}}],[\"0<c1​≤▽w​g\",{\"1\":{\"85\":1}}],[\"0\",{\"1\":{\"13\":17,\"14\":1,\"19\":1,\"34\":1,\"74\":1,\"91\":1}}],[\"0≤t≤ts​\",{\"1\":{\"8\":2,\"11\":2}}],[\"状态转换\",{\"1\":{\"19\":1}}],[\"状态转换模型\",{\"1\":{\"13\":1}}],[\"状态除了要考虑无人机的3d位置外\",{\"1\":{\"14\":1}}],[\"状态空间\",{\"1\":{\"13\":1,\"19\":1}}],[\"状态\",{\"1\":{\"13\":1,\"19\":1}}],[\"智能体\",{\"1\":{\"13\":1}}],[\"来进行求解\",{\"1\":{\"64\":1}}],[\"来进行迭代\",{\"1\":{\"13\":1}}],[\"来提升当前策略\",{\"1\":{\"53\":1}}],[\"来表示\",{\"1\":{\"13\":1}}],[\"来最大化mos总和\",{\"1\":{\"13\":1}}],[\"来考虑无人机的机动性\",{\"1\":{\"4\":1}}],[\"目标是获得无人机的最佳3d位置\",{\"1\":{\"13\":1}}],[\"划分完毕\",{\"1\":{\"13\":1}}],[\"再次出现\",{\"1\":{\"68\":1}}],[\"再将\",{\"1\":{\"64\":1}}],[\"再将用户划分给距离最近的无人机\",{\"1\":{\"13\":1}}],[\"再求平均值\",{\"1\":{\"28\":1}}],[\"再根据欧几里得距离重新划分\",{\"1\":{\"13\":1}}],[\"再找到新的簇的各中心\",{\"1\":{\"13\":1}}],[\"描述如下\",{\"1\":{\"13\":1}}],[\"每次都是从对应的状态\",{\"1\":{\"71\":1}}],[\"每次迭代都会使得策略进行提升\",{\"1\":{\"55\":1}}],[\"每次无人机会根据当前状态st​∈s\",{\"1\":{\"13\":1}}],[\"每个用户的移动方向均匀分布在左\",{\"1\":{\"14\":1}}],[\"每个用户都需要判断是否与每个无人机关联\",{\"1\":{\"14\":1}}],[\"每个用户只能属于一个集群\",{\"1\":{\"8\":1}}],[\"每个集群中无人机的最优位置也会发生变化\",{\"1\":{\"14\":1}}],[\"每架无人机的带宽和发射功率都均匀分配给每个用户\",{\"1\":{\"13\":1}}],[\"将\",{\"1\":{\"45\":1,\"72\":1}}],[\"将所有状态的\",{\"1\":{\"33\":1}}],[\"将对应的轨迹所获得的所有reward的总和\",{\"1\":{\"19\":1}}],[\"将无人机部署在每个中心内\",{\"1\":{\"13\":1}}],[\"将上述优化问题简化\",{\"1\":{\"13\":1}}],[\"将其平均分配给其∣kn​∣个关联用户\",{\"1\":{\"9\":1}}],[\"解决方案\",{\"0\":{\"12\":1}}],[\"水平位置和高度\",{\"1\":{\"11\":1}}],[\"数量和位置\",{\"1\":{\"11\":1}}],[\"数据包从发送端\",{\"1\":{\"10\":1}}],[\"因为在\",{\"1\":{\"79\":1}}],[\"因为在不考虑用户自由穿梭集群的情况\",{\"1\":{\"14\":1}}],[\"因为最终策略更新的核心仍然是\",{\"1\":{\"66\":1}}],[\"因为\",{\"1\":{\"64\":1,\"91\":1}}],[\"因为无论是\",{\"1\":{\"62\":1}}],[\"因为其满足该理论\",{\"1\":{\"45\":1}}],[\"因为q\",{\"1\":{\"13\":1}}],[\"因为目标函数对于无人机的3d坐标是非凸的\",{\"1\":{\"11\":1}}],[\"因此我们可以通过\",{\"1\":{\"86\":1}}],[\"因此我们将优化问题简化为区域分割问题\",{\"1\":{\"13\":1}}],[\"因此是一个确定的贪心策略\",{\"1\":{\"75\":1}}],[\"因此是2n\",{\"1\":{\"14\":1}}],[\"因此随着用户位置的变化\",{\"1\":{\"14\":1}}],[\"因此mos不仅与欧氏距离有关\",{\"1\":{\"13\":1}}],[\"因此moskn​​delay\",{\"1\":{\"10\":1}}],[\"因此gak\",{\"1\":{\"13\":1}}],[\"因此对于用户kn​的在时刻t的传输速率rkn​​\",{\"1\":{\"9\":1}}],[\"因此\",{\"1\":{\"9\":2,\"10\":1,\"29\":1,\"35\":1,\"45\":1,\"58\":1,\"81\":1,\"88\":1,\"91\":2}}],[\"∀s∈s​\",{\"1\":{\"42\":1}}],[\"∀s∈s=πmax​a∑​π\",{\"1\":{\"42\":1}}],[\"∀s∈s\",{\"1\":{\"32\":1,\"45\":1}}],[\"∀kn​\",{\"1\":{\"11\":3,\"13\":3}}],[\"∀t\",{\"1\":{\"11\":4,\"13\":4}}],[\"∀n\",{\"1\":{\"11\":2,\"13\":2}}],[\"具体解决如下\",{\"1\":{\"84\":1}}],[\"具体求解方法\",{\"1\":{\"66\":1}}],[\"具体算法\",{\"0\":{\"66\":1}}],[\"具体步骤\",{\"0\":{\"50\":1}}],[\"具体分两步\",{\"1\":{\"43\":1}}],[\"具体代码\",{\"1\":{\"13\":1}}],[\"具体表述如下\",{\"1\":{\"11\":1}}],[\"具体如下\",{\"1\":{\"10\":1}}],[\"从\",{\"1\":{\"91\":1}}],[\"从给定的\",{\"1\":{\"69\":1}}],[\"从状态\",{\"1\":{\"65\":1}}],[\"从状态st​到st+1​\",{\"1\":{\"13\":1}}],[\"从指定的\",{\"1\":{\"65\":1}}],[\"从此可以发现\",{\"1\":{\"64\":1}}],[\"从而引入\",{\"1\":{\"92\":1}}],[\"从而转换为一个\",{\"1\":{\"91\":1}}],[\"从而进行多次利用\",{\"1\":{\"68\":1}}],[\"从而选择每个状态下最大的\",{\"1\":{\"64\":1}}],[\"从而最大化所有用户的总mos值\",{\"1\":{\"11\":1}}],[\"从无人机n到用户kn​的信道功率增益\",{\"1\":{\"9\":1}}],[\"本文目的是优化无人机在每个时隙的位置\",{\"1\":{\"11\":1}}],[\"是否成立\",{\"1\":{\"91\":1}}],[\"是否是收敛的\",{\"0\":{\"59\":1}}],[\"是需要被优化的参数\",{\"1\":{\"88\":1}}],[\"是第\",{\"1\":{\"84\":2}}],[\"是最优的\",{\"1\":{\"71\":1}}],[\"是最大报文长度\",{\"1\":{\"10\":1}}],[\"是针对\",{\"1\":{\"67\":1}}],[\"是针对一条trajectory所求的\",{\"1\":{\"28\":1}}],[\"是优于\",{\"1\":{\"55\":1}}],[\"是已知的\",{\"1\":{\"50\":1}}],[\"是依赖于策略π的\",{\"1\":{\"35\":1}}],[\"是由环境决定的\",{\"1\":{\"32\":1}}],[\"是一个期望值\",{\"1\":{\"88\":1}}],[\"是一个随机变量\",{\"1\":{\"88\":1}}],[\"是一个\",{\"1\":{\"84\":1}}],[\"是一个黑盒\",{\"1\":{\"84\":1}}],[\"是一个必要条件\",{\"1\":{\"71\":1}}],[\"是一个contraction\",{\"1\":{\"45\":1}}],[\"是一个有关状态s的函数\",{\"1\":{\"27\":1}}],[\"是一致的\",{\"1\":{\"28\":1}}],[\"是不同的\",{\"1\":{\"27\":1}}],[\"是基于一个给定策略\",{\"1\":{\"27\":1}}],[\"是\",{\"1\":{\"27\":1,\"36\":1,\"73\":1}}],[\"是根据q\",{\"1\":{\"13\":1}}],[\"是网页大小\",{\"1\":{\"10\":1}}],[\"是与传输速率有关的延迟时间\",{\"1\":{\"10\":1}}],[\"接收端\",{\"1\":{\"10\":1}}],[\"6746\",{\"1\":{\"10\":1}}],[\"取值范围从1−4\",{\"1\":{\"10\":1}}],[\"r→r\",{\"1\":{\"83\":1}}],[\"rm\",{\"0\":{\"82\":1},\"1\":{\"84\":1,\"86\":1,\"91\":3}}],[\"robbins\",{\"0\":{\"82\":1},\"1\":{\"85\":2}}],[\"root\",{\"1\":{\"79\":1,\"85\":1,\"91\":1}}],[\"r+s\",{\"1\":{\"53\":2}}],[\"r+γ∑s\",{\"1\":{\"35\":1}}],[\"r+γs\",{\"1\":{\"32\":1,\"35\":1,\"42\":1,\"50\":1}}],[\"rπ​+γpπ​v1​\",{\"1\":{\"57\":1}}],[\"rπ​+γpπ​vπ1​​\",{\"1\":{\"57\":1}}],[\"rπ​+γpπ​vπ0​​\",{\"1\":{\"57\":2}}],[\"rπ​+γpπ​vπk​​\",{\"1\":{\"53\":1,\"57\":1,\"64\":1}}],[\"rπ​+γpπ​vk​\",{\"1\":{\"49\":1,\"50\":1,\"57\":1}}],[\"rπ​+γpπ​v\",{\"1\":{\"42\":1,\"45\":1,\"48\":1}}],[\"rπ​\",{\"1\":{\"33\":2}}],[\"rπ​=\",{\"1\":{\"33\":1}}],[\"r​​+mean\",{\"1\":{\"32\":1}}],[\"r​\",{\"1\":{\"30\":1}}],[\"r∑​p\",{\"1\":{\"30\":1,\"32\":2,\"35\":1,\"42\":1,\"50\":1,\"53\":2}}],[\"rl7\",{\"0\":{\"96\":1}}],[\"rl\",{\"1\":{\"79\":1}}],[\"rl6\",{\"0\":{\"79\":1}}],[\"rl5\",{\"0\":{\"62\":1}}],[\"rl4\",{\"0\":{\"48\":1}}],[\"rl3\",{\"0\":{\"39\":1}}],[\"rl2\",{\"0\":{\"23\":1}}],[\"rl1\",{\"0\":{\"17\":1}}],[\"r∣s\",{\"1\":{\"20\":1,\"28\":1,\"30\":1,\"32\":3,\"35\":2,\"42\":1,\"50\":2,\"53\":2,\"62\":1,\"64\":3}}],[\"r\",{\"1\":{\"20\":3}}],[\"rate\",{\"1\":{\"19\":1,\"26\":1,\"45\":1}}],[\"random\",{\"1\":{\"14\":2,\"29\":1}}],[\"r=1∣s1​\",{\"1\":{\"19\":1}}],[\"refers\",{\"1\":{\"79\":1}}],[\"resulting\",{\"1\":{\"19\":1}}],[\"respect\",{\"1\":{\"19\":1}}],[\"returns\",{\"1\":{\"28\":1}}],[\"return为\",{\"1\":{\"26\":1}}],[\"return的描述\",{\"1\":{\"26\":1}}],[\"return越短视\",{\"1\":{\"19\":1}}],[\"return\",{\"0\":{\"28\":1},\"1\":{\"19\":2,\"26\":1,\"28\":3,\"29\":1,\"35\":2,\"65\":1,\"69\":1}}],[\"rewards\",{\"0\":{\"30\":1,\"31\":1},\"1\":{\"20\":1,\"32\":2}}],[\"reward\",{\"1\":{\"13\":1,\"19\":2,\"20\":2}}],[\"reinforcement\",{\"0\":{\"3\":1}}],[\"rt+1​∣st​=s\",{\"1\":{\"29\":2,\"30\":2,\"32\":1}}],[\"rt+1​∣at+1​\",{\"1\":{\"20\":1}}],[\"rt+1​+γgt+1​∣st​=s\",{\"1\":{\"29\":1}}],[\"rt+2​+γrt+3​+\",{\"1\":{\"29\":1}}],[\"rt​∣st​\",{\"1\":{\"13\":1}}],[\"rtt\",{\"1\":{\"10\":1}}],[\"rkn​​mss​\",{\"1\":{\"10\":1}}],[\"rkn​​\",{\"1\":{\"10\":3}}],[\"5\",{\"0\":{\"36\":1},\"1\":{\"10\":3,\"57\":1}}],[\"404\",{\"1\":{\"102\":1}}],[\"4\",{\"0\":{\"33\":1,\"35\":1,\"71\":1,\"86\":1,\"93\":1},\"1\":{\"10\":1,\"57\":1}}],[\"根据策略\",{\"1\":{\"65\":1}}],[\"根据策略π\",{\"1\":{\"35\":1}}],[\"根据对应的\",{\"1\":{\"53\":1}}],[\"根据\",{\"1\":{\"40\":2,\"49\":1,\"50\":1,\"53\":1,\"55\":1}}],[\"根据一个\",{\"1\":{\"29\":1}}],[\"根据所给定的用户划分情况\",{\"1\":{\"13\":1}}],[\"根据遗传算法找到cn​个最优个体作为簇的中心\",{\"1\":{\"13\":1}}],[\"根据n个用户\",{\"1\":{\"13\":1}}],[\"根据mos数值\",{\"1\":{\"10\":1}}],[\"根据香农定理\",{\"1\":{\"9\":1}}],[\"可知\",{\"1\":{\"9\":1}}],[\"可以参考\",{\"1\":{\"91\":1}}],[\"可以是标量\",{\"1\":{\"88\":1}}],[\"可以平衡\",{\"1\":{\"74\":1}}],[\"可以拆分为多个\",{\"1\":{\"68\":1}}],[\"可以通过\",{\"1\":{\"53\":1}}],[\"可以通过contraction\",{\"1\":{\"45\":1}}],[\"可以通过设置将episodic\",{\"1\":{\"19\":1}}],[\"可以互相转化\",{\"1\":{\"35\":1}}],[\"可以得到一个序列v0​\",{\"1\":{\"34\":1}}],[\"可以最开始均初始化为\",{\"1\":{\"34\":1}}],[\"可以求解\",{\"1\":{\"32\":1}}],[\"可以用来衡量一个状态的价值\",{\"1\":{\"27\":1}}],[\"可以粗步衡量一个策略的好坏\",{\"1\":{\"19\":1}}],[\"可以有限\",{\"1\":{\"19\":1}}],[\"可以忽略\",{\"1\":{\"10\":1}}],[\"可以表示为\",{\"1\":{\"9\":1}}],[\"可以减轻无人机对用户接收到的干扰\",{\"1\":{\"9\":1}}],[\"≤c2​\",{\"1\":{\"85\":1}}],[\"≤pmax​\",{\"1\":{\"11\":1,\"13\":1}}],[\"≤hmax​\",{\"1\":{\"11\":1,\"13\":1}}],[\"≤hn​\",{\"1\":{\"9\":1}}],[\"≤\",{\"1\":{\"9\":1}}],[\"ζ2​是系数\",{\"1\":{\"10\":1}}],[\"ζ1​\",{\"1\":{\"10\":1}}],[\"ζ+em\",{\"1\":{\"9\":1}}],[\"ζ是由环境决定的常数\",{\"1\":{\"9\":1}}],[\"1−∣a\",{\"1\":{\"74\":1}}],[\"10​a=ak∗​\",{\"1\":{\"50\":1,\"53\":1}}],[\"10​a=a∗a=a∗​\",{\"1\":{\"44\":1}}],[\"120和4\",{\"1\":{\"10\":1}}],[\"1\",{\"0\":{\"19\":1,\"25\":1,\"26\":2,\"27\":1,\"28\":1,\"30\":1,\"40\":1,\"42\":1,\"44\":1,\"49\":1,\"50\":2,\"51\":1,\"53\":1,\"57\":1,\"63\":1,\"64\":2,\"65\":1,\"66\":1,\"68\":1,\"73\":1,\"80\":1,\"81\":2,\"83\":1,\"88\":1},\"1\":{\"10\":1,\"13\":7,\"14\":1,\"19\":1,\"53\":1,\"57\":1,\"66\":2,\"74\":1,\"85\":2,\"88\":1}}],[\"1~2\",{\"1\":{\"10\":1}}],[\"180π​\",{\"1\":{\"9\":1}}],[\"1+σ2pkn​​gkn​​\",{\"1\":{\"9\":1}}],[\"1+ns​\",{\"1\":{\"9\":1}}],[\"表示对于对应\",{\"1\":{\"71\":1}}],[\"表示对于每一个\",{\"1\":{\"71\":1}}],[\"表示状态转移矩阵\",{\"1\":{\"33\":1}}],[\"表示一个给定的策略\",{\"1\":{\"32\":1}}],[\"表示为\",{\"1\":{\"27\":1}}],[\"表示为rkn​​\",{\"1\":{\"9\":1}}],[\"表示在各状态执行各动作的概率\",{\"1\":{\"20\":1}}],[\"表示在状态s下采取动作a\",{\"1\":{\"20\":2}}],[\"表示是最佳部署位置\",{\"1\":{\"13\":1}}],[\"表示\",{\"1\":{\"10\":1}}],[\"表示round\",{\"1\":{\"10\":1}}],[\"表示无人机与用户之间的仰角\",{\"1\":{\"9\":1}}],[\"且需要被求解出来\",{\"1\":{\"83\":1}}],[\"且最终会收敛到最优策略v∗\",{\"1\":{\"55\":1}}],[\"且\",{\"1\":{\"40\":1,\"74\":1,\"85\":2}}],[\"且ζ1​+ζ2​=1\",{\"1\":{\"10\":1}}],[\"且传输率永远都不可能超过信道容量c\",{\"1\":{\"9\":1}}],[\"且无人机向关联用户的发射功率是恒定的\",{\"1\":{\"9\":1}}],[\"信道容量c=b∗log\",{\"1\":{\"9\":1}}],[\"信号模型\",{\"0\":{\"9\":1}}],[\"由无人机的位置和它们在最后时隙采取的动作决定\",{\"1\":{\"14\":1}}],[\"由用户的初始位置和运动模型决定\",{\"1\":{\"14\":1}}],[\"由于πk+1​是\",{\"1\":{\"50\":1}}],[\"由于\",{\"1\":{\"35\":1,\"50\":1}}],[\"由于用户在每个时隙都处于漫游状态\",{\"1\":{\"14\":1}}],[\"由于gak\",{\"1\":{\"13\":1}}],[\"由于特定用户的mos与该用户与无人机之间的距离有关\",{\"1\":{\"13\":1}}],[\"由于不同用户对于传输速率的需求是不同的\",{\"1\":{\"10\":1}}],[\"由于不同集群的频谱不同\",{\"1\":{\"9\":1}}],[\"由此进行迭代\",{\"1\":{\"53\":1}}],[\"由此可以根据contraction\",{\"1\":{\"45\":1}}],[\"由此可以推导出一个多步的trajectory\",{\"1\":{\"26\":1}}],[\"由此\",{\"1\":{\"9\":1}}],[\"同理\",{\"1\":{\"19\":1}}],[\"同样地\",{\"1\":{\"35\":1}}],[\"同样可以用条件概率的形式进行描述\",{\"1\":{\"19\":1}}],[\"同样\",{\"1\":{\"9\":1,\"86\":1}}],[\"同一无人机通过fdma同时为同一集群中的多个用户提供服务\",{\"1\":{\"8\":1}}],[\"μnlos​\",{\"1\":{\"9\":1}}],[\"μnlos​是表示los和nlos链路的衰减因子\",{\"1\":{\"9\":1}}],[\"μlos​−μnlos​\",{\"1\":{\"9\":1}}],[\"μlos​\",{\"1\":{\"9\":1}}],[\"常数\",{\"1\":{\"9\":1}}],[\"α是表示路径损耗指数\",{\"1\":{\"9\":1}}],[\"g~​\",{\"1\":{\"91\":1}}],[\"gd\",{\"1\":{\"88\":1,\"91\":1}}],[\"gradient\",{\"0\":{\"87\":1},\"1\":{\"88\":2}}],[\"greedy\",{\"0\":{\"72\":1,\"74\":1,\"75\":1},\"1\":{\"40\":1,\"50\":2,\"53\":1,\"57\":2,\"66\":1,\"74\":3}}],[\"g​\",{\"1\":{\"84\":2,\"86\":2}}],[\"gpi\",{\"1\":{\"69\":1}}],[\"generalized\",{\"1\":{\"69\":1}}],[\"get\",{\"1\":{\"35\":2}}],[\"g\",{\"1\":{\"65\":3,\"83\":4,\"84\":2,\"85\":1,\"86\":2,\"91\":1}}],[\"gt+1​∣st+1​=s\",{\"1\":{\"31\":2}}],[\"gt+1​∣st​=s\",{\"1\":{\"29\":2,\"31\":5,\"32\":1}}],[\"gt​​=rt+1​+γrt+2​+γ2rt+3​+\",{\"1\":{\"29\":1}}],[\"gt​∣st​=s\",{\"1\":{\"27\":1,\"29\":1,\"35\":3,\"36\":2,\"62\":2,\"64\":1,\"65\":1}}],[\"gt​\",{\"1\":{\"27\":2,\"29\":1,\"65\":1}}],[\"gt​也是一个随机变量\",{\"1\":{\"26\":1}}],[\"gt​=rt+1​+γrt+2​+γ2rt+3​+\",{\"1\":{\"26\":1}}],[\"gt​=e\",{\"1\":{\"13\":1}}],[\"good\",{\"1\":{\"10\":1}}],[\"gkn​​\",{\"1\":{\"9\":1}}],[\"gain\",{\"1\":{\"9\":1}}],[\"class\",{\"1\":{\"79\":1}}],[\"closed\",{\"1\":{\"34\":1}}],[\"code\",{\"0\":{\"108\":1},\"2\":{\"100\":1}}],[\"coefficient\",{\"1\":{\"84\":1}}],[\"comments\",{\"1\":{\"57\":1}}],[\"core\",{\"1\":{\"39\":1}}],[\"converges\",{\"1\":{\"85\":1}}],[\"convergence\",{\"1\":{\"45\":1}}],[\"convex问题\",{\"1\":{\"11\":1}}],[\"consider\",{\"1\":{\"45\":1}}],[\"considered\",{\"1\":{\"4\":1}}],[\"contractive\",{\"1\":{\"45\":1}}],[\"contraction\",{\"1\":{\"40\":2,\"45\":2,\"49\":1}}],[\"continuing\",{\"1\":{\"19\":1}}],[\"concepts\",{\"1\":{\"39\":1}}],[\"carlo\",{\"0\":{\"62\":1},\"1\":{\"62\":1}}],[\"can\",{\"1\":{\"28\":1,\"35\":2}}],[\"called\",{\"1\":{\"19\":1,\"68\":1,\"73\":1}}],[\"choose\",{\"1\":{\"20\":1}}],[\"chapter\",{\"1\":{\"49\":1,\"55\":1}}],[\"chain\",{\"1\":{\"19\":1}}],[\"channel\",{\"1\":{\"9\":1}}],[\"cmax​\",{\"1\":{\"14\":1}}],[\"c\",{\"1\":{\"11\":1,\"13\":1}}],[\"cycles\",{\"1\":{\"10\":1}}],[\"c1​和c2​是通过分析web浏览应用程序的实验结果确定的常数\",{\"1\":{\"10\":1}}],[\"c是光速\",{\"1\":{\"9\":1}}],[\"c4πfc​​\",{\"1\":{\"9\":1}}],[\"为一个函数方程\",{\"1\":{\"83\":1}}],[\"为状态\",{\"1\":{\"74\":1}}],[\"为什么不去求\",{\"1\":{\"66\":1}}],[\"为什么考虑\",{\"1\":{\"62\":1}}],[\"为什么这个迭代算法最终可以找到最优策略\",{\"1\":{\"55\":1}}],[\"为discounted\",{\"1\":{\"26\":1}}],[\"为t时刻的mos评分\",{\"1\":{\"10\":1}}],[\"为了让右边取到最大值的情况\",{\"1\":{\"44\":1}}],[\"为了进行\",{\"1\":{\"34\":1}}],[\"为了应对具有无限步的trajectory的return=∞的情况\",{\"1\":{\"19\":1}}],[\"为了保证所有用户都能连接到网络\",{\"1\":{\"9\":1}}],[\"为了满足不同用户传输速率要求\",{\"1\":{\"9\":1}}],[\"为了在los信道概率和路径损耗之间取得平衡\",{\"1\":{\"9\":1}}],[\"为\",{\"1\":{\"9\":1,\"29\":2,\"33\":1,\"65\":1}}],[\"为指标\",{\"1\":{\"5\":1}}],[\"​≐g\",{\"1\":{\"86\":1}}],[\"​forthegreedyaction\",{\"1\":{\"74\":1}}],[\"​vu​u2​pu​\",{\"1\":{\"57\":1}}],[\"​vu​u1​pu​π2\",{\"1\":{\"57\":1}}],[\"​p\",{\"1\":{\"35\":1}}],[\"​​+η∇w​f\",{\"1\":{\"91\":1}}],[\"​​π\",{\"1\":{\"35\":1}}],[\"​​=a∑​qπ​\",{\"1\":{\"35\":1}}],[\"​​\",{\"1\":{\"32\":1,\"53\":1,\"91\":2}}],[\"​=∇w​f\",{\"1\":{\"91\":1}}],[\"​=w−x​=w−x+e\",{\"1\":{\"86\":1}}],[\"​=wk​−k1​\",{\"1\":{\"81\":1}}],[\"​=argmaxπ​\",{\"1\":{\"57\":1}}],[\"​=a∑​π\",{\"1\":{\"30\":1,\"31\":1}}],[\"​=rπk​​+γpπk​​vπk​\",{\"1\":{\"55\":1}}],[\"​=πmax​a∑​π\",{\"1\":{\"42\":1}}],[\"​=s\",{\"1\":{\"31\":1}}],[\"​=e\",{\"1\":{\"29\":1,\"32\":1}}],[\"​=ϕ\",{\"1\":{\"8\":1,\"11\":1,\"13\":1}}],[\"​ifmosnew​>mosold​ifmosnew​=mosold​ifmosnew​<mosold​​\",{\"1\":{\"13\":1}}],[\"​≥0\",{\"1\":{\"11\":1,\"13\":1}}],[\"​≥γkn​​\",{\"1\":{\"11\":1,\"13\":1}}],[\"​−μlos​−μnlos​μnlos​​​s\",{\"1\":{\"9\":1}}],[\"​\",{\"1\":{\"9\":3,\"10\":1,\"11\":1,\"13\":1,\"29\":1,\"31\":2,\"32\":1,\"33\":1,\"35\":2,\"42\":1,\"50\":1,\"53\":5,\"55\":1,\"64\":1,\"68\":2,\"74\":1,\"81\":1,\"91\":2}}],[\"​hn​\",{\"1\":{\"9\":1}}],[\"bgd\",{\"0\":{\"93\":1},\"1\":{\"88\":1}}],[\"batch\",{\"1\":{\"88\":1}}],[\"based\",{\"0\":{\"75\":1},\"1\":{\"64\":1,\"69\":1}}],[\"basic\",{\"0\":{\"63\":1},\"1\":{\"67\":1,\"68\":1,\"75\":1}}],[\"broad\",{\"1\":{\"79\":1}}],[\"boe\",{\"0\":{\"41\":1},\"1\":{\"39\":1,\"45\":1}}],[\"bootstrapping\",{\"1\":{\"32\":1}}],[\"bit\",{\"1\":{\"10\":2}}],[\"bkn​​=bn​\",{\"1\":{\"9\":1}}],[\"b2​\",{\"1\":{\"9\":1}}],[\"b2​pnlos​=1−plos​\",{\"1\":{\"9\":1}}],[\"b1​\",{\"1\":{\"9\":2}}],[\"bellman\",{\"0\":{\"29\":1,\"32\":1,\"33\":1,\"41\":1},\"1\":{\"24\":1,\"33\":2,\"34\":1,\"36\":2,\"39\":1,\"53\":1,\"55\":2,\"57\":1,\"58\":1,\"64\":1}}],[\"be\",{\"1\":{\"4\":1,\"28\":1}}],[\"θkn​​\",{\"1\":{\"9\":1}}],[\"pair\",{\"1\":{\"66\":1,\"68\":6,\"69\":1,\"71\":2,\"73\":1}}],[\"pu\",{\"1\":{\"57\":1}}],[\"pi\",{\"1\":{\"53\":1,\"55\":1,\"57\":1}}],[\"pe\",{\"1\":{\"53\":2,\"55\":1,\"57\":1,\"64\":1}}],[\"periods\",{\"1\":{\"10\":1}}],[\"pπ​\",{\"1\":{\"33\":1}}],[\"pπ​∈rn×n\",{\"1\":{\"33\":1}}],[\"p\",{\"1\":{\"19\":1,\"20\":3,\"28\":2,\"31\":1,\"32\":2,\"50\":2,\"62\":1,\"64\":4,\"85\":1}}],[\"problems\",{\"1\":{\"79\":1}}],[\"probability\",{\"1\":{\"19\":1,\"20\":4,\"73\":1,\"85\":1}}],[\"property\",{\"1\":{\"20\":1}}],[\"proposition1展示了无人机为相关用户提供可靠服务所需的高度的必要条件\",{\"1\":{\"9\":1}}],[\"proposition1\",{\"1\":{\"9\":1}}],[\"proposed\",{\"1\":{\"4\":1}}],[\"process\",{\"0\":{\"20\":1},\"1\":{\"20\":1}}],[\"pkn​\",{\"1\":{\"11\":1,\"13\":1}}],[\"pkn​​=pmax​\",{\"1\":{\"9\":1}}],[\"positive\",{\"1\":{\"73\":1,\"84\":1}}],[\"possible\",{\"1\":{\"28\":1}}],[\"policies\",{\"1\":{\"72\":1,\"74\":2}}],[\"policyevaluation\",{\"1\":{\"64\":1}}],[\"policy\",{\"0\":{\"40\":1,\"52\":1,\"56\":1,\"57\":1,\"58\":1,\"59\":1,\"69\":1,\"73\":1,\"74\":1,\"75\":1},\"1\":{\"19\":2,\"20\":1,\"32\":1,\"34\":1,\"39\":1,\"40\":2,\"50\":3,\"53\":2,\"55\":1,\"56\":1,\"57\":11,\"58\":2,\"63\":1,\"64\":1,\"66\":4,\"69\":3,\"73\":2,\"74\":2,\"75\":1}}],[\"point\",{\"1\":{\"45\":1}}],[\"poor\",{\"1\":{\"10\":1}}],[\"power\",{\"1\":{\"9\":1}}],[\"pm​ax​\",{\"1\":{\"9\":1}}],[\"pmax​≥γσ2k0​dkn​​α\",{\"1\":{\"9\":1}}],[\"plos​μlos​+pnlos​μnlos​\",{\"1\":{\"9\":1}}],[\"plos​\",{\"1\":{\"9\":1}}],[\"−x\",{\"1\":{\"86\":1}}],[\"−xkn​​\",{\"1\":{\"8\":1}}],[\"−e\",{\"1\":{\"86\":1,\"91\":2}}],[\"−vπk​\",{\"1\":{\"53\":1}}],[\"−f\",{\"1\":{\"45\":1}}],[\"−0\",{\"1\":{\"13\":1}}],[\"−1rπk​​\",{\"1\":{\"55\":1}}],[\"−1rπ​\",{\"1\":{\"34\":1}}],[\"−1rπ​​\",{\"1\":{\"34\":1}}],[\"−1\",{\"1\":{\"9\":1,\"10\":2,\"13\":4}}],[\"−ykn​​\",{\"1\":{\"8\":1}}],[\"+η▽w​f\",{\"1\":{\"91\":1}}],[\"+η​\",{\"1\":{\"86\":1}}],[\"+ηk​\",{\"1\":{\"84\":1}}],[\"+γs\",{\"1\":{\"33\":1,\"64\":1}}],[\"+γe\",{\"1\":{\"29\":1,\"32\":1}}],[\"+rtt−rkn​​\",{\"1\":{\"10\":1}}],[\"+c2​\",{\"1\":{\"10\":1}}],[\"+ζ2​moskn​​rate\",{\"1\":{\"10\":1}}],[\"+\",{\"1\":{\"8\":1,\"86\":1}}],[\"yuser​\",{\"1\":{\"14\":2}}],[\"yuav​\",{\"1\":{\"13\":2,\"14\":2}}],[\"yd​+1\",{\"1\":{\"13\":1}}],[\"yd​\",{\"1\":{\"13\":1}}],[\"yn​\",{\"1\":{\"8\":2}}],[\"ykn​​\",{\"1\":{\"8\":1}}],[\"xi​\",{\"1\":{\"88\":2}}],[\"xk+1​=f\",{\"1\":{\"45\":1}}],[\"xk​→x∗\",{\"1\":{\"45\":1}}],[\"xk​\",{\"1\":{\"45\":2,\"88\":1,\"91\":1}}],[\"xkn​​\",{\"1\":{\"8\":1}}],[\"x∗\",{\"1\":{\"45\":1}}],[\"x=f\",{\"1\":{\"45\":1}}],[\"x2​\",{\"1\":{\"45\":1,\"62\":1}}],[\"x1​\",{\"1\":{\"45\":1}}],[\"x\",{\"1\":{\"45\":2,\"62\":1,\"81\":1,\"86\":10,\"88\":6,\"91\":17}}],[\"xuser​\",{\"1\":{\"14\":2}}],[\"xuav​\",{\"1\":{\"13\":1,\"14\":2}}],[\"xt​=⎩⎨⎧​1\",{\"1\":{\"13\":1}}],[\"xd​+1\",{\"1\":{\"13\":1}}],[\"xd​\",{\"1\":{\"13\":1}}],[\"xn​\",{\"1\":{\"8\":2,\"62\":1}}],[\"=▽w​j\",{\"1\":{\"91\":1}}],[\"=wk​−αk​▽w​f\",{\"1\":{\"91\":1}}],[\"=wk​−αk​e\",{\"1\":{\"88\":1}}],[\"=wk​−αk​\",{\"1\":{\"86\":1}}],[\"=g\",{\"1\":{\"84\":1,\"91\":1}}],[\"=0\",{\"1\":{\"83\":1,\"85\":2,\"86\":1,\"91\":2}}],[\"=k1​\",{\"1\":{\"81\":1}}],[\"=k0​−1dkn​​−α\",{\"1\":{\"9\":1}}],[\"=n1​∑i=1n​xi​\",{\"1\":{\"81\":1}}],[\"=1\",{\"1\":{\"66\":1}}],[\"=vπ0​​\",{\"1\":{\"57\":1}}],[\"=argmaxπ​∑a​π\",{\"1\":{\"66\":1}}],[\"=argmaxa​qπk​​\",{\"1\":{\"53\":1}}],[\"=argmaxa​qk​\",{\"1\":{\"50\":1}}],[\"=a∑​πk​\",{\"1\":{\"53\":1}}],[\"=a∑​π\",{\"1\":{\"30\":1,\"31\":3,\"32\":1,\"35\":1}}],[\"=πargmax​a∑​πk​\",{\"1\":{\"53\":1}}],[\"=πargmax​a∑​π\",{\"1\":{\"50\":1}}],[\"=πmax​\",{\"1\":{\"48\":1,\"49\":1}}],[\"=x∗\",{\"1\":{\"45\":1}}],[\"=x\",{\"1\":{\"45\":1}}],[\"=maxa​qk​\",{\"1\":{\"50\":1}}],[\"=maxπ​\",{\"1\":{\"45\":1}}],[\"=maxπ​∑a​π\",{\"1\":{\"44\":1}}],[\"=mean\",{\"1\":{\"32\":1}}],[\"=∑a​π\",{\"1\":{\"36\":1}}],[\"=∑a​qπ​\",{\"1\":{\"35\":1}}],[\"=∑r​p\",{\"1\":{\"35\":1}}],[\"=r∑​p\",{\"1\":{\"64\":1}}],[\"=rπ​\",{\"1\":{\"33\":1}}],[\"=rt+1​+γgt+1​​\",{\"1\":{\"29\":1}}],[\"=rt+1​+γ\",{\"1\":{\"29\":1}}],[\"=s\",{\"1\":{\"31\":3}}],[\"=sin−1\",{\"1\":{\"9\":1}}],[\"=e\",{\"1\":{\"27\":1,\"29\":2,\"35\":1,\"36\":2,\"62\":2,\"64\":1,\"65\":1,\"88\":1,\"91\":4}}],[\"=p\",{\"1\":{\"20\":2}}],[\"=3rtt+rkn​​\",{\"1\":{\"10\":1}}],[\"=−c1​ln\",{\"1\":{\"10\":1}}],[\"=ζ1​moskn​​delay\",{\"1\":{\"10\":1}}],[\"=γk0​σ2dkn​​α\",{\"1\":{\"9\":1}}],[\"=b2​ln\",{\"1\":{\"9\":1}}],[\"=bkn​​log2​\",{\"1\":{\"9\":1}}],[\"=b1​\",{\"1\":{\"9\":1}}],[\"=σ2pkn​​gkn​​\",{\"1\":{\"9\":1}}],[\"=\",{\"1\":{\"8\":1,\"19\":1,\"44\":1,\"50\":1,\"53\":1,\"74\":1,\"86\":1}}],[\"has\",{\"1\":{\"45\":1}}],[\"hard问题\",{\"1\":{\"13\":1}}],[\"hard\",{\"1\":{\"5\":1}}],[\"hybrid\",{\"1\":{\"14\":1}}],[\"huav​\",{\"1\":{\"13\":2,\"14\":2}}],[\"hmax​−hmin​+1\",{\"1\":{\"13\":1}}],[\"hmax​mostotal​=∑n=1n​∑kn​=1kn​​moskn​​\",{\"1\":{\"13\":1}}],[\"hmax​mostotal​=∑n=1n​∑kn​=1kn​​∑t=0ts​​moskn​​\",{\"1\":{\"11\":1}}],[\"hmax​\",{\"1\":{\"8\":1,\"13\":1}}],[\"hmin​≤hn​\",{\"1\":{\"11\":1,\"13\":1}}],[\"hmin​\",{\"1\":{\"8\":1,\"13\":1}}],[\"∈\",{\"1\":{\"8\":1}}],[\"飞行速度恒定\",{\"1\":{\"8\":1}}],[\"其定义都是一个均值\",{\"1\":{\"79\":1}}],[\"其探索性就很强\",{\"1\":{\"74\":1}}],[\"其属于\",{\"1\":{\"74\":1}}],[\"其原始定义都是从期望出发的\",{\"1\":{\"62\":1}}],[\"其核心思想是\",{\"1\":{\"62\":1}}],[\"其对应的\",{\"1\":{\"50\":1}}],[\"其策略π表示的是最优策略\",{\"1\":{\"42\":1}}],[\"其目标也应该不一样\",{\"1\":{\"13\":1}}],[\"其状态为其3d坐标\",{\"1\":{\"13\":1}}],[\"其高度的下界是距离dkn​​\",{\"1\":{\"9\":1}}],[\"其每个用户带宽表示为\",{\"1\":{\"9\":1}}],[\"其可用带宽为bn​\",{\"1\":{\"9\":1}}],[\"其水平坐标表示为qn​\",{\"1\":{\"8\":1}}],[\"其垂直高度表示为hn​\",{\"1\":{\"8\":1}}],[\"其坐标表示为wkn​​=\",{\"1\":{\"8\":1}}],[\"其中hk​=wk​\",{\"1\":{\"85\":1}}],[\"其中ak∗​=argmaxa​qπk​​\",{\"1\":{\"66\":1}}],[\"其中ak∗​\",{\"1\":{\"50\":1}}],[\"其中a∗表示在该状态下计算出来的最大\",{\"1\":{\"44\":1}}],[\"其中vk​是给定的\",{\"1\":{\"50\":1}}],[\"其中f\",{\"1\":{\"45\":1}}],[\"其中cmax​表示用户的最大速度\",{\"1\":{\"14\":1}}],[\"其中\",{\"1\":{\"9\":1,\"10\":2,\"33\":1,\"53\":1,\"74\":1,\"83\":1,\"84\":1,\"88\":1}}],[\"其中σ2=bkn​​n0​\",{\"1\":{\"9\":1}}],[\"其中k0​=\",{\"1\":{\"9\":1}}],[\"其中kn​表示划分到集群n的用户\",{\"1\":{\"8\":1}}],[\"其中θkn​​\",{\"1\":{\"9\":1}}],[\"其中用户表示为k=k1​\",{\"1\":{\"8\":1}}],[\"在之前关于使用\",{\"1\":{\"92\":1}}],[\"在这里\",{\"1\":{\"74\":1}}],[\"在收集到了足够多的\",{\"1\":{\"69\":1}}],[\"在求解\",{\"1\":{\"58\":1}}],[\"在策略更新上\",{\"1\":{\"57\":1}}],[\"在\",{\"1\":{\"55\":2,\"73\":1}}],[\"在当前状态s下\",{\"1\":{\"35\":1}}],[\"在当前状态s下采取动作\",{\"1\":{\"35\":1}}],[\"在policy是确定的情况下\",{\"1\":{\"20\":1}}],[\"在执行一个动作后获得的一个常数\",{\"1\":{\"19\":1}}],[\"在此情况下\",{\"1\":{\"14\":1}}],[\"在本文中\",{\"1\":{\"14\":1}}],[\"在本文中不考虑用户移动到其他集群的情况\",{\"1\":{\"14\":1}}],[\"在设计无人机的移动之前\",{\"1\":{\"14\":1}}],[\"在该文中考虑的是网页浏览应用传输情况\",{\"1\":{\"10\":1}}],[\"在该文中\",{\"1\":{\"10\":1}}],[\"在时刻t关联到无人机n的地面用户kn​的接受到的信噪比表示为\",{\"1\":{\"9\":1}}],[\"在时间t\",{\"1\":{\"9\":1}}],[\"在实际应用中\",{\"1\":{\"9\":1}}],[\"在任意时刻t\",{\"1\":{\"8\":1}}],[\"在初始时间假设用户处于静止下不断调整\",{\"1\":{\"5\":1}}],[\"20240814230747\",{\"1\":{\"93\":1}}],[\"20240814014058\",{\"1\":{\"90\":1}}],[\"20240812010538\",{\"1\":{\"76\":1}}],[\"20240812011140\",{\"1\":{\"75\":1}}],[\"20240812004534\",{\"1\":{\"70\":1}}],[\"20240811233346\",{\"1\":{\"66\":1}}],[\"20240811011334\",{\"1\":{\"59\":1}}],[\"20240811010933\",{\"1\":{\"58\":1}}],[\"20240811002219\",{\"1\":{\"54\":1}}],[\"20240810190018\",{\"1\":{\"51\":1}}],[\"2019\",{\"1\":{\"3\":1}}],[\"2种方法\",{\"1\":{\"36\":1}}],[\"2l−1\",{\"1\":{\"10\":1}}],[\"2mssfs​+1\",{\"1\":{\"10\":1}}],[\"2mss\",{\"1\":{\"10\":1}}],[\"2~3\",{\"1\":{\"10\":1}}],[\"2​\",{\"1\":{\"8\":1}}],[\"2+\",{\"1\":{\"8\":1}}],[\"2\",{\"0\":{\"20\":1,\"27\":1,\"29\":1,\"30\":1,\"31\":2,\"32\":1,\"33\":1,\"41\":1,\"42\":1,\"43\":2,\"44\":2,\"45\":1,\"51\":1,\"52\":1,\"53\":1,\"54\":2,\"55\":1,\"58\":1,\"65\":1,\"67\":1,\"68\":1,\"69\":2,\"70\":1,\"71\":1,\"74\":1,\"82\":1,\"83\":1,\"84\":2,\"85\":1,\"86\":1,\"89\":1},\"1\":{\"8\":1,\"9\":1,\"13\":1,\"14\":1,\"49\":1,\"50\":1,\"53\":1,\"55\":1,\"57\":1,\"66\":2,\"81\":1,\"84\":1,\"88\":1}}],[\"即函数\",{\"1\":{\"84\":1}}],[\"即不需要完全精确地求出\",{\"1\":{\"69\":1}}],[\"即不具备terminal\",{\"1\":{\"19\":1}}],[\"即对于数据\",{\"1\":{\"67\":1}}],[\"即对于每个状态\",{\"1\":{\"66\":1}}],[\"即对于给定策略\",{\"1\":{\"34\":1}}],[\"即使先估计了\",{\"1\":{\"66\":1}}],[\"即p\",{\"1\":{\"62\":1}}],[\"即j→∞\",{\"1\":{\"53\":1}}],[\"即\",{\"1\":{\"50\":2,\"53\":2,\"55\":1,\"64\":1,\"65\":2,\"81\":1,\"86\":1}}],[\"即求解右边的式子\",{\"1\":{\"50\":1}}],[\"即f\",{\"1\":{\"45\":1}}],[\"即a∗=argmaxa​q\",{\"1\":{\"44\":1}}],[\"即可\",{\"1\":{\"44\":1,\"64\":1}}],[\"即vπ​\",{\"1\":{\"36\":1}}],[\"即一个trajectory下的discounted\",{\"1\":{\"26\":1}}],[\"即无记忆的特性\",{\"1\":{\"20\":1}}],[\"即无人机作为空中基站\",{\"1\":{\"8\":1}}],[\"即表示具有终止状态terminal\",{\"1\":{\"19\":1}}],[\"即此时的discounted\",{\"1\":{\"19\":1}}],[\"即在状态s1​下采用动作a1​获得的奖励r=1的概率\",{\"1\":{\"19\":1}}],[\"即状态s1​采用动作a1​转到状态s2​的概率\",{\"1\":{\"19\":1}}],[\"即ξ=\",{\"1\":{\"14\":1}}],[\"即xuav​\",{\"1\":{\"13\":1}}],[\"即γ≥γkn​​\",{\"1\":{\"9\":1}}],[\"即需要考虑地面不同用户的具体需求\",{\"1\":{\"4\":1}}],[\"large\",{\"1\":{\"62\":1}}],[\"law\",{\"1\":{\"62\":1}}],[\"l2​=log2​\",{\"1\":{\"10\":1}}],[\"l2​\",{\"1\":{\"10\":1}}],[\"l1​=log2​\",{\"1\":{\"10\":1}}],[\"l1​\",{\"1\":{\"10\":1}}],[\"l=min\",{\"1\":{\"10\":1}}],[\"link\",{\"1\":{\"8\":1}}],[\"learning是优化长期目标\",{\"1\":{\"13\":1}}],[\"learning的部署算法不同的是\",{\"1\":{\"14\":1}}],[\"learning的移动算法\",{\"1\":{\"14\":1}}],[\"learning的优化目标是最大化长期收益\",{\"1\":{\"13\":1}}],[\"learning的方案来解决无人机的np\",{\"1\":{\"5\":1}}],[\"learning算法\",{\"1\":{\"13\":1}}],[\"learning\",{\"0\":{\"3\":1,\"96\":1},\"1\":{\"5\":2}}],[\"考虑能否仅用一次\",{\"1\":{\"88\":1}}],[\"考虑一个\",{\"1\":{\"68\":1}}],[\"考虑\",{\"1\":{\"57\":2}}],[\"考虑用户在每个时隙移动的情况\",{\"1\":{\"14\":1}}],[\"考虑以下场景\",{\"1\":{\"13\":1}}],[\"考虑无人机辅助无线网络的下行链路传输\",{\"1\":{\"8\":1}}],[\"考虑qoe\",{\"1\":{\"4\":1}}],[\"系统结构\",{\"0\":{\"7\":1}}],[\"的思想\",{\"1\":{\"88\":1}}],[\"的值\",{\"1\":{\"86\":1}}],[\"的动作数量\",{\"1\":{\"74\":1}}],[\"的高效利用\",{\"0\":{\"68\":1}}],[\"的一些改进\",{\"1\":{\"67\":1}}],[\"的一个观测值\",{\"1\":{\"91\":1}}],[\"的一个\",{\"1\":{\"65\":1}}],[\"的定义出发\",{\"1\":{\"64\":1}}],[\"的情况\",{\"1\":{\"63\":1,\"64\":2,\"91\":1}}],[\"的情况下进行估计\",{\"1\":{\"62\":1}}],[\"的原理\",{\"1\":{\"63\":1}}],[\"的方法\",{\"1\":{\"55\":1,\"72\":1}}],[\"的策略来进行选择\",{\"1\":{\"53\":1}}],[\"的策略πk+1​\",{\"1\":{\"50\":1}}],[\"的\",{\"1\":{\"50\":1,\"53\":1,\"64\":1,\"65\":2,\"68\":1,\"71\":1,\"73\":1,\"74\":1}}],[\"的形式\",{\"1\":{\"45\":1,\"86\":1}}],[\"的根据策略π加权平均\",{\"1\":{\"36\":1}}],[\"的加权均值\",{\"1\":{\"35\":1}}],[\"的转到下一个状态的\",{\"1\":{\"35\":1}}],[\"的过程\",{\"1\":{\"34\":1}}],[\"的计算即可\",{\"1\":{\"29\":1}}],[\"的区别\",{\"0\":{\"28\":1}}],[\"的期望\",{\"1\":{\"27\":1}}],[\"的概率\",{\"1\":{\"20\":2}}],[\"的s是有范围的\",{\"1\":{\"13\":1}}],[\"的函数\",{\"1\":{\"9\":1}}],[\"的功率谱密度\",{\"1\":{\"9\":1}}],[\"的无人机3d动态运动设计算法\",{\"1\":{\"5\":1}}],[\"的部署方法\",{\"1\":{\"5\":1}}],[\"提出解决总用户mos最大化问题的三步骤\",{\"1\":{\"5\":1}}],[\"提出了一个理想的由qoe驱动的多无人机协助通信框架\",{\"1\":{\"5\":1}}],[\"以及是否可以到最后优化的成果\",{\"1\":{\"88\":1}}],[\"以及\",{\"1\":{\"56\":1,\"75\":1}}],[\"以向用户提供可靠的服务\",{\"1\":{\"9\":1}}],[\"以\",{\"1\":{\"5\":1}}],[\"该方法成立的数学依据是\",{\"1\":{\"62\":1}}],[\"该算法是\",{\"1\":{\"56\":1}}],[\"该步骤是根据\",{\"1\":{\"53\":1}}],[\"该步骤是用来计算当前策略\",{\"1\":{\"53\":1}}],[\"该式子针对状态空间中的所有状态均成立\",{\"1\":{\"32\":1}}],[\"该论文中在精度和模型复杂型上作出平衡\",{\"1\":{\"13\":1}}],[\"该问题依然是np\",{\"1\":{\"13\":1}}],[\"该优化问题是一个non\",{\"1\":{\"11\":1}}],[\"该文中不同集群所利用的频谱是不同的\",{\"1\":{\"9\":1}}],[\"该文中表示为\",{\"1\":{\"9\":1}}],[\"该文提出的算法具较快的收敛性\",{\"1\":{\"5\":1}}],[\"该文基于q\",{\"1\":{\"5\":1}}],[\"该文设计的是3d部署\",{\"1\":{\"4\":1}}],[\"该框架将无人机部署在三维空间内\",{\"1\":{\"5\":1}}],[\"主要框架\",{\"1\":{\"18\":1}}],[\"主要内容\",{\"0\":{\"6\":1}}],[\"主要贡献\",{\"0\":{\"5\":1}}],[\"主要动机\",{\"0\":{\"4\":1}}],[\"过去研究主要考虑的是2d部署\",{\"1\":{\"4\":1}}],[\"过去研究大多没有基于用户的移动\",{\"1\":{\"4\":1}}],[\"sgdw\",{\"0\":{\"93\":1}}],[\"sgd的目标是\",{\"1\":{\"91\":1}}],[\"sgd\",{\"0\":{\"89\":1,\"91\":1,\"92\":1},\"1\":{\"88\":1,\"91\":3,\"92\":2}}],[\"s5​\",{\"1\":{\"68\":1}}],[\"s5​a1​​\",{\"1\":{\"68\":1}}],[\"s2​\",{\"1\":{\"68\":4}}],[\"s2​a3​​s5​a1​​\",{\"1\":{\"68\":1}}],[\"s2​a4​​s1​a2​​s2​a3​​s5​a1​​\",{\"1\":{\"68\":1}}],[\"s2​∣s1​\",{\"1\":{\"19\":1}}],[\"satisfying\",{\"1\":{\"85\":1}}],[\"satisfaction\",{\"1\":{\"4\":1}}],[\"sa\",{\"1\":{\"79\":2}}],[\"samples\",{\"1\":{\"88\":1}}],[\"sample\",{\"1\":{\"65\":1,\"86\":1,\"88\":1}}],[\"sequence\",{\"1\":{\"45\":1}}],[\"set\",{\"1\":{\"20\":3}}],[\"sets\",{\"1\":{\"20\":1}}],[\"s​=maxπ​∑a​π\",{\"1\":{\"45\":1}}],[\"s∣a\",{\"1\":{\"44\":1,\"45\":1}}],[\"soloving\",{\"1\":{\"79\":1}}],[\"solution\",{\"1\":{\"34\":2}}],[\"soft\",{\"0\":{\"73\":1},\"1\":{\"72\":1,\"73\":2,\"74\":1}}],[\"some\",{\"1\":{\"19\":1}}],[\"slove\",{\"0\":{\"34\":1}}],[\"slow\",{\"1\":{\"10\":1}}],[\"sj​∣si​\",{\"1\":{\"33\":1}}],[\"sn​\",{\"1\":{\"33\":2}}],[\"s0​\",{\"1\":{\"20\":2}}],[\"s∈s\",{\"1\":{\"20\":1,\"50\":1,\"53\":2}}],[\"s1​a2​​s2​a3​​s5​a1​​\",{\"1\":{\"68\":1}}],[\"s1​a2​​s2​a4​​s1​a2​​s2​a3​​s5​a1​​\",{\"1\":{\"68\":2}}],[\"s1​\",{\"1\":{\"33\":2,\"68\":4}}],[\"s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​r=1→​a2​​s9​r=1→​a2​​s9​\",{\"1\":{\"19\":1}}],[\"s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​\",{\"1\":{\"19\":1}}],[\"s1​→a1​s2​\",{\"1\":{\"19\":1}}],[\"si​\",{\"1\":{\"19\":2}}],[\"single\",{\"1\":{\"69\":1}}],[\"sin\",{\"1\":{\"9\":1}}],[\"s=\",{\"1\":{\"19\":1}}],[\"step\",{\"1\":{\"53\":2,\"66\":2}}],[\"st+2​→at+3​\",{\"1\":{\"26\":1,\"29\":1}}],[\"st+2​→at+2​rt+3​\",{\"1\":{\"26\":1,\"29\":1}}],[\"st+1​=s\",{\"1\":{\"31\":2}}],[\"st+1​→at+1​rt+2​\",{\"1\":{\"26\":1,\"29\":1}}],[\"st+1​∣at+1​\",{\"1\":{\"20\":3}}],[\"st+1​\",{\"1\":{\"13\":1}}],[\"st​→at​rt+1​\",{\"1\":{\"26\":1,\"29\":1}}],[\"st​\",{\"1\":{\"20\":4}}],[\"stochastic\",{\"0\":{\"87\":1},\"1\":{\"19\":1,\"40\":1,\"73\":1,\"79\":2}}],[\"stop\",{\"1\":{\"19\":1}}],[\"statrts的解释\",{\"0\":{\"71\":1}}],[\"status\",{\"1\":{\"19\":1}}],[\"states中\",{\"1\":{\"19\":1}}],[\"states中限制action\",{\"1\":{\"19\":1}}],[\"states的任务\",{\"1\":{\"19\":1}}],[\"states的trajectory\",{\"1\":{\"19\":1}}],[\"states\",{\"1\":{\"19\":1,\"20\":1}}],[\"state\",{\"0\":{\"25\":1,\"27\":1,\"28\":1,\"34\":1,\"45\":1},\"1\":{\"13\":2,\"19\":6,\"20\":4,\"24\":1,\"27\":4,\"28\":4,\"29\":1,\"32\":1,\"34\":1,\"35\":6,\"36\":2,\"39\":1,\"42\":1,\"43\":1,\"50\":1,\"53\":2,\"55\":1,\"57\":2,\"58\":1,\"62\":1,\"64\":3,\"66\":3,\"68\":6,\"69\":1,\"71\":2,\"73\":1,\"79\":1}}],[\"starts\",{\"0\":{\"67\":1,\"70\":1},\"1\":{\"67\":1,\"71\":2,\"72\":1}}],[\"starting\",{\"1\":{\"28\":1,\"35\":2}}],[\"start\",{\"1\":{\"10\":1}}],[\"space\",{\"1\":{\"13\":2,\"19\":3}}],[\"s\",{\"1\":{\"9\":1,\"10\":1,\"11\":1,\"13\":5,\"20\":5,\"27\":2,\"28\":1,\"29\":1,\"31\":10,\"32\":7,\"33\":4,\"35\":13,\"36\":4,\"40\":3,\"42\":4,\"44\":2,\"50\":11,\"53\":13,\"62\":3,\"64\":9,\"65\":8,\"66\":4,\"69\":1,\"71\":6,\"74\":6}}],[\"score\",{\"1\":{\"5\":1}}],[\"supposed\",{\"1\":{\"4\":1}}],[\"qπk​​\",{\"1\":{\"53\":1,\"64\":2,\"65\":2,\"66\":1}}],[\"qπ​\",{\"1\":{\"35\":3,\"36\":1,\"62\":1}}],[\"q\",{\"1\":{\"5\":2,\"11\":1,\"13\":2,\"14\":1,\"36\":1,\"42\":1,\"44\":1,\"45\":1,\"64\":1,\"66\":1,\"68\":2}}],[\"qoe\",{\"1\":{\"4\":1}}],[\"quality\",{\"0\":{\"10\":1},\"1\":{\"4\":1}}],[\"uses\",{\"1\":{\"69\":1}}],[\"users\",{\"1\":{\"4\":2}}],[\"u0​pu​π1\",{\"1\":{\"57\":1}}],[\"update\",{\"1\":{\"50\":3,\"57\":2}}],[\"uniqueness\",{\"1\":{\"45\":1}}],[\"unmanned\",{\"1\":{\"4\":1}}],[\"uavn\",{\"1\":{\"13\":1}}],[\"uavs\",{\"1\":{\"4\":1,\"5\":1}}],[\"uav\",{\"0\":{\"3\":1,\"104\":1},\"1\":{\"4\":1},\"2\":{\"16\":1}}],[\"other\",{\"1\":{\"40\":1}}],[\"optimization\",{\"1\":{\"79\":1}}],[\"optimality\",{\"0\":{\"41\":1},\"1\":{\"39\":1}}],[\"optimal\",{\"0\":{\"40\":1},\"1\":{\"39\":2,\"40\":1}}],[\"opinion\",{\"1\":{\"5\":1}}],[\"obtained\",{\"1\":{\"28\":1}}],[\"originalepisode\",{\"1\":{\"68\":1}}],[\"or\",{\"1\":{\"19\":1,\"27\":1,\"79\":1}}],[\"of\",{\"0\":{\"10\":1,\"30\":1,\"31\":1},\"1\":{\"4\":3,\"10\":1,\"19\":4,\"20\":3,\"27\":1,\"28\":1,\"32\":2,\"45\":1,\"62\":1,\"68\":1,\"69\":1,\"79\":1,\"86\":1}}],[\"on\",{\"1\":{\"3\":1},\"2\":{\"16\":1}}],[\"finding\",{\"1\":{\"79\":1,\"91\":1}}],[\"first\",{\"1\":{\"68\":1}}],[\"fix\",{\"1\":{\"45\":1}}],[\"fast\",{\"1\":{\"45\":1}}],[\"fair\",{\"1\":{\"10\":1}}],[\"f\",{\"1\":{\"45\":4,\"88\":3,\"91\":1}}],[\"fundamental\",{\"1\":{\"39\":1}}],[\"function\",{\"1\":{\"27\":1,\"45\":1}}],[\"future\",{\"0\":{\"31\":1},\"1\":{\"32\":1}}],[\"free\",{\"0\":{\"62\":1},\"1\":{\"32\":1,\"63\":1,\"64\":1}}],[\"from\",{\"1\":{\"28\":1,\"35\":2}}],[\"framework\",{\"1\":{\"4\":1}}],[\"found\",{\"1\":{\"102\":1}}],[\"foem\",{\"1\":{\"42\":1}}],[\"following\",{\"1\":{\"19\":1}}],[\"forallw\",{\"1\":{\"85\":1}}],[\"foralls∈s\",{\"1\":{\"66\":1}}],[\"fortheother∣a\",{\"1\":{\"74\":1}}],[\"formulation\",{\"0\":{\"92\":1}}],[\"form\",{\"0\":{\"33\":1},\"1\":{\"33\":1,\"34\":1,\"36\":2,\"42\":1,\"45\":1,\"50\":1,\"53\":2}}],[\"for\",{\"1\":{\"4\":2,\"20\":1,\"40\":2,\"45\":1}}],[\"fs\",{\"1\":{\"10\":1}}],[\"fs​+l\",{\"1\":{\"10\":1}}],[\"fc​是载波频率\",{\"1\":{\"9\":1}}],[\"not\",{\"1\":{\"102\":1}}],[\"novel\",{\"1\":{\"4\":1}}],[\"n=0∑∞​βnrt+n​\",{\"1\":{\"13\":1}}],[\"n∈n=\",{\"1\":{\"13\":1}}],[\"n∈1\",{\"1\":{\"8\":1}}],[\"numbers\",{\"1\":{\"62\":1}}],[\"number\",{\"1\":{\"10\":1}}],[\"n0​为用户所在位置的加性高斯白噪声\",{\"1\":{\"9\":1}}],[\"n\",{\"1\":{\"8\":2,\"11\":1,\"13\":2,\"57\":1,\"58\":1}}],[\"networks\",{\"0\":{\"3\":1},\"1\":{\"4\":1}}],[\"t∈rn\",{\"1\":{\"33\":2}}],[\"t∈r2×1\",{\"1\":{\"8\":2}}],[\"take\",{\"1\":{\"73\":1}}],[\"taking\",{\"1\":{\"35\":1}}],[\"tasks转换成continuing\",{\"1\":{\"19\":1}}],[\"tasks\",{\"1\":{\"19\":3}}],[\"table过大\",{\"1\":{\"14\":1}}],[\"table来找出对应q\",{\"1\":{\"13\":1}}],[\"table\",{\"1\":{\"13\":2}}],[\"table管理\",{\"1\":{\"13\":1}}],[\"temporal\",{\"0\":{\"96\":1}}],[\"terminal\",{\"1\":{\"19\":1}}],[\"technology\",{\"1\":{\"3\":1},\"2\":{\"16\":1}}],[\"time\",{\"1\":{\"10\":1,\"68\":1}}],[\"truncated\",{\"0\":{\"56\":1,\"58\":1,\"59\":1},\"1\":{\"58\":1}}],[\"trial\",{\"1\":{\"19\":2}}],[\"trip\",{\"1\":{\"10\":1}}],[\"trajectory是在策略给定下\",{\"1\":{\"19\":1}}],[\"trajectory\",{\"1\":{\"19\":4,\"28\":1,\"29\":1}}],[\"transition\",{\"1\":{\"19\":2,\"20\":1}}],[\"transmission\",{\"1\":{\"8\":1}}],[\"transactions\",{\"1\":{\"3\":1},\"2\":{\"16\":1}}],[\"t\",{\"1\":{\"8\":11,\"9\":20,\"10\":12,\"11\":8,\"13\":6}}],[\"tool\",{\"1\":{\"39\":1}}],[\"to\",{\"0\":{\"34\":1},\"1\":{\"4\":1,\"19\":1,\"20\":1,\"69\":1,\"73\":1,\"79\":1,\"85\":1}}],[\"that\",{\"1\":{\"28\":1,\"45\":1,\"68\":1}}],[\"then\",{\"1\":{\"45\":2}}],[\"theorem来求解贝尔曼最优公式\",{\"1\":{\"45\":1}}],[\"theorem\",{\"1\":{\"40\":2,\"45\":1,\"49\":1,\"85\":1}}],[\"the\",{\"0\":{\"30\":1,\"31\":1},\"1\":{\"4\":1,\"9\":1,\"10\":1,\"19\":6,\"20\":4,\"24\":1,\"27\":1,\"28\":2,\"34\":1,\"35\":4,\"36\":2,\"39\":1,\"40\":2,\"45\":2,\"68\":1,\"69\":2,\"73\":1,\"85\":2}}],[\"throughput\",{\"1\":{\"4\":1}}],[\"identically\",{\"1\":{\"62\":1}}],[\"idle\",{\"1\":{\"10\":1}}],[\"iid\",{\"1\":{\"62\":1}}],[\"i\",{\"1\":{\"62\":2,\"65\":1}}],[\"i−γpπk​​\",{\"1\":{\"55\":1}}],[\"i−γpπ​\",{\"1\":{\"34\":2}}],[\"improvement\",{\"1\":{\"53\":1,\"57\":1,\"66\":1,\"69\":1,\"75\":1}}],[\"immediate\",{\"0\":{\"30\":1},\"1\":{\"32\":1}}],[\"if\",{\"1\":{\"40\":1,\"45\":1,\"73\":1,\"85\":1}}],[\"ij​=pπ​\",{\"1\":{\"33\":1}}],[\"i=1n​\",{\"1\":{\"19\":2}}],[\"iteration\",{\"0\":{\"49\":1,\"52\":1,\"56\":1,\"57\":2,\"58\":1,\"59\":1},\"1\":{\"49\":1,\"55\":2,\"56\":2,\"57\":10,\"58\":3,\"63\":1,\"64\":1,\"66\":1,\"69\":1}}],[\"iterative\",{\"1\":{\"34\":1,\"79\":1}}],[\"it\",{\"1\":{\"4\":1,\"68\":1}}],[\"is\",{\"1\":{\"4\":3,\"19\":1,\"20\":2,\"28\":1,\"40\":1,\"45\":2,\"68\":1,\"73\":2}}],[\"ieee\",{\"1\":{\"3\":1},\"2\":{\"16\":1}}],[\"independent\",{\"1\":{\"62\":1}}],[\"interacting\",{\"1\":{\"19\":1}}],[\"invoked\",{\"1\":{\"4\":1}}],[\"in\",{\"0\":{\"3\":1},\"1\":{\"4\":1,\"68\":1,\"85\":1}}],[\"mbgd\",{\"0\":{\"93\":1}}],[\"minimize\",{\"1\":{\"91\":1}}],[\"method\",{\"1\":{\"68\":2,\"88\":3}}],[\"methods\",{\"1\":{\"68\":1}}],[\"means\",{\"1\":{\"79\":1}}],[\"means来划分各个无人机所管理的用户簇\",{\"1\":{\"13\":1}}],[\"means的优化目标是最小化无人机与对应集群用户的欧氏距离\",{\"1\":{\"13\":1}}],[\"means可以视为获得无人机部署的低复杂度方案\",{\"1\":{\"13\":1}}],[\"means算法\",{\"1\":{\"13\":1}}],[\"means和igk算法比具有较低的复杂度\",{\"1\":{\"5\":1}}],[\"mean算法获得初始单元划分\",{\"1\":{\"5\":1}}],[\"mean\",{\"0\":{\"30\":1,\"31\":1,\"86\":1,\"90\":1},\"1\":{\"5\":1,\"27\":1,\"28\":1,\"62\":1,\"64\":1,\"69\":1,\"79\":1}}],[\"mc\",{\"0\":{\"63\":1,\"67\":1,\"70\":1,\"72\":1,\"75\":1},\"1\":{\"63\":1,\"67\":2,\"68\":1,\"69\":1,\"75\":2,\"88\":1}}],[\"mdp就变为mp\",{\"1\":{\"20\":1}}],[\"mdp\",{\"0\":{\"20\":1}}],[\"mapping\",{\"1\":{\"40\":2,\"45\":5,\"49\":1}}],[\"matrix\",{\"0\":{\"33\":1},\"1\":{\"33\":1,\"36\":1,\"42\":1}}],[\"markov\",{\"0\":{\"20\":1},\"1\":{\"20\":2}}],[\"markovian\",{\"1\":{\"14\":1}}],[\"may\",{\"1\":{\"19\":1}}],[\"mssrkn​​rtt​+1\",{\"1\":{\"10\":1}}],[\"mss\",{\"1\":{\"10\":1}}],[\"m\",{\"1\":{\"9\":1}}],[\"monro\",{\"1\":{\"85\":2}}],[\"monto\",{\"0\":{\"82\":1}}],[\"monte\",{\"0\":{\"62\":1},\"1\":{\"62\":1}}],[\"moreover\",{\"1\":{\"45\":1}}],[\"mobility\",{\"1\":{\"14\":1}}],[\"modles可选择\",{\"1\":{\"14\":1}}],[\"model|environment\",{\"1\":{\"32\":1}}],[\"model\",{\"0\":{\"10\":1,\"62\":1},\"1\":{\"14\":3,\"32\":2,\"63\":1,\"64\":2}}],[\"mos主要是有关传输速率rkn​​的函数\",{\"1\":{\"13\":1}}],[\"mosrkn​​​=t=0∑ts​​moskn​​\",{\"1\":{\"10\":1}}],[\"moskn​​\",{\"1\":{\"10\":3}}],[\"mos\",{\"1\":{\"5\":1}}],[\"movement\",{\"0\":{\"3\":1},\"1\":{\"4\":2}}],[\"multiple\",{\"0\":{\"3\":1},\"1\":{\"4\":1}}],[\"ak​=k1​是满足上面三个条件的\",{\"1\":{\"85\":1}}],[\"ak​→0不要过快\",{\"1\":{\"85\":1}}],[\"ak​→0\",{\"1\":{\"85\":1}}],[\"ak​\",{\"1\":{\"84\":1}}],[\"ak∗​∣s\",{\"1\":{\"66\":1}}],[\"ak∗​\",{\"1\":{\"53\":1}}],[\"a3​\",{\"1\":{\"68\":2}}],[\"a4​\",{\"1\":{\"68\":2}}],[\"a2​\",{\"1\":{\"68\":4}}],[\"approximation\",{\"1\":{\"79\":1}}],[\"approximate\",{\"1\":{\"69\":1}}],[\"approach\",{\"1\":{\"14\":2}}],[\"appears\",{\"1\":{\"68\":1}}],[\"a=ak∗​\",{\"1\":{\"50\":1,\"53\":1}}],[\"as\",{\"1\":{\"45\":1,\"91\":1}}],[\"associate\",{\"1\":{\"20\":1}}],[\"assisted\",{\"1\":{\"4\":1}}],[\"algorithms\",{\"1\":{\"79\":1}}],[\"algorithm\",{\"0\":{\"49\":1,\"52\":1,\"56\":1,\"58\":1,\"59\":1,\"82\":1},\"1\":{\"45\":1,\"55\":2,\"57\":2,\"58\":1,\"63\":1,\"66\":1,\"85\":1}}],[\"all\",{\"1\":{\"28\":1,\"40\":1}}],[\"average\",{\"1\":{\"35\":2}}],[\"a∑​π\",{\"1\":{\"32\":1}}],[\"a∑​p\",{\"1\":{\"31\":1}}],[\"a∣s\",{\"1\":{\"20\":1,\"28\":1,\"30\":2,\"31\":5,\"32\":4,\"35\":3,\"36\":1,\"42\":2,\"44\":2,\"45\":1,\"50\":2,\"53\":3,\"66\":1,\"74\":1}}],[\"any\",{\"1\":{\"40\":1,\"45\":1,\"73\":1}}],[\"an\",{\"1\":{\"19\":1,\"34\":1,\"35\":1}}],[\"and\",{\"0\":{\"3\":1},\"1\":{\"4\":2,\"14\":1,\"35\":1,\"36\":1,\"39\":1,\"40\":1,\"62\":1}}],[\"at\",{\"1\":{\"19\":1,\"20\":1}}],[\"at​=a\",{\"1\":{\"30\":1,\"31\":2,\"35\":2,\"36\":1,\"62\":1,\"64\":1,\"65\":1}}],[\"at​\",{\"1\":{\"13\":1}}],[\"a1​\",{\"1\":{\"19\":2,\"20\":2,\"68\":1}}],[\"ai​\",{\"1\":{\"19\":1}}],[\"academic\",{\"0\":{\"105\":1},\"2\":{\"15\":1,\"21\":1,\"37\":1,\"46\":1,\"60\":1,\"77\":1,\"94\":1,\"97\":1}}],[\"actions\",{\"1\":{\"20\":1}}],[\"action\",{\"0\":{\"35\":1},\"1\":{\"13\":1,\"19\":3,\"20\":2,\"35\":8,\"36\":2,\"40\":2,\"44\":2,\"50\":1,\"53\":1,\"57\":2,\"62\":1,\"64\":5,\"66\":6,\"68\":9,\"69\":3,\"71\":4,\"73\":2,\"79\":1}}],[\"agent从一个状态出发\",{\"1\":{\"35\":1}}],[\"agent从一个状态出发可以得到的平均return\",{\"1\":{\"35\":1}}],[\"agent可能走出的全部轨迹\",{\"1\":{\"19\":1}}],[\"agent将获得负奖励\",{\"1\":{\"13\":1}}],[\"agent\",{\"1\":{\"13\":1,\"19\":2,\"35\":2}}],[\"awgn\",{\"1\":{\"9\":1}}],[\"aerial\",{\"1\":{\"4\":1}}],[\"a\",{\"1\":{\"4\":1,\"13\":3,\"14\":2,\"19\":7,\"20\":5,\"28\":3,\"30\":1,\"31\":3,\"32\":6,\"35\":13,\"36\":2,\"39\":1,\"40\":1,\"42\":3,\"44\":1,\"45\":2,\"50\":6,\"53\":6,\"57\":1,\"62\":3,\"64\":11,\"65\":8,\"66\":3,\"68\":2,\"69\":2,\"71\":6,\"73\":1,\"79\":1}}],[\"a+b=c\",{\"1\":{\"0\":1,\"99\":1}}],[\"difference\",{\"0\":{\"96\":1}}],[\"distributed\",{\"1\":{\"62\":1}}],[\"distribution\",{\"1\":{\"20\":1}}],[\"discounted\",{\"1\":{\"19\":1,\"29\":1}}],[\"data\",{\"1\":{\"68\":1}}],[\"daily\",{\"0\":{\"103\":1},\"2\":{\"1\":1}}],[\"daily1\",{\"0\":{\"0\":1}}],[\"d\",{\"1\":{\"10\":3,\"62\":1}}],[\"dkn​​\",{\"1\":{\"9\":1}}],[\"dkn​​=hn2​\",{\"1\":{\"8\":1}}],[\"dkn​\",{\"1\":{\"9\":1}}],[\"down\",{\"1\":{\"8\":1}}],[\"dynamic\",{\"1\":{\"4\":1,\"32\":1}}],[\"driven\",{\"1\":{\"4\":1}}],[\"descent\",{\"0\":{\"87\":1},\"1\":{\"88\":2}}],[\"design\",{\"0\":{\"3\":1}}],[\"decision\",{\"0\":{\"20\":1}}],[\"deterministic\",{\"0\":{\"92\":1},\"1\":{\"14\":1,\"19\":1,\"40\":2}}],[\"demonstrating\",{\"1\":{\"4\":1}}],[\"deployment\",{\"0\":{\"3\":1},\"1\":{\"4\":1}}],[\"d1\",{\"2\":{\"2\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(et(t,v[s],n)):e==="search"?self.postMessage(tt(t,v[s],n)):self.postMessage({suggestions:et(t,v[s],n),results:tt(t,v[s],n)})};
//# sourceMappingURL=index.js.map
