const L=Object.entries,st=Object.fromEntries,nt="ENTRIES",T="KEYS",R="VALUES",_="";class k{set;_type;_path;constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===_)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==_).join("")}value(){return E(this._path).node.get(_)}result(){switch(this._type){case R:return this.value();case T:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],ot=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return W(e,t,s,n,i,1,o,""),n},W=(e,t,s,n,o,u,i,r)=>{const d=u*i;t:for(const c of e.keys())if(c===_){const a=o[d-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let h=0;h<c.length;++h,++a){const g=c[h],m=i*a,p=m-i;let l=o[m];const f=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let F=f;F<y;++F){const D=g!==t[F],w=o[p+F]+ +D,A=o[p+F+1]+1,z=o[m+F]+1,V=o[m+F+1]=Math.min(w,A,z);V<l&&(l=V)}if(l>s)continue t}W(e.get(c),t,s,n,o,a,i,r+c)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=M(n);for(const i of o.keys())if(i!==_&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ut(this._tree,t)}entries(){return new k(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ot(this._tree,t,s)}get(t){const s=I(this._tree,t);return s!==void 0?s.get(_):void 0}has(t){const s=I(this._tree,t);return s!==void 0&&s.has(_)}keys(){return new k(this,T)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,O(this._tree,t).set(_,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);return n.set(_,s(n.get(_))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);let o=n.get(_);return o===void 0&&n.set(_,o=s()),o}values(){return new k(this,R)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==_&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},I=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==_&&t.startsWith(s))return I(e.get(s),t.slice(s.length))},O=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==_&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const d=e.get(u);if(r===u.length)e=d;else{const c=new Map;c.set(u.slice(r),d),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ut=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(_),s.size===0)q(n);else if(s.size===1){const[o,u]=s.entries().next().value;$(n,o,u)}}},q=e=>{if(e.length===0)return;const[t,s]=M(e);if(t.delete(s),t.size===0)q(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==_&&$(e.slice(0,-1),n,o)}},$=(e,t,s)=>{if(e.length===0)return;const[n,o]=M(e);n.set(o+t,s),n.delete(o)},M=e=>e[e.length-1],it=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},rt=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,S="or",N="and",ct="and_not",lt=(e,t)=>{e.includes(t)||e.push(t)},P=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},G=({score:e},{score:t})=>t-e,ht=()=>new Map,b=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},H=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,dt={[S]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),P(n.terms,u)}}return e},[N]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);P(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[ct]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},at=(e,t,s,n,o,u)=>{const{k:i,b:r,d}=u;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/o)))},ft=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},J=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},gt=(e,t,s,n)=>{if(!e._index.has(n)){J(e,s,t,n);return}const o=e._index.fetch(n,ht),u=o.get(t);u==null||u.get(s)==null?J(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},mt={k:1.2,b:.7,d:.5},pt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(rt),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},U={combineWith:S,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:mt},Ft={combineWith:N,prefix:(e,t,s)=>t===s.length-1},_t={batchSize:1e3,batchWait:10},K={minDirtFactor:.1,minDirtCount:20},yt={..._t,...K},X=Symbol("*"),At=(e,t)=>{const s=new Map,n={...e._options.searchOptions,...t};for(const[o,u]of e._documentIds){const i=n.boostDocument?n.boostDocument(u,"",e._storedFields.get(o)):1;s.set(o,{score:i,terms:[],match:{}})}return s},Y=(e,t=S)=>{if(e.length===0)return new Map;const s=t.toLowerCase(),n=dt[s];if(!n)throw new Error(`Invalid combination operator: ${t}`);return e.reduce(n)||new Map},B=(e,t,s,n,o,u,i,r,d=new Map)=>{if(o==null)return d;for(const c of Object.keys(u)){const a=u[c],h=e._fieldIds[c],g=o.get(h);if(g==null)continue;let m=g.size;const p=e._avgFieldLength[h];for(const l of g.keys()){if(!e._documentIds.has(l)){gt(e,h,l,s),m-=1;continue}const f=i?i(e._documentIds.get(l),s,e._storedFields.get(l)):1;if(!f)continue;const y=g.get(l),F=e._fieldLength.get(l)[h],D=at(y,m,e._documentCount,F,p,r),w=n*a*f*D,A=d.get(l);if(A){A.score+=w,lt(A.terms,t);const z=H(A.match,s);z?z.push(c):A.match[s]=[c]}else d.set(l,{score:w,terms:[t],match:{[s]:[c]}})}}return d},Ct=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((l,f)=>({...l,[f]:H(n.boost,f)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:c,prefix:a}={...U.weights,...i},h=e._index.get(t.term),g=B(e,t.term,t.term,1,h,o,u,d);let m,p;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const l=t.fuzzy===!0?.2:t.fuzzy,f=l<1?Math.min(r,Math.round(t.term.length*l)):l;f&&(p=e._index.fuzzyGet(t.term,f))}if(m)for(const[l,f]of m){const y=l.length-t.term.length;if(!y)continue;p?.delete(l);const F=a*l.length/(l.length+.3*y);B(e,t.term,l,F,f,o,u,d,g)}if(p)for(const l of p.keys()){const[f,y]=p.get(l);if(!y)continue;const F=c*l.length/(l.length+y);B(e,t.term,l,F,f,o,u,d,g)}return g},Q=(e,t,s={})=>{if(t===X)return At(e,s);if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(g=>Q(e,g,a));return Y(h,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:d}=i,c=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(ft(i)).map(a=>Ct(e,a,i));return Y(c,i.combineWith)},Z=(e,t,s={})=>{const n=Q(e,t,s),o=[];for(const[u,{score:i,terms:r,match:d}]of n){const c=r.length||1,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(d),queryTerms:r,match:d};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return t===X&&s.boostDocument==null&&e._options.searchOptions.boostDocument==null||o.sort(G),o},Et=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Z(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=u,d.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:d}]of n)o.push({suggestion:u,terms:r,score:i/d});return o.sort(G),o};class wt{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?yt:t.autoVacuum;this._options={...pt,...t,autoVacuum:s,searchOptions:{...U,...t.searchOptions||{}},autoSuggestOptions:{...Ft,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=K,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new wt(a);h._documentCount=t,h._nextId=s,h._documentIds=b(n),h._idToShortId=new Map,h._fieldIds=o,h._fieldLength=b(u),h._avgFieldLength=i,h._storedFields=b(r),h._dirtCount=d||0,h._index=new C;for(const[g,m]of h._documentIds)h._idToShortId.set(m,g);for(const[g,m]of e){const p=new Map;for(const l of Object.keys(m)){let f=m[l];c===1&&(f=f.ds),p.set(parseInt(l,10),b(f))}h._index.set(g,p)}return h},j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let h="";i===0?h=c.length>20?`… ${c.slice(-20)}`:c:a?h=c.length+i>100?`${c.slice(0,100-i)}… `:c:h=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,h&&o.push(h),i+=h.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let d=s.indexOf(n,u);if(d===-1)return null;for(;d>=0;){const c=d+n.length;if(r(e.slice(u,d)),u=c,i>100)break;d=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},xt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),bt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),tt=(e,t,s={})=>{const n={};return Z(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...s}).forEach(o=>{const{id:u,terms:i,score:r}=o,d=u.includes("@"),c=u.includes("#"),[a,h]=u.split(/[#@]/),g=Number(a),m=i.sort((l,f)=>l.length-f.length).filter((l,f)=>i.slice(f+1).every(y=>!y.includes(l))),{contents:p}=n[g]??={title:"",contents:[]};if(d)p.push([{type:"customField",id:g,index:h,display:m.map(l=>o.c.map(f=>j(f,l))).flat().filter(l=>l!==null)},r]);else{const l=m.map(f=>j(o.h,f)).filter(f=>f!==null);if(l.length&&p.push([{type:c?"heading":"title",id:g,...c&&{anchor:h},display:l},r]),"t"in o)for(const f of o.t){const y=m.map(F=>j(f,F)).filter(F=>F!==null);y.length&&p.push([{type:"text",id:g,...c&&{anchor:h},display:y},r])}}}),L(n).sort(([,o],[,u])=>"max"==="total"?xt(o,u):bt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=it(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},et=(e,t,s={})=>Et(t,e,{fuzzy:.2,...s}).map(({suggestion:n})=>n),v=st(L(JSON.parse("{\"/\":{\"documentCount\":240,\"nextId\":240,\"documentIds\":{\"0\":\"2\",\"1\":\"2@0\",\"2\":\"2@1\",\"3\":\"3\",\"4\":\"3#类与对象\",\"5\":\"3#方法的创建与使用\",\"6\":\"3#方法的进阶使用\",\"7\":\"3#this-的使用\",\"8\":\"3#方法的重载\",\"9\":\"3#构造方法\",\"10\":\"3@0\",\"11\":\"3@1\",\"12\":\"4\",\"13\":\"4#静态变量和静态方法\",\"14\":\"4#静态变量初始化\",\"15\":\"4#包的访问与控制\",\"16\":\"4#包的声明和导入\",\"17\":\"4#访问权限控制\",\"18\":\"4@0\",\"19\":\"4@1\",\"20\":\"5\",\"21\":\"5#封装-继承和多态\",\"22\":\"5#封装\",\"23\":\"5#继承\",\"24\":\"5#object-类\",\"25\":\"5#方法重写-override\",\"26\":\"5#控制符-final\",\"27\":\"5#抽象类-abstract\",\"28\":\"5#接口-interface\",\"29\":\"5#object类中的-克隆方法\",\"30\":\"5#枚举类-enum\",\"31\":\"5@0\",\"32\":\"5@1\",\"33\":\"6\",\"34\":\"6#面向对象高级篇1\",\"35\":\"6#基本类型包装类\",\"36\":\"6#所有包装类如下\",\"37\":\"6#包装类的方法\",\"38\":\"6#特殊包装类\",\"39\":\"6@0\",\"40\":\"6@1\",\"41\":\"7\",\"42\":\"7#面向对象高级篇-2\",\"43\":\"7#数组\",\"44\":\"7#定义\",\"45\":\"7#方法\",\"46\":\"7#访问元素\",\"47\":\"7#特性\",\"48\":\"7#final性质\",\"49\":\"7#多维数组\",\"50\":\"7#可变长参数\",\"51\":\"7#main函数的-string-args\",\"52\":\"7#字符串\",\"53\":\"7#string-类\",\"54\":\"7#stringbuilder-类\",\"55\":\"7#正则表达式\",\"56\":\"7@0\",\"57\":\"7@1\",\"58\":\"8\",\"59\":\"8#面向对象高级篇-3\",\"60\":\"8#内部类\",\"61\":\"8#成员内部类-属于-对象\",\"62\":\"8#静态内部类-属于-类\",\"63\":\"8#局部内部类\",\"64\":\"8#静态内部类编译特性\",\"65\":\"8#匿名内部类\",\"66\":\"8#匿名内部类特性\",\"67\":\"8#lambda表达式\",\"68\":\"8#方法引用\",\"69\":\"8@0\",\"70\":\"8@1\",\"71\":\"9\",\"72\":\"9#面向对象高级篇-4\",\"73\":\"9#异常\",\"74\":\"9@0\",\"75\":\"9@1\",\"76\":\"10\",\"77\":\"10#主要动机\",\"78\":\"10#主要贡献\",\"79\":\"10#主要内容\",\"80\":\"10#系统结构\",\"81\":\"10#基本设置\",\"82\":\"10#信号模型\",\"83\":\"10#quality-of-experience-model\",\"84\":\"10#优化问题建立\",\"85\":\"10#解决方案\",\"86\":\"10#无人机的3d部署\",\"87\":\"10#无人机的动态移动设计\",\"88\":\"10@0\",\"89\":\"10@1\",\"90\":\"11\",\"91\":\"11#强化学习框架图\",\"92\":\"11#_1-基本概念\",\"93\":\"11#_2-markov-decision-process-mdp\",\"94\":\"11@0\",\"95\":\"11@1\",\"96\":\"12\",\"97\":\"12#_1-the-simplest-actor-critic-qac\",\"98\":\"12#_2-advantage-actor-critic-a2c\",\"99\":\"12#_2-1-baseline\",\"100\":\"12#_2-2-最好的-baseline\",\"101\":\"12#_2-3-对应算法\",\"102\":\"12#_3-off-policy-actor-critic\",\"103\":\"12#_3-1-重要性采样-importance-sampling\",\"104\":\"12#_3-2-off-policy\",\"105\":\"12#_3-3-伪代码\",\"106\":\"12#_4-deterministic-actor-critic-dpg\",\"107\":\"12@0\",\"108\":\"12@1\",\"109\":\"13\",\"110\":\"13#核心内容\",\"111\":\"13#_1-state-value\",\"112\":\"13#_1-1\",\"113\":\"13#_1-2-state-value\",\"114\":\"13#_1-3-state-value-与-return-的区别\",\"115\":\"13#_2-bellman-equation\",\"116\":\"13#_2-1-the-mean-of-immediate-rewards\",\"117\":\"13#_2-2-the-mean-of-future-rewards\",\"118\":\"13#_2-3-bellman-equation\",\"119\":\"13#_2-4-bellman-equation-matrix-vector-form\",\"120\":\"13#_3-why-to-slove-state-value\",\"121\":\"13#_4-action-value\",\"122\":\"13#_5-总结\",\"123\":\"13@0\",\"124\":\"13@1\",\"125\":\"14\",\"126\":\"14#_1-optimal-policy\",\"127\":\"14#_2-bellman-optimality-equation-boe\",\"128\":\"14#_2-1-基本形式\",\"129\":\"14#_2-2-如何求解\",\"130\":\"14#_2-2-1-如何处理等式右边的-最优策略\",\"131\":\"14#_2-求解-state-value\",\"132\":\"14@0\",\"133\":\"14@1\",\"134\":\"15\",\"135\":\"15#_1-value-iteration-algorithm\",\"136\":\"15#_1-1-具体步骤\",\"137\":\"15#_1-2-伪代码\",\"138\":\"15#_2-policy-iteration-algorithm\",\"139\":\"15#_2-1-算法描述\",\"140\":\"15#_2-2-伪代码\",\"141\":\"15#_2-3-一些问题\",\"142\":\"15#_3-truncated-policy-iteration-algorithm\",\"143\":\"15#_3-1-value-iteration-与-policy-iteration-算法比较\",\"144\":\"15#_3-2-truncated-policy-iteration-algorithm\",\"145\":\"15#truncated-policy-iteration-algorithm-是否是收敛的\",\"146\":\"15@0\",\"147\":\"15@1\",\"148\":\"16\",\"149\":\"16#_1-mc-basic\",\"150\":\"16#_1-1-算法思路\",\"151\":\"16#_1-2-如何估计\",\"152\":\"16#_1-3-具体算法\",\"153\":\"16#_2-mc-exploring-starts\",\"154\":\"16#_2-1-episode-的高效利用\",\"155\":\"16#_2-2-高效地更新-policy\",\"156\":\"16#_2-3-mc-exploring-starts\",\"157\":\"16#_2-4-exploring-statrts的解释\",\"158\":\"16#_3-mc-eplison-greedy\",\"159\":\"16#_3-1-soft-policy\",\"160\":\"16#_3-2-greedy-policy\",\"161\":\"16#_3-3-greedy-policy-引入-mc-based-算法中\",\"162\":\"16#_3-3-算法流程\",\"163\":\"16@0\",\"164\":\"16@1\",\"165\":\"17\",\"166\":\"17#_1-引言\",\"167\":\"17#_1-1-求均值的方法\",\"168\":\"17#_2-robbins-monto-rm-algorithm\",\"169\":\"17#_2-1-问题引入\",\"170\":\"17#_2-2-算法介绍\",\"171\":\"17#_2-3-收敛性分析\",\"172\":\"17#_2-4-应用于-mean-estimation-中\",\"173\":\"17#_3-stochastic-gradient-descent\",\"174\":\"17#_3-1-问题引入\",\"175\":\"17#_3-2-sgd-分析\",\"176\":\"17#mean-estimation-问题转化\",\"177\":\"17#sgd-正确性和收敛性分析\",\"178\":\"17#_3-3-sgd-另一种问题描述方法-deterministic-formulation\",\"179\":\"17#_3-4-bgd-mbgd-sgdw\",\"180\":\"17@0\",\"181\":\"17@1\",\"182\":\"18\",\"183\":\"18#_1-引入\",\"184\":\"18#_2-td-learning-of-state-value\",\"185\":\"18#_2-1-算法描述\",\"186\":\"18#_2-2-算法分析\",\"187\":\"18#_2-3-td-算法-与-mc-算法的比较\",\"188\":\"18#_3-td-learning-of-action-value\",\"189\":\"18#_3-1-sarsa\",\"190\":\"18#_3-2-n-step-sarsa\",\"191\":\"18#_3-3-expected-sarsa\",\"192\":\"18#_4-td-learning-of-optimal-action-value\",\"193\":\"18#_4-1-q-learning\",\"194\":\"18#_4-2-off-policy-on-policy\",\"195\":\"18#on-policy\",\"196\":\"18#off-policy\",\"197\":\"18#_4-3-q-learning-伪代码\",\"198\":\"18#off-poicy-版本\",\"199\":\"18#on-policy-版本\",\"200\":\"18#_5-td-算法的统一形式和总结\",\"201\":\"18@0\",\"202\":\"18@1\",\"203\":\"19\",\"204\":\"19#_1-引入\",\"205\":\"19#_2-alogorithm-of-state-value-estimation\",\"206\":\"19#_2-1-obejctive-function\",\"207\":\"19#uniform-distributon\",\"208\":\"19#stationary-distribution\",\"209\":\"19#_2-2-optimization-algorithms-优化算法\",\"210\":\"19#monte-carlo-learning-with-function-approximation\",\"211\":\"19#td-learning-with-function-approximation\",\"212\":\"19#_3-sarsa-with-function-approximation\",\"213\":\"19#_4-q-learning-with-function-approximation\",\"214\":\"19#_5-deep-q-learning-dqn\",\"215\":\"19#优化方法\",\"216\":\"19#经验回放-replay-buffer\",\"217\":\"19#伪代码\",\"218\":\"19@0\",\"219\":\"19@1\",\"220\":\"20\",\"221\":\"20#_1-基本思路\",\"222\":\"20#_2-目标函数定义\",\"223\":\"20#_2-1-average-state-value\",\"224\":\"20#另一种表达\",\"225\":\"20#d-s-的选择\",\"226\":\"20#_2-2-average-return-value\",\"227\":\"20#另一种表达-1\",\"228\":\"20#_3-目标函数梯度求解\",\"229\":\"20#_4-reinforce-梯度上升算法\",\"230\":\"20#reinforce-算法\",\"231\":\"20@0\",\"232\":\"20@1\",\"233\":\"21\",\"234\":\"22\",\"235\":\"23\",\"236\":\"24\",\"237\":\"25\",\"238\":\"26\",\"239\":\"27\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,1],\"1\":[null,null,1],\"2\":[null,null,1],\"3\":[2],\"4\":[1,85],\"5\":[1,56],\"6\":[1],\"7\":[2,23],\"8\":[1,20],\"9\":[1,75],\"10\":[null,null,1],\"11\":[null,null,1],\"12\":[2],\"13\":[1,52],\"14\":[1,27],\"15\":[1],\"16\":[1,90],\"17\":[1,60],\"18\":[null,null,1],\"19\":[null,null,1],\"20\":[2],\"21\":[2,16],\"22\":[1],\"23\":[1,4],\"24\":[2,79],\"25\":[1,74],\"26\":[1,14],\"27\":[1,73],\"28\":[1,141],\"29\":[2,73],\"30\":[1,72],\"31\":[null,null,1],\"32\":[null,null,1],\"33\":[2],\"34\":[1],\"35\":[1,9],\"36\":[1,97],\"37\":[1,25],\"38\":[1,69],\"39\":[null,null,1],\"40\":[null,null,1],\"41\":[5],\"42\":[2],\"43\":[1,5],\"44\":[1,47],\"45\":[1,40],\"46\":[1,29],\"47\":[1,32],\"48\":[1,22],\"49\":[1,24],\"50\":[1,43],\"51\":[1,25],\"52\":[1,15],\"53\":[2,65],\"54\":[2,60],\"55\":[1,148],\"56\":[null,null,1],\"57\":[null,null,1],\"58\":[2],\"59\":[2,6],\"60\":[1,3],\"61\":[4,93],\"62\":[4,34],\"63\":[1,35],\"64\":[1,42],\"65\":[1,84],\"66\":[1,35],\"67\":[1,53],\"68\":[1,67],\"69\":[null,null,1],\"70\":[null,null,1],\"71\":[4],\"72\":[2],\"73\":[1],\"74\":[null,null,1],\"75\":[null,null,1],\"76\":[10,6],\"77\":[1,45],\"78\":[1,29],\"79\":[1],\"80\":[1],\"81\":[1,44],\"82\":[1,105],\"83\":[4,82],\"84\":[1,34],\"85\":[1],\"86\":[1,157],\"87\":[1,67],\"88\":[null,null,1],\"89\":[null,null,6],\"90\":[2],\"91\":[1,1],\"92\":[2,118],\"93\":[6,54],\"94\":[null,null,1],\"95\":[null,null,1],\"96\":[4,19],\"97\":[7,2],\"98\":[6,8],\"99\":[3,28],\"100\":[3,8],\"101\":[3,3],\"102\":[5,10],\"103\":[6,3],\"104\":[4,4],\"105\":[2,1],\"106\":[6,3],\"107\":[null,null,1],\"108\":[null,null,1],\"109\":[2],\"110\":[1,5],\"111\":[3],\"112\":[1,16],\"113\":[4,27],\"114\":[7,36],\"115\":[3,35],\"116\":[8,10],\"117\":[7,23],\"118\":[4,47],\"119\":[8,34],\"120\":[6,34],\"121\":[3,72],\"122\":[2,29],\"123\":[null,null,1],\"124\":[null,null,1],\"125\":[2,16],\"126\":[3,36],\"127\":[6],\"128\":[3,28],\"129\":[2,8],\"130\":[5,21],\"131\":[4,75],\"132\":[null,null,1],\"133\":[null,null,1],\"134\":[4,6],\"135\":[4,18],\"136\":[2,53],\"137\":[3,1],\"138\":[4,3],\"139\":[3,70],\"140\":[2,1],\"141\":[3,44],\"142\":[5,6],\"143\":[7,71],\"144\":[6,23],\"145\":[5,1],\"146\":[null,null,1],\"147\":[null,null,1],\"148\":[6,51],\"149\":[3,16],\"150\":[2,61],\"151\":[3,33],\"152\":[3,49],\"153\":[4,10],\"154\":[4,63],\"155\":[3,52],\"156\":[5,1],\"157\":[4,33],\"158\":[4,9],\"159\":[4,25],\"160\":[4,39],\"161\":[7,12],\"162\":[2,1],\"163\":[null,null,1],\"164\":[null,null,1],\"165\":[2,33],\"166\":[2],\"167\":[2,32],\"168\":[5],\"169\":[3,17],\"170\":[2,31],\"171\":[3,44],\"172\":[6,41],\"173\":[4],\"174\":[3,57],\"175\":[4],\"176\":[3,8],\"177\":[2,67],\"178\":[6,11],\"179\":[5,1],\"180\":[null,null,1],\"181\":[null,null,1],\"182\":[4],\"183\":[2,50],\"184\":[6,10],\"185\":[3,36],\"186\":[2,37],\"187\":[7],\"188\":[6,23],\"189\":[3,37],\"190\":[5,3],\"191\":[3,2],\"192\":[7,21],\"193\":[4,24],\"194\":[6,7],\"195\":[2,9],\"196\":[2,17],\"197\":[5,17],\"198\":[3,10],\"199\":[3,1],\"200\":[3,2],\"201\":[null,null,1],\"202\":[null,null,1],\"203\":[6,11],\"204\":[2,12],\"205\":[6,9],\"206\":[4,15],\"207\":[2,22],\"208\":[2,52],\"209\":[4,40],\"210\":[6,19],\"211\":[5,15],\"212\":[5,10],\"213\":[6,13],\"214\":[6,21],\"215\":[1,110],\"216\":[4,3],\"217\":[1,13],\"218\":[null,null,1],\"219\":[null,null,1],\"220\":[5,10],\"221\":[2,38],\"222\":[2],\"223\":[5,22],\"224\":[1,5],\"225\":[3,21],\"226\":[4,20],\"227\":[1,27],\"228\":[2,60],\"229\":[3,40],\"230\":[2,1],\"231\":[null,null,1],\"232\":[null,null,1],\"233\":[1,3],\"234\":[1],\"235\":[1],\"236\":[1],\"237\":[1],\"238\":[1],\"239\":[1]},\"averageFieldLength\":[2.731344884244202,34.75395835179082,0.8742101801799845],\"storedFields\":{\"0\":{\"h\":\"daily1\",\"t\":[\"a+b=c\"]},\"1\":{\"c\":[\"daily\"]},\"2\":{\"c\":[\"d1\"]},\"3\":{\"h\":\"Java - 类与对象1\"},\"4\":{\"h\":\"类与对象\",\"t\":[\"类: 是对一类事物的描述，是抽象的、概念上的定义.对象: 是某一类事物实际存在的每个个体，因而也被称为实例（instance）， 是类的一个具体化个体.\",\"类的创建: 类名的首字母通常是大写的.\",\"public class Person {//这里定义的人类具有三个属性，名字、年龄、性别 String name; //直接在类中定义变量，表示类具有的属性 int age; String sex; } \",\"对象实例的创建 new Person() :\",\"public static void main(String[] args) { Person p = new Person(); } \",\"对于对象而言，其变量名存储的是对象的引用（类似于c++指针的情况），并非是所对应的对象本身，即\",\"public static void main(String[] args) { //这里的a存放的是具体的某个值 int a = 10; //创建一个变量指代我们刚刚创建好的对象，变量的类型就是对应的类名 //这里的p1存放的是对象的引用，而不是本体，我们可以通过对象的引用来间接操作对象 Person p1 = new Person(); Person p2 = p1; // 我们将变量p2赋值为p1的值，那么实际上只是传递了对象的引用，而不是对象本身的复制 } \",\"在创建了对象之后，就可以进行一定操作，如: 访问、修改对象的属性. 不同对象的属性是分开独立存放的，每个对象都有一个自己的空间，修改一个对象的属性并不会影响到其他对象. 关于对象类型的变量，我们也可以不对任何对象进行引用：\",\"public static void main(String[] args) { Person p = null; //此时变量没有引用任何对象 p.name = \\\"小红\\\"; //我任性，就是要操作 System.out.println(p.name); } \",\"会出现异常，即空指针异常. 对象创建成功之后，它的属性没有进行赋值，但是我们前面说了，变量使用之前需要先赋值，那么创建对象之后能否直接访问呢？ 果直接创建对象，那么对象的属性都会存在初始值，如果是基本类型，那么默认是统一为0（如果是boolean的话，默认值为false）如果是引用类型，那么默认是null。\"]},\"5\":{\"h\":\"方法的创建与使用\",\"t\":[\"类除了具有属性外，还可以定义一些方法来描述同一类的行为。 方法是语句的集合，是为了完成某件事情而存在的。 方法名称同样可以随便起，但是规则跟变量的命名差不多，也是尽量使用小写字母开头的单词，如果是多个单词，一般使用驼峰命名法最规范。\",\"方法的定义如下:\",\"返回值类型 方法名称() { 方法体... } \",\"具体而言:\",\"public class Person { String name; int age; String sex; //自我介绍只需要完成就行，没有返回值，所以说使用void void hello(){ //完成自我介绍需要执行的所有代码就在这个花括号中编写 //这里编写代码跟我们之前在main中是一样的（实际上main就是一个函数） //自我介绍需要用到当前对象的名字和年龄，我们直接使用成员变量即可，变量的值就是当前对象的存放值 System.out.println(\\\"我叫 \\\"+name+\\\" 今年 \\\"+age+\\\" 岁了！\\\"); } } \",\"方法的调用:\",\"public static void main(String[] args) { Person p = new Person(); p.name = \\\"小明\\\"; p.age = 18; p.hello(); //我们只需要使用 . 运算符，就可以执行定义好的方法了，只需要 .方法名称() 即可 } \"]},\"6\":{\"h\":\"方法的进阶使用\"},\"7\":{\"h\":\"this 的使用\",\"t\":[\"有时候我们的方法中可能会出现一些与成员变量重名的变量：\",\"void setName(String name) { name = name; //出现重名时，优先使用作用域最接近的 //这里实际上是将方法参数的局部变量name赋值为本身 } \",\"我们如果想要在方法中访问到当前对象的属性，那么可以使用this关键字，来明确表示当前类的示例对象本身：\",\"void setName(String name) { this.name = name; //让当前对象的name变量值等于参数传入的值 } \",\"当然，如果方法内没有变量出现重名的情况，那么默认情况下可以不使用this关键字来明确表示当前对象：\",\"String getName() { return name; //这里没有使用this，但是当前作用域下只有对象属性的name变量，所以说直接就使用了 } \"]},\"8\":{\"h\":\"方法的重载\",\"t\":[\"有些时候，参数类型可能会多种多样，我们的方法需要能够同时应对多种情况。\",\"一个类中可以包含多个同名的方法，但是需要的形式参数不一样，方法的返回类型，可以相同，也可以不同，但是仅返回类型不同，是不允许的！\",\"int sum(int a, int b){ return a + b; } double sum(double a, double b){ //为了支持小数加法，我们可以进行一次重载 return a + b; } \"]},\"9\":{\"h\":\"构造方法\",\"t\":[\"我们前面创建对象，都是直接使用new关键字就能直接搞定了，但是我们发现，对象在创建之后，各种属性都是默认值，那么能否实现在对象创建时就为其指定名字、年龄、性别呢？ 要在对象创建时进行处理，我们可以使用**构造方法（构造器）**来完成。\",\"构造方法不需要填写返回值，并且方法名称与类名相同，默认情况下每个类都会自带一个没有任何参数的无参构造方法（只是不用我们去写，编译出来就自带）当然，我们也可以手动声明，对其进行修改：\",\"public class Person { String name; int age; String sex; Person(){ //构造方法不需要指定返回值，并且方法名称与类名相同 name = \\\"小明\\\"; //构造方法会在对象创建时执行，我们可以将各种需要初始化的操作都在这里进行处理 age = 18; sex = \\\"男\\\"; } } \",\"构造方法会在new的时候自动执行, 当然，我们也可以为构造方法设定参数：\",\"public class Person { String name; int age; String sex; Person(String name, int age, String sex){ //跟普通方法是一样的 this.name = name; this.age = age; this.sex = sex; } } \",\"注意，在我们自己定义一个构造方法之后，会覆盖掉默认的那一个无参构造方法，除非我们手动重载一个无参构造，否则要创建这个类的对象，必须调用我们自己定义的构造方法.\",\"当然，要给成员变量设定初始值，我们不仅可以通过构造方法，也可以直接在定义时赋值：\",\"public class Person { String name = \\\"未知\\\"; //直接赋值，那么对象构造好之后，属性默认就是这个值 int age = 10; String sex = \\\"男\\\"; } \",\"这里需要特别注意，成员变量的初始化，并不是在构造方法之后，而是在这之前就已经完成了.\",\"Person(String name, int age, String sex){ System.out.println(this.age); // 在赋值之前看看是否有初始值 // 这里是 this.age 而非 age // 此时this.age已经初始化完，但还未复制，this.age = 0 this.name = name; this.age = age; this.sex = sex; } \",\"我们也可以在类中添加代码块，代码块同样会在对象构造之前进行，在成员变量初始化之后执行：\",\"public class Person { String name; int age; String sex; { System.out.println(\\\"我是代码块\\\"); //代码块中的内容会在对象创建时仅执行一次 } Person(String name, int age, String sex){ System.out.println(\\\"我被构造了\\\"); this.name = name; this.age = age; this.sex = sex; } } \"]},\"10\":{\"c\":[\"code\"]},\"11\":{\"c\":[\"java\"]},\"12\":{\"h\":\"Java - 类与对象2\"},\"13\":{\"h\":\"静态变量和静态方法\",\"t\":[\"Static 静态的内容，我们可以理解为是属于这个类的，也可以理解为是所有对象共享的内容。 我们通过使用 static 关键字来声明一个变量或一个方法为静态的，一旦被声明为静态，那么通过这个类创建的所有对象，操作的都是同一个目标，也就是说，对象再多，也只有这一个静态的变量或方法。 一个对象改变了静态变量的值，那么其他的对象读取的就是被改变的值。\",\"一般情况下，我们并不会通过一个具体的对象去修改和使用静态属性，而是通过这个类去使用：\",\"public class Person { String name; int age; String sex; static String info; //这里我们定义一个info静态变量 } \",\"public static void main(String[] args) { Person.info = \\\"让我看看\\\"; System.out.println(Person.info); } \",\"同样的，我们可以将方法标记为静态：\",\"static void test(){ System.out.println(\\\"我是静态方法\\\"); } \",\"静态方法同样是属于类的，而不是具体的某个对象，所以说，就像下面这样:\",\"因为静态方法属于类的，所以说我们在静态方法中，无法获取成员变量的值, 同样的，在静态方法中，无法使用this关键字，因为this关键字代表的是当前的对象本身。 但是静态方法是可以访问到静态变量的.\"]},\"14\":{\"h\":\"静态变量初始化\",\"t\":[\"我们实际上是将 .class 文件丢给 JVM 去执行的，而每一个 .class 文件其实就是我们编写的一个类，我们在 Java 中使用一个类之前， JVM 并不会在一开始就去加载它，而是在需要时才会去加载（优化）一般遇到以下情况时才会会加载类：\",\"访问类的静态变量，或者为静态变量赋值\",\"new 创建类的实例（隐式加载）\",\"调用类的静态方法\",\"子类初始化时\",\"其他的情况会在讲到反射时介绍\",\"所有被标记为静态的内容，会在类刚加载的时候就分配，而不是在对象创建的时候分配，所以说静态内容一定会在第一个对象初始化之前完成加载。\"]},\"15\":{\"h\":\"包的访问与控制\"},\"16\":{\"h\":\"包的声明和导入\",\"t\":[\"包其实就是用来区分类位置的东西，也可以用来将我们的类进行分类（类似于C++中的namespace）随着我们的程序不断变大，可能会创建各种各样的类，他们可能会做不同的事情，那么这些类如果都放在一起的话，有点混乱，我们可以通过包的形式将这些类进行分类存放。\",\"包的命名规则同样是英文和数字的组合，最好是一个域名的格式，比如我们经常访问的 www.baidu.com ，后面的 baidu.com 就是域名，我们的包就可以命名为com.baidu，其中的.就是用于分割的，对应多个文件夹，比如com.test\",\"20240815234719\",\"我们之前都是直接创建的类，所以说没有包这个概念，但是现在，我们将类放到包中，就需要注意了： 需要通过关键字 package，用于指定当前类所处的包的，注意，所处的包和对应的目录是一一对应的。\",\"package com.test; //在放入包中，需要在类的最上面添加package关键字来指明当前类所处的包 public class Main { //将Main类放到com.test这个包中 public static void main(String[] args) { } } \",\"当我们使用同一个包中的类时，直接使用即可（之前就是直接使用的，因为都直接在一个缺省的包中） 而当我们需要使用其他包中的类时，需要先进行导入才可以： 需要通过关键字 import 导入我们需要使用的类，当然，只有在类不在同一个包下时才需要进行导入，如果一个包中有多个类，我们可以使用*表示导入这个包中全部的类:\",\"import com.test.entity.Person; //使用import关键字导入其他包中的类 import com.test.entity.*; \",\"Java会默认导入java.lang这个包下的所有类，因此我们不需要手动指定。\",\"不同类的重名问题 在不同包下的类，即使类名相同，也是不同的两个类：\",\"package com.test.entity; public class String { //我们在自己的包中也建一个名为String的类 } \",\"由于默认导入了系统自带的String类，并且也导入了我们自己定义的String类，那么此时就出现了歧义，编译器不知道到底我们想用的是哪一个String类，所以说我们需要明确指定：\",\"public class Main { public static void main(java.lang.String[] args) { //主方法的String参数是java.lang包下的，我们需要明确指定一下，只需要在类名前面添加包名就行了 com.test.entity.String string = new com.test.entity.String(); } } \",\"我们只需要在类名前面把完整的包名也给写上，就可以表示这个是哪一个包里的类了，当然，如果没有出现歧义，默认情况下包名是可以省略的，可写可不写。\"]},\"17\":{\"h\":\"访问权限控制\",\"t\":[\"Java中引入了访问权限控制（可见性），我们可以为成员变量、成员方法、静态变量、静态方法甚至是类指定访问权限，不同的访问权限，有着不同程度的访问限制：\",\"private - 私有，标记为私有的内容无法被除当前类以外的任何位置访问。\",\"什么都不写 - 默认，默认情况下，只能被类本身和同包中的其他类访问。\",\"protected - 受保护，标记为受保护的内容可以能被类本身和同包中的其他类访问，也可以被子类访问（子类我们会在下一章介绍）\",\"public - 公共，标记为公共的内容，允许在任何地方被访问。\",\"当前类\",\"同一个包下的类\",\"不同包下的子类\",\"不同包下的类\",\"public\",\"✅\",\"✅\",\"✅\",\"✅\",\"protected\",\"✅\",\"✅\",\"✅\",\"❌\",\"默认\",\"✅\",\"✅\",\"❌\",\"❌\",\"private\",\"✅\",\"❌\",\"❌\",\"❌\",\"默认的情况下，在当前包以外的其他包中无法访问。\",\"如果某个类中存在静态方法或是静态变量，那么我们可以通过静态导入的方式将其中的静态方法或是静态变量直接导入使用，但是同样需要有访问权限的情况下才可以：\",\"public class Person { String name; int age; String sex; public static void test(){ System.out.println(\\\"我是静态方法！\\\"); } } \",\"静态导入：\",\"import static com.test.entity.Person.test; //静态导入test方法 public class Main { public static void main(String[] args) { test(); //直接使用就可以，就像在这个类定义的方法一样 } } \"]},\"18\":{\"c\":[\"code\"]},\"19\":{\"c\":[\"java\"]},\"20\":{\"h\":\"Java - 类与对象3\"},\"21\":{\"h\":\"封装 继承和多态\",\"t\":[\"封装、继承和多态是面向对象编程的三大特性。\",\"封装，把对象的属性和方法结合成一个独立的整体，隐藏实现细节，并提供对外访问的接口。\",\"继承，从已知的一个类中派生出一个新的类，叫子类。子类实现了父类所有非私有化的属性和方法，并根据实际需求扩展出新的行为。\",\"多态，多个不同的对象对同一消息作出响应，同一消息根据不同的对象而采用各种不同的方法。\",\"正是这三大特性，让我们的Java程序更加生动形象。\"]},\"22\":{\"h\":\"封装\"},\"23\":{\"h\":\"继承\",\"t\":[\"父类是 super 子类是 this\"]},\"24\":{\"h\":\"Object 类\",\"t\":[\"Object 是最顶层的类，所有其他类都是继承它的 方法： euqals toString clone hashcode\",\"public class Object { private static native void registerNatives(); //标记为native的方法是本地方法，底层是由C++实现的 static { registerNatives(); //这个类在初始化时会对类中其他本地方法进行注册，本地方法不是我们SE中需要学习的内容，我们会在JVM篇视频教程中进行介绍 } //获取当前的类型Class对象，这个我们会在最后一章的反射中进行讲解，目前暂时不会用到 public final native Class<?> getClass(); //获取对象的哈希值，我们会在第五章集合类中使用到，目前各位小伙伴就暂时理解为会返回对象存放的内存地址 public native int hashCode(); //判断当前对象和给定对象是否相等，默认实现是直接用等号判断，也就是直接判断是否为同一个对象 public boolean equals(Object obj) { return (this == obj); } //克隆当前对象，可以将复制一个完全一样的对象出来，包括对象的各个属性 protected native Object clone() throws CloneNotSupportedException; //将当前对象转换为String的形式，默认情况下格式为 完整类名@十六进制哈希值 public String toString() { return getClass().getName() + \\\"@\\\" + Integer.toHexString(hashCode()); } //唤醒一个等待当前对象锁的线程，有关锁的内容，我们会在第六章多线程部分中讲解，目前暂时不会用到 public final native void notify(); //唤醒所有等待当前对象锁的线程，同上 public final native void notifyAll(); //使得持有当前对象锁的线程进入等待状态，同上 public final native void wait(long timeout) throws InterruptedException; //同上 public final void wait(long timeout, int nanos) throws InterruptedException { ... } //同上 public final void wait() throws InterruptedException { ... } //当对象被判定为已经不再使用的“垃圾”时，在回收之前，会由JVM来调用一次此方法进行资源释放之类的操作，这同样不是SE中需要学习的内容，这个方法我们会在JVM篇视频教程中详细介绍，目前暂时不会用到 protected void finalize() throws Throwable { } } \"]},\"25\":{\"h\":\"方法重写\",\"t\":[\"方法的重载是为某个方法提供更多种类 而方法的重写是覆盖原有的方法实现,重写方法要求与父类的定义完全一致 比如我们现在不希望使用Object类中提供的equals方法，那么我们就可以将其重写了\",\"public class Person{ ... @Override //重写方法可以添加 @Override 注解，有关注解我们会在最后一章进行介绍，这个注解默认情况下可以省略 public boolean equals(Object obj) { //重写方法要求与父类的定义完全一致 if(obj == null) return false; //如果传入的对象为null，那肯定不相等 if(obj instanceof Person) { //只有是当前类型的对象，才能进行比较，要是都不是这个类型还比什么 Person person = (Person) obj; //先转换为当前类型，接着我们对三个属性挨个进行比较 return this.name.equals(person.name) && //字符串内容的比较，不能使用==，必须使用equals方法 this.age == person.age && //基本类型的比较跟之前一样，直接== this.sex.equals(person.sex); } return false; } } \",\"在修改后 即使强制类型转换 但实际上还是在调用本身的方法\",\" Person p1 = new Student(\\\"小明\\\", 18, \\\"男\\\"); Person p2 = new Student(\\\"小明\\\", 18, \\\"男\\\"); // Object p1 = new Student(\\\"小明\\\", 18, \\\"男\\\"); // Object p2 = new Student(\\\"小明\\\", 18, \\\"男\\\"); System.out.println(p1.equals(p2)); //此时由于三个属性完全一致，所以说判断结果为真，即使是两个不同的对象 \",\"我们在重写父类方法时，如果希望调用父类原本的方法实现，那么同样可以使用 super 关键字： satic 成员方法中不能用 super\",\"@Override public void exam() { super.exam(); //调用父类的实现 System.out.println(\\\"我是工人，做题我并不擅长，只能得到 D\\\"); } \",\"如果父类的方法是 private, 那么无法重写\"]},\"26\":{\"h\":\"控制符\",\"t\":[\"final 对于成员变量，则表示只能赋一次值。只能在构造函数进行赋值(如果有初始值，构造函数也不能赋值)，其他地方不能修改 对于成员方法，会限制其子类不允许其重写所对应的成员变量 在 类 上 加 final, 表示这个类不能再被继承了\"]},\"27\":{\"h\":\"抽象类\",\"t\":[\"抽象类具有 抽象方法，正常实例化方法是无法创造抽象类的实例\",\"抽象方法是指：只保留方法的定义，并不编写方法的主体，具体的实现由 子类 来实现.\",\"要使用抽象类，我们只能去创建它的子类对象。\",\"抽象类一般只用作继承使用，当然，抽象类的子类也可以是一个抽象类\",\"抽象方法的访问权限不能为 private, 因为抽象方法一定要由子类实现，如果子类都访问不了，那么还有什么意义呢？所以说不能为私有。\",\"public abstract class Person { //通过添加abstract关键字，表示这个类是一个抽象类 protected String name; //大体内容其实普通类差不多 protected int age; protected String sex; protected String profession; protected Person(String name, int age, String sex, String profession) { this.name = name; this.age = age; this.sex = sex; this.profession = profession; } public abstract void exam(); //抽象类中可以具有抽象方法，也就是说这个方法只有定义，没有方法体 } \",\"而具体的实现，需要由子类来完成，而且如果是子类，必须要实现抽象类中所有抽象方法 不过如果子类也是抽象类，就不一定需要实现。\",\"public class Worker extends Person{ public Worker(String name, int age, String sex) { super(name, age, sex, \\\"工人\\\"); } @Override public void exam() { //子类必须要实现抽象类所有的抽象方法，这是强制要求的，否则会无法通过编译 System.out.println(\\\"我是工人，做题我并不擅长，只能得到 D\\\"); } } \",\"发现对于 抽象类 中定义的 抽象方法，其子类的对应的方法的访问权限需要高于抽象类中的方法，且同样不能使用 private。 即 如果抽象方法在抽象类定义的是 public, 子类对应必须是 public 不能是 protected; 而如果抽象类定义的是 protected，子类也可以定义 public\"]},\"28\":{\"h\":\"接口\",\"t\":[\"接口甚至比抽象类还抽象，他只代表某个确切的功能！也就是只包含方法的定义，甚至都不是一个类！\",\"接口一般只代表某些功能的抽象，接口包含了一些列方法的定义，类可以实现这个接口，表示类支持接口代表的功能（类似于一个插件，只能作为一个附属功能加在主体上，同时具体实现还需要由主体来实现）\",\"实际上接口的目标就是将类所具有某些的行为抽象出来。\",\"可以理解为 接口 相当于 只有抽象类中的抽象方法，甚至都不是一个类了。\",\"接口里只能定义对应的抽象方法，不过可以省略 abstract 定义 并且默认在类中实现的权限是 public\",\"定义接口 interface\",\"实现接口 implements\",\"接口可以实现很多个，只需要用 逗号 隔开即可，类只能继承一个、\",\"所以说有些人说接口其实就是Java中的多继承，但是我个人认为这种说法是错的，实际上实现接口更像是一个类的功能列表，作为附加功能存在，一个类可以附加很多个功能，接口的使用和继承的概念有一定的出入，顶多说是多继承的一种替代方案。\",\"java8开始，接口中的方法可以存在默认实现，default 如果方法在接口中存在默认实现，那么实现类中不强制要求进行实现。\",\"接口不同于类，接口中不允许存在成员变量和成员方法，但是可以存在静态变量和静态方法 接口中定义的静态变量只能是public static final的 接口中定义的静态方法也只能是public的 这些可以省 直接int a = 1static void test()这种即可 跟普通的类一样，我们可以直接通过接口名.的方式使用静态内容\",\"接口是可以继承 (extends) 自其他接口的, 并且接口没有继承数量限制，接口支持多继承 接口的继承相当于是对接口功能的融合罢了\",\"接口的默认方法是保底的，只要一个类的父类或者自身有对应方法，就不会执行接口的默认方法\",\"接口中如果定义了与 Object 同名的方法，不能使用默认，因为其他类就算继承这个接口，由于类本身都是继承 Object 的，这个默认方法没有任何作用\",\"比如说，对于人类的不同子类，学生和老师来说，他们都具有学习这个能力，既然都有，那么我们就可以将学习这个能力，抽象成接口来进行使用，只要是实现这个接口的类，都有学习的能力：\",\"接口定义:\",\"public interface Study { //使用interface表示这是一个接口 void study(); //接口中只能定义访问权限为public抽象方法，其中public和abstract关键字可以省略 } \",\"让类来使用这个接口\",\"public class Student extends Person implements Study { //使用implements关键字来实现接口 public Student(String name, int age, String sex) { super(name, age, sex, \\\"学生\\\"); } @Override public void study() { //实现接口时，同样需要将接口中所有的抽象方法全部实现 System.out.println(\\\"我会学习！\\\"); } } \",\"接口跟抽象类一样，不能直接创建对象，但是我们也可以将接口实现类的对象以接口的形式去使用，即\",\"public class Main { public static void main(String[] args) { Study study = new Teacher(\\\"penguin\\\",18,\\\"male\\\"); study.study() //这里的话只能使用接口中的方法，以及Object的方法 } } \",\"接口同样支持向下转型：\",\"public static void main(String[] args) { Study study = new Teacher(\\\"小王\\\", 27, \\\"男\\\"); if(study instanceof Teacher) { //直接判断引用的对象是不是Teacher类型 Teacher teacher = (Teacher) study; //强制类型转换 teacher.study(); } } \",\"从Java8开始，接口中可以存在让抽象方法的默认实现：\",\"public interface Study { void study(); default void test() { //使用default关键字为接口中的方法添加默认实现 System.out.println(\\\"我是默认实现\\\"); } } \"]},\"29\":{\"h\":\"Object类中的 克隆方法\",\"t\":[\"这是浅拷贝，克隆出来的与原来的对象不是一个对象，但对象中的属性都是同一个地址\",\"克隆操作可以完全复制一个对象的所有属性，但是像这样的拷贝操作其实也分为浅拷贝和深拷贝。\",\"浅拷贝： 对于类中基本数据类型，会直接复制值给拷贝对象；对于引用类型，只会复制对象的地址，而实际上指向的还是原来的那个对象，拷贝个基莫。\",\"深拷贝： 无论是基本类型还是引用类型，深拷贝会将引用类型的所有内容，全部拷贝为一个新的对象，包括对象内部的所有成员变量，也会进行拷贝。\",\"package java.lang; public interface Cloneable { //这个接口中什么都没定义 } \",\"具体实现克隆:\",\"public class Student extends Person implements Study, Cloneable { //首先实现Cloneable接口，表示这个类具有克隆的功能 public Student(String name, int age, String sex) { super(name, age, sex, \\\"学生\\\"); } @Override public Object clone() throws CloneNotSupportedException { //提升clone方法的访问权限 return super.clone(); //因为底层是C++实现，我们直接调用父类的实现就可以了 } @Override public void study() { System.out.println(\\\"我会学习！\\\"); } } \",\"克隆实现：\",\"public static void main(String[] args) throws CloneNotSupportedException { //这里向上抛出一下异常，还没学异常，所以说照着写就行了 Student student = new Student(\\\"小明\\\", 18, \\\"男\\\"); Student clone = (Student) student.clone(); //调用clone方法，得到一个克隆的对象 System.out.println(student); System.out.println(clone); System.out.println(student == clone); } \"]},\"30\":{\"h\":\"枚举类\",\"t\":[\"public enum Status { //enum表示这是一个枚举类，枚举类的语法稍微有一些不一样 RUNNING, STUDY, SLEEP; //直接写每个状态的名字即可，最后面分号可以不打，但是推荐打上 } \",\"使用枚举类也非常方便，就像使用普通类型那样：\",\"public class Student { private Status status; //状态，可以是跑步、学习、睡觉这三个之中的其中一种 public Status getStatus() { return status; } public void setStatus(Status status) { this.status = status; } } \",\"使用就像对象的参数一样:\",\"Status.RUNNING Status.STUDY Status.SLEEP \",\"枚举类型使用起来就非常方便了，其实枚举类型的本质就是一个普通的类，但是它继承自Enum类，我们定义的每一个状态其实就是一个public static final的Status类型成员变量：\",\"//这里使用javap命令对class文件进行反编译得到 Compiled from \\\"Status.java\\\" public final class com.test.Status extends java.lang.Enum<com.test.Status> { public static final com.test.Status RUNNING; public static final com.test.Status STUDY; public static final com.test.Status SLEEP; public static com.test.Status[] values(); public static com.test.Status valueOf(java.lang.String); static {}; } \",\"枚举类型是普通的类，那么我们也可以给枚举类型添加独有的成员方法：\",\"public enum Status { RUNNING(\\\"睡觉\\\"), STUDY(\\\"学习\\\"), SLEEP(\\\"睡觉\\\"); //无参构造方法被覆盖，创建枚举需要添加参数（本质就是调用的构造方法） private final String name; //枚举的成员变量 Status(String name){ //覆盖原有构造方法（默认private，只能内部使用！） this.name = name; } public String getName() { //获取封装的成员变量 return name; } } \",\"public static void main(String[] args) { Student student = new Student(\\\"小明\\\", 18, \\\"男\\\"); student.setStatus(Status.RUNNING); System.out.println(student.getStatus().getName()); } \"]},\"31\":{\"c\":[\"code\"]},\"32\":{\"c\":[\"java\"]},\"33\":{\"h\":\"Java - 类与对象4\"},\"34\":{\"h\":\"面向对象高级篇1\"},\"35\":{\"h\":\"基本类型包装类\",\"t\":[\"Java并不是纯面向对象的语言，虽然Java语言是一个面向对象的语言，但是Java中的基本数据类型却不是面向对象的。Java中的基本类型，如果想通过对象的形式去使用他们，Java提供的基本类型包装类，使得Java能够更好的体现面向对象的思想，同时也使得基本类型能够支持对象操作！\"]},\"36\":{\"h\":\"所有包装类如下\",\"t\":[\"20241017002218\",\"其中能够表示数字的基本类型包装类，继承自Number类，对应关系如下表：\",\"byte -> Byte\",\"boolean -> Boolean\",\"short -> Short\",\"char -> Character\",\"int -> Integer\",\"long -> Long\",\"float -> Float\",\"double -> Double\",\"包装类型的自动装箱和拆箱机制 包装类实际上就是将我们的基本数据类型，封装成一个类（运用了封装的思想） 包装类型支持自动装箱，我们可以直接将一个对应的基本类型值作为对应包装类型引用变量的值：\",\" public static void main(String[] args) { Integer i = 10; //将int类型值作为包装类型使用 // 不需要 Integer i = new Integer(10) // 这里本质上就是被自动包装成了一个Integer类型的对象， // 只是语法上为了简单，就支持像这样编写。既然能装箱，也是支持拆箱的 Integer i = 10; int a = i; } \",\"因为包装类是一个类，不是基本类型，所以说两个不同的对象，那么是不相等的：\",\" public static void main(String[] args) { Integer a = new Integer(10); Integer b = new Integer(10); System.out.println(a == b); //虽然a和b的值相同，但是并不是同一个对象，所以说==判断为假 } \",\"那么自动装箱的呢？\",\" public static void main(String[] args) { Integer a = 128, b = 128; System.out.println(a == b); } \",\"我们发现，通过自动装箱转换的Integer对象，如果值相同，得到的会是同一个对象，这是因为：\",\"public static Integer valueOf(int i) { if (i >= IntegerCache.low && i <= IntegerCache.high) //这里会有一个IntegerCache，如果在范围内，那么会直接返回已经提前创建好的对象 return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } \",\"IntegerCache会默认缓存-128~127之间的所有值，将这些值提前做成包装类放在数组中存放，虽然我们目前还没有学习数组，但是各位小伙伴只需要知道，我们如果直接让 -128~127之间的值自动装箱为Integer类型的对象，那么始终都会得到同一个对象，这是为了提升效率，因为小的数使用频率非常高，有些时候并不需要创建那么多对象，创建对象越多，内存也会消耗更多。\",\"但是如果超出这个缓存范围的话，就会得到不同的对象了：\",\"public static void main(String[] args) { Integer a = 128, b = 128; System.out.println(a == b); } \",\"这样就不会得到同一个对象了，因为超出了缓存的范围。同样的，Long、Short、Byte类型的包装类也有类似的机制。\"]},\"37\":{\"h\":\"包装类的方法\",\"t\":[\"包装类支持字符串直接转换\",\" public static void main(String[] args) { Integer i = new Integer(\\\"666\\\"); //直接将字符串的666，转换为数字666 System.out.println(i); // 字符串转Integer有多个方法： Integer i = Integer.valueOf(\\\"5555\\\"); Integer i = Integer.decode(\\\"0xA6\\\"); Integer.toHexString(166) } \"]},\"38\":{\"h\":\"特殊包装类\",\"t\":[\"BigInteger 和 BigDecimal 都在 java.math 中\",\"Void类 没啥意义 不能 new 只能 Void v = null\",\"BigInteger import java.math.BigInteger 用于计算超大数字，即使是最大的long类型，也只能表示64bit的数据，无法表示一个非常大的数，但是BigInteger没有这些限制，我们可以让他等于一个非常大的数字。 但不支持 自动装箱|拆箱机制，计算的话也只能通过 BigInteger 提供的方法进行计算。 一般情况，对于非常大的整数计算，我们就可以使用BigInteger来完成。\",\" public static void main(String[] args) { BigInteger i = BigInteger.valueOf(Long.MAX_VALUE); //表示Long的最大值，轻轻松松 System.out.println(i); BigInteger h = BigInteger.valueOf(100) // 乘法 BigInteger a = h.multiply(BigInteger.TEN) } \",\"BigDecimal import java.math.BigDecimal 浮点类型精度有限，对于需要精确计算的场景，就没办法了，而BigDecimal可以实现小数的精确计算\",\"public static void main(String[] args) { BigDecimal i = BigDecimal.valueOf(10); i = i.divide(BigDecimal.valueOf(3), 100, RoundingMode.CEILING); //计算10/3的结果，精确到小数点后100位 //RoundingMode是舍入模式，就是精确到最后一位时，该怎么处理，这里CEILING表示向上取整 System.out.println(i); } \"]},\"39\":{\"c\":[\"code\"]},\"40\":{\"c\":[\"java\"]},\"41\":{\"h\":\"Java - 数组 | 字符串 | 正则表达式\"},\"42\":{\"h\":\"面向对象高级篇 2\"},\"43\":{\"h\":\"数组\",\"t\":[\"数组是相同类型数据的有序集合，数组可以代表任何相同类型的一组内容（包括引用类型和基本类型）其中存放的每一个数据称为数组的一个元素。\"]},\"44\":{\"h\":\"定义\",\"t\":[\"数组类型比较特殊，它本身也是类，但是编程不可见（底层C++写的，在运行时动态创建） 即使是基本类型的数组，也是以对象的形式存在的，并不是基本数据类型。所以，我们要创建一个数组，同样需要使用 new 关键字\",\"public static void main(String[] args) { int[] array = new int[10]; //类型[]就表示这个是一个数组类型 Object obj = array; //因为同样是类，肯定是继承自Object的，所以说可以直接向上转型 } \",\"创建出来的数组每个位置上都有默认值，如果是引用类型，就是null，如果是基本数据类型，就是0，或者是false，跟对象成员变量的默认值是一样的 其他定义方法:\",\"类型[] 变量名称 = new 类型[数组大小]; 类型 变量名称[] = new 类型[数组大小]; //支持C语言样式，但不推荐！ 类型[] 变量名称 = new 类型[]{...}; //静态初始化（直接指定值和大小） 类型[] 变量名称 = {...}; //同上，但是只能在定义时赋值 \"]},\"45\":{\"h\":\"方法\",\"t\":[\"数组的 length 是在一开始就确定的，而且是 final类型 的，不允许进行修改，也就是说数组的长度一旦确定，不能随便进行修改，如果需要使用更大的数组，只能重新创建。\",\" public static void main(String[] args) { int[] array = new int[10]; System.out.println(\\\"当前数组长度为：\\\"+array.length); //length属性是int类型的值，表示当前数组长度，长度是在一开始创建数组的时候就确定好的 } \",\"array 虽然是继承于 Object，但是，很遗憾，除了clone()之外，这些方法并没有被重写，也就是说依然是采用的Object中的默认实现，所以可能不满足真实需求。\"]},\"46\":{\"h\":\"访问元素\",\"t\":[\"for 或者 foreach\",\" public static void main(String[] args) { int[] array = new int[10]; for (int i = 0; i < array.length; i++) { System.out.print(array[i] + \\\" \\\"); } for (int i : array) { //int i就是每一个数组中的元素，array就是我们要遍历的数组 System.out.print(i+\\\" \\\"); //每一轮循环，i都会更新成数组中下一个元素 } } \"]},\"47\":{\"h\":\"特性\",\"t\":[\"这里需要特别说一下，对于基本类型的数组来说，是不支持自动装箱和拆箱的：\",\" public static void main(String[] args) { int[] arr = new int[10]; Integer[] test = arr; // 这样是不能赋值的 会报错 } \",\"由于基本数据类型和引用类型不同，所以说int类型的数组时不能被Object类型的数组变量接收的, 即 int[] arr = new int[10] 是不能 Object[] arrav = arr 这样的\",\"但是如果是引用类型的话，是可以的,因为父类都是 Object\",\" public static void main(String[] args) { String[] arr = new String[10]; Object[] array = arr; //数组同样支持向上转型 Object[] arr = new Object[10]; String[] array = (String[]) arr; //也支持向下转型 } \"]},\"48\":{\"h\":\"性质\",\"t\":[\" public static void main(String[] args) { final int[] a = {1,2,3,4}; // 值还是可以修改，只是数组的地址不准修改 a[0] = 4; // 允许 a = {2,4,8,7}; //不允许 } \"]},\"49\":{\"h\":\"多维数组\",\"t\":[\"既然数组可以是任何类型的，那么我们能否创建数组类型的数组呢？答案是可以的\",\"public static void main(String[] args) { int[][] arr = { {1, 2}, {3, 4}, {5, 6}}; //一个三行两列的数组 System.out.println(arr[2][1]); //访问第三行第二列的元素 } \"]},\"50\":{\"h\":\"可变长参数\",\"t\":[\"public void function(参数类型...参数名称) 这样参数名称所表示的就是一个数组\",\"public class Person { String name; int age; String sex; // 可以传入 0 - N 个类型的实参 public void test(String... strings){ //strings这个变量就是一个String[]类型的 for (String string : strings) { System.out.println(string); //遍历打印数组中每一个元素 } } } public static void main(String[] args) { Person person = new Person(); person.test(\\\"1！\\\", \\\"5！\\\", \\\"哥们在这跟你说唱\\\"); //这里我们可以自由传入任意数量的字符串 } \",\"注意，如果同时存在其他参数，那么可变长参数只能放在最后：\",\"public void test(int a, int b, String... strings){ } \"]},\"51\":{\"h\":\"函数的\",\"t\":[\"public static void main(String[] args) { for (String arg : args) { System.out.println(arg); } } \",\"可以看到，默认情况下直接运行什么都没有，但是如果我们在运行时，添加点内容的话：java com/test/Main lbwnb aaaa xxxxx 因此会读取命令行中的指令参数进行存储到 args 中。\"]},\"52\":{\"h\":\"字符串\",\"t\":[\"字符串类是一个比较特殊的类，它用于保存字符串。 我们知道，基本类型 char 可以保存一个2字节的Unicode字符，而字符串则是一系列字符的序列（在C中就是一个字符数组） Java中没有字符串这种基本类型，因此只能使用类来进行定义。 注意，字符串中的字符一旦确定，无法进行修改，只能重新创建。\"]},\"53\":{\"h\":\"String 类\",\"t\":[\"String本身也是一个类，只不过它比较特殊，每个用双引号括起来的字符串，都是String类型的一个实例对象, 也可以象征性 new 不过没必要 如果是直接使用双引号创建的字符串，如果内容相同，为了优化效率，那么始终都是同一个对象 但是如果我们使用构造方法主动创建两个新的对象，那么就是不同的对象了\",\"public static void main(String[] args) { String str1 = \\\"Hello World\\\"; String str2 = \\\"Hello World\\\"; System.out.println(str1 == str2); // 这样就不同 String str3 = new String(\\\"Hello World\\\"); String str4 = new String(\\\"Hello World\\\"); System.out.println(str3 == str4); System.out.println(str1.equals(str2)); //字符串的内容比较，一定要用equals } \",\"因此，如果我们仅仅是想要判断两个字符串的内容是否相同，不要使用 ==， String类重载了equals方法用于判断和比较内容是否相同\",\"获取长度 str.length()\",\"public static void main(String[] args) { String str = \\\"Hello World\\\"; System.out.println(str.length()); //length方法可以求字符串长度，这个长度是字符的数量 } \",\"字符串类中提供了很多方便我们操作的方法， 比如字符串的裁剪 (substring)、分割操作 (split)\",\" public static void main(String[] args) { String str = \\\"Hello World\\\"; String sub = str.substring(0, 3); //分割字符串，并返回一个新的子串对象 System.out.println(sub); String[] strings = str.split(\\\" \\\"); //使用split方法进行字符串分割，比如这里就是通过空格分隔，得到一个字符串数组 for (String string : strings) { System.out.println(string); } } \",\"字符数组和字符串之间是可以快速进行相互转换的字符串转字符数组: char[] chars = str.toCharArray()字符数组转字符串: String str = new String(chars)\"]},\"54\":{\"h\":\"StringBuilder 类\",\"t\":[\"StringBuilder 就是专门用于构造字符串的，我们可以使用它来对字符串进行拼接、裁剪等操作，它就像一个字符串编辑器，弥补了字符串不能修改的不足\",\"public static void main(String[] args) { StringBuilder builder = new StringBuilder(); //一开始创建时，内部什么都没有 builder.append(\\\"AAA\\\"); //我们可以使用append方法来讲字符串拼接到后面 builder.append(\\\"BBB\\\"); builder.delete(2, 4); //删除2到4这个范围内的字符 System.out.println(builder.toString()); //当我们字符串编辑完成之后，就可以使用toString转换为字符串了 } \",\"字符串支持使用 + 和 += 进行拼接操作, 但是拼接字符串实际上底层需要进行很多操作，如果程序中大量进行字符串的拼接似乎不太好，编译器是很聪明的，String的拼接会在编译时进行各种优化：\",\"对于变量来说\",\"public static void main(String[] args) { String str1 = \\\"你看\\\"; String str2 = \\\"这\\\"; String str3 = \\\"汉堡\\\"; String str4 = \\\"做滴\\\"; String str5 = \\\"行不行\\\"; String result = str1 + str2 + str3 + str4 + str5; //5个变量连续加 System.out.println(result); } \",\"如果直接使用加的话，每次运算都会生成一个新的对象，这里进行4次加法运算，那么中间就需要产生4个字符串对象出来，是不是有点太浪费了？ 这种情况实际上会被优化为下面的写法：\",\"public static void main(String[] args) { String str1 = \\\"你看\\\"; String str2 = \\\"这\\\"; String str3 = \\\"汉堡\\\"; String str4 = \\\"做滴\\\"; String str5 = \\\"行不行\\\"; StringBuilder builder = new StringBuilder(); builder.append(str1).append(str2).append(str3).append(str4).append(str5); System.out.println(builder.toString()); } \"]},\"55\":{\"h\":\"正则表达式\",\"t\":[\"str.match(正则表达式)\",\"正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等。\",\"public static void main(String[] args) { String str = \\\"oooo\\\"; //matches方法用于对给定正则表达式进行匹配，匹配成功返回true，否则返回false System.out.println(str.matches(\\\"o+\\\")); //+表示对前面这个字符匹配一次或多次，这里字符串是oooo，正好可以匹配 } \",\"用于规定给定组件必须要出现多少次才能满足匹配的，我们一般称为限定符，限定符表如下：\",\"字符\",\"描述\",\"*\",\"匹配前面的子表达式零次或多次。例如，zo* 能匹配 \\\"z\\\" 以及 \\\"zoo\\\"。***** 等价于 {0,}。\",\"+\",\"匹配前面的子表达式一次或多次。例如，zo+ 能匹配 \\\"zo\\\" 以及 \\\"zoo\\\"，但不能匹配 \\\"z\\\"。+ 等价于 {1,}。\",\"?\",\"匹配前面的子表达式零次或一次。例如，do(es)? 可以匹配 \\\"do\\\" 、 \\\"does\\\"、 \\\"doxy\\\" 中的 \\\"do\\\" 。? 等价于 {0,1}。\",\"n 是一个非负整数。匹配确定的 n 次。例如，o{2} 不能匹配 \\\"Bob\\\" 中的 o，但是能匹配 \\\"food\\\" 中的两个 o。\",\"n 是一个非负整数。至少匹配n 次。例如，o{2,} 不能匹配 \\\"Bob\\\" 中的 o，但能匹配 \\\"foooood\\\" 中的所有 o。o{1,} 等价于 o+。o{0,} 则等价于 o*。\",\"m 和 n 均为非负整数，其中 n <= m。最少匹配 n 次且最多匹配 m 次。例如，o{1,3} 将匹配 \\\"fooooood\\\" 中的前三个 o。o{0,1} 等价于 o?。请注意在逗号和两个数之间不能有空格。\",\"如果我们想要表示一个范围内的字符，可以使用方括号：\",\"public static void main(String[] args) { String str = \\\"abcabccaa\\\"; System.out.println(str.matches(\\\"[abc]*\\\")); //表示abc这几个字符可以出现 0 - N 次 } \",\"对于普通字符来说，我们可以下面的方式实现多种字符匹配：\",\"字符\",\"描述\",\"[ABC]\",\"匹配 [...] 中的所有字符，例如 [aeiou] 匹配字符串 \\\"google runoob taobao\\\" 中所有的 e o u a 字母。\",\"[^ABC]\",\"匹配除了 [...] 中字符的所有字符，例如 [^aeiou] 匹配字符串 \\\"google runoob taobao\\\" 中除了 e o u a 字母的所有字母。\",\"[A-Z]\",\"[A-Z] 表示一个区间，匹配所有大写字母，[a-z] 表示所有小写字母。\",\".\",\"匹配除换行符（\\\\n、\\\\r）之外的任何单个字符，相等于 [^\\\\n\\\\r]\",\"[\\\\s\\\\S]\",\"匹配所有。\\\\s 是匹配所有空白符，包括换行，\\\\S 非空白符，不包括换行。\",\"\\\\w\",\"匹配字母、数字、下划线。等价于 [A-Za-z0-9_]\",\"当然，这里仅仅是对正则表达式的简单使用，实际上正则表达式内容非常多，如果需要完整学习正则表达式，可以到：https://www.runoob.com/regexp/regexp-syntax.html\",\"正则表达式并不是只有Java才支持，其他很多语言比如JavaScript、Python等等都是支持正则表达式的。\"]},\"56\":{\"c\":[\"code\"]},\"57\":{\"c\":[\"java\"]},\"58\":{\"h\":\"Java - 内部类\"},\"59\":{\"h\":\"面向对象高级篇 3\",\"t\":[\"静态 属于 类， 成员 属于 对象。\"]},\"60\":{\"h\":\"内部类\",\"t\":[\"内部类顾名思义，就是创建在内部的类。\"]},\"61\":{\"h\":\"成员内部类 (属于 对象)\",\"t\":[\"成员内部类其实在某些情况下使用起来比较麻烦，对于这种成员内部类，我们一般只会在类的内部自己使用\",\"成员内部类和成员方法、成员变量一样，是对象所有的，而不是类所有的， 如果我们要使用成员内部类，那么就需要创造一个对象，才能去 new 一个成员内部类。\",\"我们可以直接在类的内部定义成员内部类：\",\"public class Test { public class Inner { //内部类也是类，所以说里面也可以有成员变量、方法等，甚至还可以继续套娃一个成员内部类 public void test(){ System.out.println(\\\"我是成员内部类！\\\"); } } } \",\"public static void main(String[] args) { Test test = new Test(); //我们首先需要创建对象 Test.Inner inner = test.new Inner(); //成员内部类的类型名称就是 外层.内部类名称 inner.test(); } \",\"注意，成员内部类也可以使用访问权限控制，如果我们我们将其权限改为private，那么就像我们把成员变量访问权限变成私有一样，外部是无法访问到这个内部类的.\",\"这里我们需要特别注意一下，在成员内部类中，是可以访问到外层的变量的\",\"public class Test { private final String name; public Test(String name){ this.name = name; } public class Inner { public void test(){ System.out.println(\\\"我是成员内部类：\\\"+name); //成员内部类可以访问到外部的成员变量 //因为成员内部类本身就是某个对象所有的，每个对象都有这样的一个类定义，这里的name是其所依附对象的 } } } \",\"每个类可以创建一个对象，每个对象中都有一个单独的类定义，可以通过这个成员内部类又创建出更多对象，套娃了属于是。\",\"20241027012950\",\"public static void main(String[] args) { Test a = new Test(\\\"小明\\\"); Test.Inner inner1 = a.new Inner(); //依附于a创建的对象，那么就是a的 inner1.test(); Test b = new Test(\\\"小红\\\"); Test.Inner inner2 = b.new Inner(); //依附于b创建的对象，那么就是b的 inner2.test(); } \",\"那么如果内部类中也定义了同名的变量，此时我们怎么去明确要使用的是哪一个 (就近原则) 如果需要指定为外部的对象，那么需要在前面添加外部类型名称Test.this.\",\"public class Test { private final String name; public Test(String name){ this.name = name; } public class Inner { String name; public void test(String name){ System.out.println(\\\"方法参数的name = \\\"+name); //依然是就近原则，最近的是参数，那就是参数了 System.out.println(\\\"成员内部类的name = \\\"+this.name); //在内部类中使用this关键字，只能表示内部类对象 System.out.println(\\\"成员内部类的name = \\\"+Test.this.name); //如果需要指定为外部的对象，那么需要在前面添加外部类型名称 } } \",\"包括对方法的调用和super关键字的使用，也是一样的：\",\"public class Inner { String name; public void test(String name){ this.toString(); //内部类自己的toString方法 super.toString(); //内部类父类的toString方法 Test.this.toString(); //外部类的toSrting方法 Test.super.toString(); //外部类父类的toString方法 } } \"]},\"62\":{\"h\":\"静态内部类 (属于 类)\",\"t\":[\"静态内部类就像静态方法和静态变量一样，是属于类的，不需要，依附任何对象我们可以直接创建使用。\",\"public class Test { private final String name; public Test(String name){ this.name = name; } public static class Inner { public void test(){ System.out.println(\\\"我是静态内部类！\\\"); } } } \",\"不需要依附任何对象，我们可以直接创建静态内部类的对象：\",\"public static void main(String[] args) { Test.Inner inner = new Test.Inner(); //静态内部类的类名同样是之前的格式，但是可以直接new了 inner.test(); } \",\"静态内部类由于是静态的，所以相对外部来说，整个内部类中都处于静态上下文（注意只是相当于外部来说）是无法访问到外部类的非静态内容的\"]},\"63\":{\"h\":\"局部内部类\",\"t\":[\"(这种局部内部类的形式，使用频率很低，基本上不会用到) 局部内部类就像局部变量一样，可以在方法中定义。 基本定义是：(不需要声明访问权限，因为它作用范围就只是方法内)\",\"public 方法名(){ class 局部内部类名 { 内容 } } \",\"既然是在方法中声明的类，那作用范围也就只能在方法中了\",\"public class Test { private final String name; public Test(String name){ this.name = name; } public void hello(){ class Inner { //直接在方法中创建局部内部类 public void test(){ System.out.println(\\\"我是局部内部类\\\"); } } Inner inner = new Inner(); //局部内部类直接使用类名就行 inner.test(); } } \"]},\"64\":{\"h\":\"静态内部类编译特性\",\"t\":[\"package com.test; import com.test.entity.Test; public class Main { public static void main(String[] args) { Test.Inner.test(); } } \",\"package com.test.entity; public class Test { static { System.out.println(\\\"外部类初始化\\\"); } public static class Inner { static { System.out.println(\\\"内部类初始化\\\"); } public static void test(){ System.out.println(\\\"内部类静态方法\\\"); } } } \",\"结果是：\",\"20241027015244\",\"说明这种情况下，只是初始化了内部类的，并没有初始化内部类，因为并没有使用到外部类的任何静态变量，所以只初始化了内部类。 因为在编译的时候，类的内部类它会单独生成一个 .class，当你使用内部类静态方法，不会调用外部类的 class 只有在你使用到外部类的静态变量或方法后，才会初始化外部类, 但还是先初始化内部类。\",\"package com.test.entity; public class Test { public static String name = \\\"penguin\\\"; static { System.out.println(\\\"外部类初始化\\\"); } public static class Inner { static { System.out.println(\\\"内部类初始化\\\"); } public static void test(){ System.out.println(\\\"使用外部静态变量\\\" + name); System.out.println(\\\"内部类静态方法\\\"); } } } \",\"输出为：\",\"20241027015718\"]},\"65\":{\"h\":\"匿名内部类\",\"t\":[\"匿名内部类是我们使用频率非常高的一种内部类，它是局部内部类的简化版。\",\"不能直接通过 new 的方式去创建一个抽象类或是接口对象，正常情况下，要创建一个抽象类的实例对象，只能对其进行继承，先实现未实现的方法，然后创建子类对象。\",\"但我们可以在方法中使用匿名内部类，将其中的抽象方法实现，并直接创建实例对象\",\"public abstract class Student { public abstract void test(); } \",\"public static void main(String[] args) { Student student = new Student() { //在new的时候，后面加上花括号，把未实现的方法实现了 @Override public void test() { System.out.println(\\\"我是匿名内部类的实现!\\\"); } }; student.test(); } \",\"此时这里创建出来的Student对象，就是一个已经实现了抽象方法的对象，这个抽象类直接就定义好了，甚至连名字都没有，就可以直接就创出对象。 匿名内部类中同样可以使用类中的属性（因为它本质上就相当于是对应类型的子类）\",\"同样的，接口也可以通过这种匿名内部类的形式，直接创建一个匿名的接口实现类 这样就是一个实现 接口 中方法 的匿名类，但类名必须与接口一致。\",\"package com.test.entity; public interface Study { void study(); } \",\"public class Main { public static void main(String[] args) { Study ss = new Study() { @Override public void study() { System.out.println(\\\"penguin\\\"); } }; ss.study(); } } \",\"匿名对象本身不能定义新的属性。匿名对象的类是在创建时匿名生成的，但它继承自一个现有的类或实现了一个接口。因此，匿名类只能访问其父类的属性或方法，无法直接定义新的属性.\",\"在 Java 中，匿名对象通常不能直接给属性赋值，因为匿名对象没有类名，无法显式定义构造函数或初始化块 因此，为了在创建匿名对象时进行属性初始化，Java 提供了一种特殊的语法，即使用初始化块 {}。\",\"当然，并不是说只有抽象类和接口才可以像这样创建匿名内部类，普通的类也可以，只不过意义不大，一般情况下只是为了进行一些额外的初始化工作而已。 类似：\",\"package com.test.entity; public class Penguin { protected String apple; public void test(){ System.out.println(apple); } } \",\"public class Main { public static void main(String[] args) { Penguin pp = new Penguin(){ { apple = \\\"ppp\\\"; } }; pp.test(); } } \"]},\"66\":{\"h\":\"匿名内部类特性\",\"t\":[\"对于 匿名内部类 或者 Lambda 中，如果想用外部变量，只能使用 final 的变量，如果不是 final，会隐式修改为 final 即之后不能修改\",\"public static void main(String[] args) { int a = 10; // a = 20; // 如果修改了a 就会报错 Study pp = new Study{ @Override public void study(){ System.out.println(a); } } pp.test(); } \"]},\"67\":{\"h\":\"Lambda表达式\",\"t\":[\"如果一个接口中有且只有一个待实现的抽象方法，那么我们可以将匿名内部类简写为Lambda表达式\",\"public static void main(String[] args) { Study study = () -> System.out.println(\\\"我是学习方法！\\\"); //是不是感觉非常简洁！ study.study(); } \",\"Lambda表达式的具体规范：\",\"标准格式为：([参数类型 参数名称,]...) ‐> { 代码语句，包括返回值 }\",\"和匿名内部类不同，Lambda 仅支持接口，不支持抽象类\",\"接口内部必须有且仅有一个抽象方法（可以有多个方法，但是必须保证其他方法有默认实现，必须留一个抽象方法出来）\",\"如果有一个参数和返回值的话：\",\"public static void main(String[] args) { Study study = (a) -> { System.out.println(\\\"我是学习方法\\\"); return \\\"今天学会了\\\"+a; //实际上这里面就是方法体，该咋写咋写 }; System.out.println(study.study(10)); } \",\"如果参数只有一个，那么可以省去小括号 如果方法体中只有一个返回语句，可以直接省去花括号和return关键字\",\"Study study = (a) -> { return \\\"今天学会了\\\"+a; //这种情况是可以简化的 }; ==================================== Study study = (a) -> \\\"今天学会了\\\"+a; ==================================== Study study = a -> \\\"今天学会了\\\"+a; \",\"如果一个方法的参数需要的是一个接口的实现:\",\"public static void main(String[] args) { test(a -> \\\"今天学会了\\\"+a); //参数直接写成lambda表达式 } private static void test(Study study){ study.study(10); } \",\"对于已经实现的方法，如果我们想直接作为接口抽象方法的实现，我们还可以使用方法引用。\"]},\"68\":{\"h\":\"方法引用\",\"t\":[\"方法引用 类名::方法名 就是将一个已实现的方法，直接作为接口中抽象方法的实现（当然前提是方法定义 参数一样，返回值一样 一样才行）\",\"public interface Study { int sum(int a, int b); //待实现的求和方法 } \",\"那么使用时候，可以直接使用Lambda表达式：\",\"public static void main(String[] args) { Study study = (a, b) -> a + b; } \",\"只不过还能更简单，因为Integer类中默认提供了求两个int值之和的静态方法：\",\"public static void main(String[] args) { Study study = (a, b) -> Integer.sum(a, b); //直接使用Integer为我们通过好的求和方法 System.out.println(study.sum(10, 20)); } ================= 方法引用 类名::方法名 ========================= public static void main(String[] args) { Study study = Integer::sum; //使用双冒号来进行方法引用，静态方法使用 类名::方法名 的形式 System.out.println(study.sum(10, 20)); } \",\"方法引用其实本质上就相当于将其他方法的实现，直接作为接口中抽象方法的实现。\",\"任何方法都可以通过方法引用作为实现：\",\"public interface Study { String study(); } \",\"如果是普通成员方法 (成员方法只能通过 对象 调用，不是静态方法)，我们同样需要使用对象来进行方法引用：\",\"public static void main(String[] args) { Main main = new Main(); Study study = main::lbwnb; //成员方法因为需要具体对象使用，所以说只能使用 对象::方法名 的形式 } public String lbwnb(){ return \\\"卡布奇诺今犹在，不见当年倒茶人。\\\"; } \",\"因为现在只需要一个String类型的返回值，由于String的构造方法在创建对象时也会得到一个String类型的结果，所以说：\",\"public static void main(String[] args) { Study study = String::new; //没错，构造方法也可以被引用，使用new表示 } \",\"反正只要是符合接口中方法的定义的，都可以直接进行方法引用。\"]},\"69\":{\"c\":[\"code\"]},\"70\":{\"c\":[\"java\"]},\"71\":{\"h\":\"Java - 异常 | 工具类\"},\"72\":{\"h\":\"面向对象高级篇 4\"},\"73\":{\"h\":\"异常\"},\"74\":{\"c\":[\"code\"]},\"75\":{\"c\":[\"java\"]},\"76\":{\"h\":\"Reinforcement Learning in Multiple-UAV Networks:Deployment and Movement Design\",\"t\":[\"2019 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY\"]},\"77\":{\"h\":\"主要动机\",\"t\":[\"A novel framework is proposed for quality of experience driven deployment and dynamic movement of multiple unmanned aerial vehicles (UAVs).\",\"过去研究大多没有基于用户的移动(movement of users)来考虑无人机的机动性，更多地是考虑多架无人机的二维部署或单架无人机在地面用户保持静止情况下的部署。\",\"考虑QoE, 而不是仅考虑吞吐量(throughput)，即需要考虑地面不同用户的具体需求。(QoE is invoked for demonstrating the users’ satisfaction, and it is supposed to be considered in UAV-assisted wireless networks)\",\"该文设计的是3D部署，过去研究主要考虑的是2D部署。\"]},\"78\":{\"h\":\"主要贡献\",\"t\":[\"提出了一个理想的由QoE驱动的多无人机协助通信框架。该框架将无人机部署在三维空间内，以 mean opinion score(MOS) 为指标。通过优化无人机的部署和动态移动来解决总用户MOS最大化问题。\",\"提出解决总用户MOS最大化问题的三步骤: \",\"通过GAK-mean算法获得初始单元划分。\",\"设计一种基于 q-learning 的部署方法，在初始时间假设用户处于静止下不断调整 UAVs 3D位置进行优化处理。\",\"设计一种基于 q-learning 的无人机3D动态运动设计算法。\",\"该文基于q-learning的方案来解决无人机的NP-hard 3D部署和移动问题，并与传统的基于遗传的学习算法进行对比。\",\"该文提出的算法具较快的收敛性，与K-means和IGK算法比具有较低的复杂度。\"]},\"79\":{\"h\":\"主要内容\"},\"80\":{\"h\":\"系统结构\"},\"81\":{\"h\":\"基本设置\",\"t\":[\"考虑无人机辅助无线网络的下行链路传输(down-link transmission)，即无人机作为空中基站。\",\"对于指定区域，会将其划分为N个簇，其中用户表示为K=K1​,…,KN​，其中KN​表示划分到集群N的用户，N∈1,2,…,N。\",\"每个用户只能属于一个集群，Kn​∩Kn′​=ϕ,n′=n,\",\"在任意时刻t，同一无人机通过FDMA同时为同一集群中的多个用户提供服务\",\"对于用户kn​∈Kn​，其坐标表示为wkn​​=[xkn​​(t),ykn​​(t)]T∈R2×1\",\"对于无人机n(飞行速度恒定)，其垂直高度表示为hn​(t)∈[hmin​,hmax​],0≤t≤Ts​，其水平坐标表示为qn​(t)=[xn​(t),yn​(t)]T∈R2×1,0≤t≤Ts​\",\"无人机n与用户kn​在时间t的距离表示为:\",\"dkn​​=hn2​(t)+[xn​(t)−xkn​​(t)]2+[yn​(t)−ykn​​(t)]2​\"]},\"82\":{\"h\":\"信号模型\",\"t\":[\"无人机往往有更高的LoS链接概率，该文中表示为:\",\"PLoS​(θkn​​)=b1​(π180​θkn​​−ζ)b2​PNLoS​=1−PLoS​\",\"其中θkn​​(t)=sin−1[dkn​(t)​hn​(t)​]，表示无人机与用户之间的仰角。b1​,b2​,ζ是由环境决定的常数。在实际应用中，为了在LoS信道概率和路径损耗之间取得平衡，需要合理选择无人机n的垂直高度hn​(t)。\",\"在时间t，从无人机n到用户kn​的信道功率增益(the channel power gain)为:\",\"gkn​​(t)=K0​−1dkn​​−α[t](PLos​μLoS​+PNLos​μNLoS​)−1\",\"其中K0​=(c4πfc​​)2，α是表示路径损耗指数(常数)，μLoS​,μNLoS​是表示LoS和NLoS链路的衰减因子，fc​是载波频率，c是光速。\",\"对于无人机n，其可用带宽为Bn​，将其平均分配给其∣Kn​∣个关联用户，其每个用户带宽表示为: Bkn​​=Bn​/Kn​. 该文中不同集群所利用的频谱是不同的，且无人机向关联用户的发射功率是恒定的。 同样，对于无人机的总发射功率也均匀地分配给每个用户，pkn​​=Pmax​/Kn​\",\"由于不同集群的频谱不同，可以减轻无人机对用户接收到的干扰。因此，在时刻t关联到无人机n的地面用户kn​的接受到的信噪比表示为:\",\"Γkn​​(t)=σ2pkn​​gkn​​(t)​\",\"其中σ2=Bkn​​N0​, N0​为用户所在位置的加性高斯白噪声(AWGN)的功率谱密度。\",\"为了满足不同用户传输速率要求，对于用户kn​存在特定的信噪比目标γkn​​, 即Γ≥γkn​​.\",\"由此，存在Lemma1： 为了保证所有用户都能连接到网络，我们对无人机的发射功率有一个约束，可以表示为\",\"Pmax​≥γσ2K0​dkn​​α(t)μNLoS​\",\"根据香农定理: 信道容量C=B∗log(1+NS​)，且传输率永远都不可能超过信道容量C。 因此对于用户kn​的在时刻t的传输速率rkn​​(t)，表示为rkn​​(t)=Bkn​​log2​[1+σ2pkn​​gkn​​(t)​].\",\"Proposition1: 无人机n的高度需满足:\",\"dkn​​(t)sin[180π​(ζ+eM(t))]≤hn​(t)≤(γK0​σ2μLoS​Pmax​​)\",\"其中\",\"M(t)=b2​ln(b1​(μLoS​−μNLoS​)S(t)​−μLoS​−μNLoS​μNLoS​​​S(t)=γK0​σ2dkn​​α(t)Pm​ax​\",\"Proposition1展示了无人机为相关用户提供可靠服务所需的高度的必要条件。 可知，其高度的下界是距离dkn​​(t)的函数；高度的上界是最大发射功率Pmax​的函数。 因此，随着无人机与用户之间距离和发射功率的变化，需要调整相应无人机的高度，以向用户提供可靠的服务。\"]},\"83\":{\"h\":\"Quality-of-Experience Model\",\"t\":[\"由于不同用户对于传输速率的需求是不同的，所以在无人机辅助通信网络中我们需要考虑QoE模型。\",\"在该文中，采用MOS作为用户QoS衡量的标准，具体如下:\",\"MOSkn​​(t)=ζ1​MOSkn​​delay(t)+ζ2​MOSkn​​rate(t)\",\"其中，ζ1​,ζ2​是系数，且ζ1​+ζ2​=1。\",\"根据MOS数值，共划分5个等级: excellent(4.5) very good(2~3.5) fair(1~2) poor(1)。\",\"在该文中考虑的是网页浏览应用传输情况，因此MOSkn​​delay(t)可以忽略，因此，此时的MOS模型定义如下:\",\"MOSkn​​(t)=−C1​ln[d(rkn​​(t))]+C2​\",\"d(rkn​​(t))是与传输速率有关的延迟时间，MOSkn​​(t)为t时刻的MOS评分，取值范围从1−4.5。C1​和C2​是通过分析web浏览应用程序的实验结果确定的常数，分别设为1.120和4.6746。\",\"d(rkn​​(t))=3RTT+rkn​​(t)FS​+L(rkn​​MSS​)+RTT−rkn​​(t)2MSS(2L−1)​\",\"其中，RTT[s]表示round trip time(数据包从发送端-接收端-发送端的时间)，FS[bit]是网页大小，MSS[bit]是最大报文长度，L=min[L1​,L2​]表示 the number of slow start cycles with idle periods。\",\"L1​=log2​(MSSrkn​​RTT​+1)−1,L2​=log2​(2MSSFS​+1)−1.\",\"用户rkn​​在一段时间Ts​内的MOS总和为:\",\"MOSrkn​​​=t=0∑Ts​​MOSkn​​(t)\"]},\"84\":{\"h\":\"优化问题建立\",\"t\":[\"假设功率Q=qn​(t),0≤t≤Ts​, 高度H=hn​(t),0≤t≤Ts​\",\"本文目的是优化无人机在每个时隙的位置，从而最大化所有用户的总MOS值。具体表述如下:\",\"C,Q,Hmax​MOStotal​=∑n=1N​∑kn​=1Kn​​∑t=0Ts​​MOSkn​​(t)s.t.Kn​∩Kn′​=ϕ,n′=n,∀n,hmin​≤hn​(t)≤hmax​,∀t,∀n,Γkn​(t)​≥γkn​​,∀t,∀kn​,∑kn​=1Kn​​pkn​​(t)≤Pmax​,∀t,∀kn​,pkn​(t)​≥0,∀kn​,∀t,​\",\"该优化问题是一个non-convex问题，因为目标函数对于无人机的3D坐标是非凸的。\",\"总用户的MOS取决于无人机的发射功率、数量和位置(水平位置和高度)。\"]},\"85\":{\"h\":\"解决方案\"},\"86\":{\"h\":\"无人机的3D部署\",\"t\":[\"考虑以下场景，将上述优化问题简化:\",\"无人机n以可变高度悬停在用户上方，用户是保持静态的。 每架无人机的带宽和发射功率都均匀分配给每个用户。 因此我们将优化问题简化为区域分割问题。\",\"描述如下: 但即使仅考虑用户聚类，该问题依然是NP-hard问题\",\"C,Q,Hmax​MOStotal​=∑n=1N​∑kn​=1Kn​​MOSkn​​(t)s.t.Kn​∩Kn′​=ϕ,n′=n,∀n,hmin​≤hn​(t)≤hmax​,∀t,∀n,Γkn​(t)​≥γkn​​,∀t,∀kn​,∑kn​=1Kn​​pkn​​(t)≤Pmax​,∀t,∀kn​,pkn​(t)​≥0,∀kn​,∀t,​\",\"无人机-用户关联策略(用户区域划分算法)\",\"采用基于遗传算法的GAK-means算法 由于特定用户的MOS与该用户与无人机之间的距离有关，因此GAK-means可以视为获得无人机部署的低复杂度方案。\",\"根据N个用户，根据遗传算法找到CN​个最优个体作为簇的中心。\",\"将无人机部署在每个中心内，再将用户划分给距离最近的无人机\",\"重复步骤，再找到新的簇的各中心，再根据欧几里得距离重新划分，直到各个簇的成员没有太大变化，划分完毕。\",\"无人机3D部署算法\",\"根据所给定的用户划分情况，目标是获得无人机的最佳3D位置，来最大化MOS总和。 由于GAK-means的优化目标是最小化无人机与对应集群用户的欧氏距离，MOS主要是有关传输速率rkn​​的函数，因此MOS不仅与欧氏距离有关，还与LoS的概率有关。\",\"采用Q-learning算法\",\"智能体(agent): UAVn,n∈N={1,2,…,N}\",\"状态(state): 对于每个智能体，其状态为其3D坐标，定义为ξ=(xUAV​,yUAV​,hUAV​)\",\"状态空间(state space S): 这里采用离散化空间坐标，即xUAV​:{0,1,…,Xd​},yUAV​:0,1,…,Yd​,hUAV​:{hmin​,…,hmax​}，所以状态其实共有(XD​+1)×(Yd​+1)×(hmax​−hmin​+1)个\",\"动作空间(action space): 每次无人机会根据当前状态st​∈S，按照所给定策略J来执行一个动作at​∈A从而获得奖励rt​以及下一个状态st+1​ 该论文中在精度和模型复杂型上作出平衡，共考虑7个方向。 (1,0,0)：右转 (−1,0,0)：左转 (0,1,0)：前进 (0,−1,0)：后退 (0,0,1)：上行 (0,0,−1)：下行 (0,0,0)：静止\",\"状态转换模型: 当执行动作at​时，从状态st​到st+1​，并获得奖励rt​的这一过程可以用条件转移概率p(st+1​,rt​∣st​,at​)来表示。 Q-learning的优化目标是最大化长期收益\",\"Gt​=E[n=0∑∞​βnrt+n​]\",\"奖励(reward): 如果agent在当前时刻t所执行的动作能够提高总MOS，则无人机将获得正奖励。否则，agent将获得负奖励。\",\"xt​=⎩⎨⎧​1,−0.1,−1,​ifMOSnew​>MOSold​ifMOSnew​=MOSold​ifMOSnew​<MOSold​​\",\"具体代码：（策略为贪心策略）\",\"算法1\",\"个人理解：\",\"通过K-means来划分各个无人机所管理的用户簇。无人机的位置初始化也是随机部署的\",\"但每个无人机所管理的用户不同，其目标也应该不一样，不能用同一个Q-table管理，这里是每个无人机都有一张自己的Q-table，来进行迭代？ 还是同一张Q-table，只不过根据区域划分，不同的无人机agent的Q(s,a)的s是有范围的？(个人感觉是这个)\",\"最终输出的结果，应该是无人机最终停的位置即是部署的最佳位置(因为q-learning是优化长期目标)，发现在该位置静止是最优的，表示是最佳部署位置。\",\"最终输出结果，是根据Q-table来找出对应q(s,a)当a为静止时，最大的q(s,a)值，对应s就是UAV的部署位置\"]},\"87\":{\"h\":\"无人机的动态移动设计\",\"t\":[\"考虑用户在每个时隙移动的情况，由于用户在每个时隙都处于漫游状态，因此随着用户位置的变化，每个集群中无人机的最优位置也会发生变化，无人机需要进行移动。\",\"在本文中不考虑用户移动到其他集群的情况 因为在不考虑用户自由穿梭集群的情况，对于动作空间而言，仅需要考虑无人机的7个移动方向即可；但若考虑集群情况，动作空间包含两个部分：选择移动方向和选择关联用户。设无人机总数为N，∣Kn​∣为第n个簇的用户总数，则用户的关联动作数为2N∑n=1N​∣Kn​∣，∑n=1N​∣Kn​∣是总用户数，每个用户都需要判断是否与每个无人机关联，因此是2N 则总动作空间的大小为7+2N∑n=1N​∣Kn​∣会导致动作空间过大，Q-table过大。\",\"1.用户漫游模型 在设计无人机的移动之前，需考虑用户的移动性，这里有多种mobility modles可选择，如a deterministic approach, a hybrid approach, and a random walk model. 在本文中，采用的是the random walk model(Markovian mobility model) 每个用户的移动方向均匀分布在左、右、前、后四个方向。 用户的速度设为[0,cmax​]，其中cmax​表示用户的最大速度。\",\"2.基于q-learning的移动算法 与基于q-learning的部署算法不同的是，在此情况下，状态除了要考虑无人机的3D位置外，还需要考虑所有用户的2D位置。即ξ={xUAV​,yUAV​,hUAV​,xuser​,yuser​}(xuser​,yuser​)由用户的初始位置和运动模型决定，(xUAV​,yUAV​,hUAV​)由无人机的位置和它们在最后时隙采取的动作决定.\",\"训练阶段: \",\"测试阶段:\\n\"]},\"88\":{\"c\":[\"academic\"]},\"89\":{\"c\":[\"UAV\",\"IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY\"]},\"90\":{\"h\":\"RL1 - 基本概念\"},\"91\":{\"h\":\"强化学习框架图\",\"t\":[\"主要框架\"]},\"92\":{\"h\":\"1. 基本概念\",\"t\":[\"State(状态)：The status of the agent with respect to the environment.\",\"State Space(状态空间): 所有状态的集合。S={si​}i=1n​。\",\"Action(动作): 对于每一个状态，都有可选择的动作。\",\"Action space of a state: 对应状态中所有可选择的动作集合。A(si​)={ai​}i=1n​\",\"State transition(状态转换): s1​→a1​s2​。定义了agent与环境的交互行为。\",\"State transition probability: p(s2​∣s1​,a1​)，即状态s1​采用动作a1​转到状态s2​的概率。\",\"Policy π: 指导agent在当前状态下选择哪个动作。\",\"Reward(奖励): 在执行一个动作后获得的一个常数(依赖于当前状态和所采取的动作)。同样可以用条件概率的形式进行描述，如p(r=1∣s1​,a1​)，即在状态s1​下采用动作a1​获得的奖励r=1的概率。\",\"Trajectory：a state-action-reward chain.(可以有限，也可以是无限长的trajectory) s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​. 个人理解，trajectory是在策略给定下，agent可能走出的全部轨迹，并非只是一个单一的轨迹。\",\"Return of a trajectory：将对应的轨迹所获得的所有reward的总和，可以粗步衡量一个策略的好坏。\",\"Discounted return(of a trajectory)：为了应对具有无限步的trajectory的return=∞的情况。 s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​r=1→​a2​​s9​r=1→​a2​​s9​…. 此时该trajectory的return=0+0+0+1+1+⋯=∞。 引入discount rate, γ∈[0,1). 此时对应的discountedrate=0+γ0+γ20+γ31+γ41+⋯=γ31−γ1​ 显然，如果γ接近0，即此时的discounted return越短视，注重近期的reward；γ接近1，更远视，更注重长远的reward。\",\"Episode(trial)：When interacting with the environment following a policy, the agent may stop at some terminal states. The resulting trajectory is called an episode(or a trial)/ 即表示具有终止状态terminal states的trajectory，通常是具有有限步长的trajectory. 同理，这样的任务称为episodic tasks。\",\"continuing tasks：即不具备terminal states的任务，会与环境一直交互下去。 可以通过设置将episodic tasks转换成continuing tasks，如可以在target states中限制action space，控制其一直待在target states中。 Deterministic — Stochastic\"]},\"93\":{\"h\":\"2.Markov decision process(MDP)\",\"t\":[\"关键元素：\",\"Sets： \",\"State：the set of states S\",\"Action：the set of actions A(s) is associate for state s∈S\",\"Reward：the set of rewards R(s,a).\",\"Probability distribution： \",\"State transition probability p(s′∣s,a): 表示在状态s下采取动作a，转换到状态s′的概率。\",\"Reward probability p(r∣s,a): 表示在状态s下采取动作a，获得reward r 的概率。\",\"Policy：at state s, the probability to choose action a is π(a∣s). 表示在各状态执行各动作的概率。\",\"Markov property：即无记忆的特性。 p(st+1​∣at+1​,st​,…,a1​,s0​)=p(st+1​∣at+1​,st​)r(st+1​∣at+1​,st​,…,a1​,s0​)=p(rt+1​∣at+1​,st​)\",\"Markov process：在policy是确定的情况下，MDP就变为MP。\"]},\"94\":{\"c\":[\"academic\"]},\"95\":{\"c\":[\"强化学习\"]},\"96\":{\"h\":\"RL10 - Actor-Critic 方法\",\"t\":[\"actor: 对应 policy update\",\"critic: 对应 policy evaluation 或者 value evaluation\",\"20240830184236\",\"显然，是在基于 策略梯度上升 算法的基础上，将对于 Q 值的估计通过一个网络来进行描述，这个便成为 critic, 而对应的策略梯度上升算法就是对应 actor。\",\"20240830184312\"]},\"97\":{\"h\":\"1. The simplest actor-critic (QAC)\",\"t\":[\"20240830184330\",\"20240830184424\"]},\"98\":{\"h\":\"2. Advantage actor-critic (A2C)\",\"t\":[\"核心思想：在 QAC 的基础上来引入偏置量(baseline)，从而减小方差，提升采样的效率。\"]},\"99\":{\"h\":\"2.1 baseline\",\"t\":[\"在策略梯度算法中引入一个 baseline, 不会影响所求的梯度。 即:\",\"▽θ​J(θ)​=ES∼η,A∼π​[▽θ​ln(A∣S,θ)qπ​(S,A)]=ES∼η,A∼π​[▽θ​ln(A∣S,θ)qπ​(S,A)−b(S)]​\",\"证明: 要证明加入baseline成立，只需要保证:\",\"ES∼η,A∼π​[▽θ​ln(A∣S,θ)b(S)]=0\",\"20240830185127\",\"作用:\",\"因此，我们需要找到一个 baseline 来保证这个梯度的方差最小即可。\"]},\"100\":{\"h\":\"2.2 最好的 baseline\",\"t\":[\"20240830185324\",\"在实际情况中，我们通常将 baseline 设置为 vπ​(s)\"]},\"101\":{\"h\":\"2.3 对应算法\",\"t\":[\"20240830185537\",\"20240830185556\",\"20240830185629\"]},\"102\":{\"h\":\"3. off-policy actor-critic\",\"t\":[\"通过 重要性采样 的方法，将处于 另一分布下 的策略所采集的数据来 运用到 策略更新 中。\"]},\"103\":{\"h\":\"3.1 重要性采样 (Importance sampling)\",\"t\":[\"20240830200056\",\"20240830200118\",\"20240830200138\"]},\"104\":{\"h\":\"3.2 off-policy\",\"t\":[\"20240830200248\",\"20240830200305\",\"20240830200320\",\"20240830200343\"]},\"105\":{\"h\":\"3.3 伪代码\",\"t\":[\"20240830200406\"]},\"106\":{\"h\":\"4. Deterministic actor-critic (DPG)\",\"t\":[\"1234\",\"20240830200608\",\"20240830200624\"]},\"107\":{\"c\":[\"academic\"]},\"108\":{\"c\":[\"强化学习\"]},\"109\":{\"h\":\"RL2 - 贝尔曼公式\"},\"110\":{\"h\":\"核心内容\",\"t\":[\"state value\",\"the Bellman equation\"]},\"111\":{\"h\":\"1.State value\"},\"112\":{\"h\":\"1.1\",\"t\":[\"引入随机变量后对应的discounted return的描述。 即一个trajectory下的discounted return。 由此可以推导出一个多步的trajectory:\",\"St​→At​Rt+1​,St+1​→At+1​Rt+2​,St+2​→At+2​Rt+3​,St+2​→At+3​…\",\"对应的discounted return为：Gt​=Rt+1​+γRt+2​+γ2Rt+3​+…\",\"γ 为discounted rate\",\"Gt​也是一个随机变量\"]},\"113\":{\"h\":\"1.2 State value\",\"t\":[\"State value 是 Gt​ 的期望, 也称为 state value function 表示为 The expection(expected value or mean) of Gt​:\",\"vπ​(s)=E[Gt​∣St​=s]\",\"是一个有关状态s的函数.\",\"vπ​(s) 是基于一个给定策略 π , 对于不同的策略，所得到的 state value 是不同的.\",\"state value 可以用来衡量一个状态的价值.\"]},\"114\":{\"h\":\"1.3 State value 与 return 的区别\",\"t\":[\"Return 是针对一条trajectory所求的，而 State value 则是对多个 trajectory 求 return 再求平均值。 The state value is the mean of all possible returns that can be obtained starting from a state. 只有当所有东西都是确定性的(π(a∣s),p(r∣s,a),p(s′∣s,a))，state value 与 return 是一致的.\"]},\"115\":{\"h\":\"2. Bellman equation\",\"t\":[\"用来描述所有状态的state value的关系. 根据一个 random trajectory:\",\"St​→At​Rt+1​,St+1​→At+1​Rt+2​,St+2​→At+2​Rt+3​,St+2​→At+3​…\",\"对应的 discounted return Gt​ 为:\",\"Gt​​=Rt+1​+γRt+2​+γ2Rt+3​+…=Rt+1​+γ(Rt+2​+γRt+3​+…)=Rt+1​+γGt+1​​\",\"因此，对应的 state value 为:\",\"vπ​(s)​=E[Gt​∣St​=s]=E[Rt+1​+γGt+1​∣St​=s]=E[Rt+1​∣St​=s]+γE[Gt+1​∣St​=s]​\",\"需要推导E[Rt+1​∣St​=s]和E[Gt+1​∣St​=s]的计算即可。\"]},\"116\":{\"h\":\"2.1 The mean of immediate rewards:\",\"t\":[\"E[Rt+1​∣St​=s]​=a∑​π(a∣s)E[Rt+1​∣St​=s,At​=a]=a∑​π(a∣s)r∑​p(r∣s,a)r​\"]},\"117\":{\"h\":\"2.2 The mean of future rewards:\",\"t\":[\"E[Gt+1​∣St​=s]​=s′∑​E[Gt+1​∣St​=s,St+1​=s′]=s′∑​E[Gt+1​∣St+1​=s′](无记忆性)=s′∑​vπ​(s′)p(s′∣s)=s′∑​vπ​(s′)a∑​p(s′∣s,a)π(a∣s)​\",\"个人推导：\",\"E[Gt+1​∣St​=s]​=a∑​π(a∣s)E[Gt+1​∣St​=s,At​=a]=a∑​π(a∣s)s′∑​E[Gt+1​∣St​=s,At​=a,St+1​=s′]=a∑​π(a∣s)s′∑​p(s′∣s,a)E[Gt+1​∣St+1​=s′]=a∑​π(a∣s)s′∑​p(s′∣s,a)vπ​(s′)​\"]},\"118\":{\"h\":\"2.3 Bellman equation\",\"t\":[\"vπ​(s)​=E[Rt+1​∣St​=s]+γE[Gt+1​∣St​=s],=mean of immediate rewards a∑​π(a∣s)r∑​p(r∣s,a)r​​+mean of future rewards γa∑​π(a∣s)s′∑​p(s′∣s,a)vπ​(s′)​​,=a∑​π(a∣s)[r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)],∀s∈S.​\",\"该式子针对状态空间中的所有状态均成立.\",\"通过 Bootstrapping , 可以求解 state value.\",\"π(a∣s) 表示一个给定的策略. 求解Bellman equation 称为策略评估(Policy evaluation).\",\"p(r∣s,a),p(s′∣s,a) 是由环境决定的(dynamic model|environment model). 后续可能是未知的(model-free)，需要通过采样解决.\"]},\"119\":{\"h\":\"2.4 Bellman equation (Matrix-vector form)\",\"t\":[\" 此时,对于所有状态s，对应的 Bellman equation 为\",\"vπ​(s)=rπ​(s)+γs′∑​pπ​(s′∣s)vπ​(s′)​\",\"将所有状态的 Bellman equation 整合，重新修改为 matrix-vector form.\",\"vπ​=rπ​+γPπ​vπ​​\",\"其中,\",\"vπ​=[vπ​(s1​),…,vπ​(sn​)]T∈Rn\",\"rπ​=[rπ​(s1​),…,rπ​(sn​)]T∈Rn\",\"Pπ​∈Rn×n, where [Pπ​]ij​=pπ​(sj​∣si​), 表示状态转移矩阵.\"]},\"120\":{\"h\":\"3. Why to slove state value\",\"t\":[\"为了进行 Policy evaluation, 即对于给定策略，求出其对应状态的 state value 的过程。\",\"通过 Bellman euqation 进行求解。\",\"The closed-form solution(不常用):\",\"vπ​=(I−γpπ​)−1rπ​​\",\"An iterative solution(一种迭代策略):\",\"vk+1​=rπ​+γPπ​vk​​\",\"可以最开始均初始化为 0 , 然后进行不断迭代，可以得到一个序列v0​,v1​,v2​,…. 最终可以证明：vk​→vπ​=(I−γpπ​)−1rπ​,k→∞\"]},\"121\":{\"h\":\"4. Action value\",\"t\":[\"State value: agent从一个状态出发可以得到的平均return. the average return the agent can get starting from a state\",\"Action value: agent从一个状态出发，采取一个指定的action可以得到的平均return。 the average return the agent can get starting from a state and taking an action.\",\"通过求解 action value 我们可以分析出在该状态下采取哪个 action 收益最大. Action value 定义:\",\"qπ​(s,a)=E[Gt​∣St​=s,At​=a]​\",\"同样地，qπ​(s,a)是依赖于策略π的，并且与状态 s 和动作 a 有关.\",\"vπ​(s)E[Gt​∣St​=s]​​=a∑​qπ​(s,a)E[Gt​∣St​=s,At​=a]​​π(a∣s)\",\"因此，vπ​(s)=∑a​qπ​(s,a)π(a∣s) 由于,\",\"vπ​(s)=a∑​π(a∣s)[r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)]​\",\"所以，qπ​(s,a)=∑r​p(r∣s,a)r+γ∑s′​p(s′∣s,a)vπ​(s′)\",\"实际意义是：在当前状态s下采取动作 a 所获得的均值，加上 γ 的转到下一个状态的 state value 加权均值。\",\"引入 action value 后，对于 state value 实际意义的解释：在当前状态s下，根据策略π, 所有可能动作的 action value 的加权均值。\",\"state value 和 action value 可以互相转化。\"]},\"122\":{\"h\":\"5. 总结\",\"t\":[\"State value: vπ​(s)=E[Gt​∣St​=s]\",\"Action value: qπ​(s,a)=E[Gt​∣St​=s,At​=a]\",\"State value 是 action value 的根据策略π加权平均，即vπ​(s)=∑a​π(a∣s)q(s,a)\",\"The Bellman equation (elementwise form and matrix-vector form)\",\"求解 the Bellman equation (2种方法)\"]},\"123\":{\"c\":[\"academic\"]},\"124\":{\"c\":[\"强化学习\"]},\"125\":{\"h\":\"RL3 - 贝尔曼最优公式\",\"t\":[\"Core concepts: optimal state value and optimal policy\",\"A fundamental tool: the Bellman optimality equation (BOE)\"]},\"126\":{\"h\":\"1. Optimal policy\",\"t\":[\"最优策略的定义: A policy π∗ is optimal if π∗(s)≥vπ​(s) for all s and for any other policy π. 需要确定几件事:\",\"最优策略是否存在 存在，根据 the contraction mapping Theorem.\",\"最优策略是否唯一 唯一，根据 the contraction mapping Theorem.\",\"最优策略是 stochastic 还是 deterministic deterministic 且 greedy\",\"如何得到最优策略 选取状态中最大的 action value 作为下一步的 action\"]},\"127\":{\"h\":\"2. Bellman optimality equation (BOE)\"},\"128\":{\"h\":\"2.1 基本形式\",\"t\":[\"对于贝尔曼最优公式而言，其策略π表示的是最优策略，除了需要求解 state value 外，还需要求解最优策略π.elementwise form:\",\"vπ​(s)​=πmax​a∑​π(a∣s)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)),∀s∈S=πmax​a∑​π(a∣s)q(a,s),∀s∈S​\",\"matrix-vector foem:\",\"v=πmax​(rπ​+γPπ​v)​\"]},\"129\":{\"h\":\"2.2 如何求解\",\"t\":[\"对于贝尔曼最优公式而言，区别于贝尔曼公式，只是求解各状态的 state value, 我们还需要理解其所描述的最优策略π∗ 具体分两步:\"]},\"130\":{\"h\":\"2.2.1 如何处理等式右边的 (最优策略)\",\"t\":[\"vπ​(s)=maxπ​∑a​π(a∣s)q(s,a), 为了让右边取到最大值的情况，我们只需要在当前状态下，保证选取最大的 action value 即可，对应策略表示为:\",\"π(a∣s)={10​a=a∗a=a∗​\",\"其中a∗表示在该状态下计算出来的最大 action value 对应的动作，即a∗=argmaxa​q(s∣a)\"]},\"131\":{\"h\":\"2. 求解 state value\",\"t\":[\"将 BOE 转换为 v=f(v) 的形式，其中f(v):=maxπ​(rπ​+γPπ​v)f(v)对应一个向量, [f(v)]s​=maxπ​∑a​π(a∣s)q(s∣a),∀s∈S\",\"求解方法：\",\"Fix point: f(x)=x\",\"Contraction mapping(contractive function): ∣∣f(x1​)−f(x2​)∣∣≤γ∣∣x1​−x2​∣∣\",\"由此可以根据Contraction Mapping Theorem: For any equation that has the form of x=f(x), if f is a contraction mapping, then\",\"Existence: 存在不动点x∗，满足f(x∗)=x∗\",\"Uniqueness: 不动点x∗是唯一的\",\"Algorithm: Consider a sequence xk​ where xk+1​=f(xk​), then xk​→x∗ as k→∞. Moreover, the convergence rate is exponentially fast.\",\"因此，可以通过Contraction Mapping Theorem来求解贝尔曼最优公式，因为其满足该理论，即f(v)是一个contraction mapping。\"]},\"132\":{\"c\":[\"academic\"]},\"133\":{\"c\":[\"强化学习\"]},\"134\":{\"h\":\"RL4 - 值迭代和策略迭代(动态规划)\",\"t\":[\"贝尔曼最优公式:\",\"v=f(v)=πmax​(rπ​+γPπ​v)\"]},\"135\":{\"h\":\"1. Value iteration algorithm\",\"t\":[\"根据 chapter 3 中涉及的 contraction mapping theorem, 我们可以通过对应的迭代算法来求解贝尔曼最优公式\",\"vk+1​=f(vk​)=πmax​(rπ​+γPπ​vk​),k=1,2,3…\",\"这种迭代算法称为 value iteration.\"]},\"136\":{\"h\":\"1.1 具体步骤\",\"t\":[\"共分为 2 步：\",\"Policy update 这步是更新策略π，即求解右边的式子，πk+1​=argmaxπ​(rπ​+γPπ​vk​), 其中vk​是给定的。 其对应的 elementwise form:\",\"πk+1​(s)=πargmax​a∑​π(a∣s)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)v(s′)),s∈S 由于 p(s′∣s,a),p(r∣s,a),v(s′) 是已知的，显然，这里的最优策略πk+1​是一个 greedy policy，我们只需要挑选在当前迭代下最大的 action value 就好了, 即:\",\"πk+1​(a∣s)={10​a=ak∗​(s)a=ak∗​(s)​ 其中ak∗​(s)=argmaxa​qk​(a,s).\",\"value update 根据 Policy update 的策略πk+1​, 求解下一步的vk+1​, 即\",\"vk+1​=rπk+1​​+γPπk+1​​vk​这里的vk​并不是 state value 由于πk+1​是 greedy 的，对应的vk+1​(s)=maxa​qk​(a,s)\"]},\"137\":{\"h\":\"1.2 伪代码\",\"t\":[\"20240810190018\"]},\"138\":{\"h\":\"2. Policy iteration algorithm\",\"t\":[\"算法迭代示意图:\",\"π0​PE​vπ0​​PI​π1​PE​vπ1​​PI​π2​PE​vπ2​​PI​…\"]},\"139\":{\"h\":\"2.1 算法描述\",\"t\":[\"首先随机设计一个初始的策略π0​\",\"Step 1: policy evaluation (PE) 策略评估 该步骤是用来计算当前策略 πk​ 的 state value. 可以通过 Bellman equation 进行求解，即:\",\"vπk​​=rπk​​+γPπk​​vπk​​\",\"根据对应的 Elementwise form:\",\"vπk​(j+1)​(s)=a∑​πk​(a∣s)(r∑​p(r∣s,a)r+s′∑​p(s′∣s,a)vπk​(j)​(s′)),s∈S\",\"由此进行迭代，直到设置的收敛条件为止，即j→∞ 或者 ∣∣vπk+1​(j+1)​(s)−vπk​(j)​(s)∣∣≤δ.\",\"Step 2: policy improvement (PI) 策略提升 该步骤是根据 PE 所求出的 state value, 根据 action value，来提升当前策略 πk​\",\"πk+1​=πargmax​(rπ​+γPπ​vπk​​)\",\"对应的 Elementwise form:\",\"πk+1​(s)=πargmax​a∑​πk​(a∣s)qπk​​(s,a)(r∑​p(r∣s,a)r+s′∑​p(s′∣s,a)vπk​​(s′))​​,s∈S\",\"这里，显然是可以通过一个 greedy 的策略来进行选择，即:\",\"πk+1​(a∣s)={10​a=ak∗​(s),a=ak∗​(s).​\",\"其中 aK∗​(s)=argmaxa​qπk​​(s,a).\"]},\"140\":{\"h\":\"2.2 伪代码\",\"t\":[\"20240811002219\"]},\"141\":{\"h\":\"2.3 一些问题\",\"t\":[\"在 PE 步骤中，如何通过 Bellman equation 得到 state value vπk​​. 根据 chapter 2 中求解 Bellman equation 的方法 一种是可以直接通过矩阵求逆进行求解，即 vπk​​=(I−γPπk​​)−1rπk​​，实际不常用. 一种是通过迭代算法来求解\",\"vπk​(j+1)​=rπk​​+γPπk​​vπk​(j)​\",\"在 PI 步骤中，如何确保策略 πk+1​ 是优于 πk​的.\",\"为什么这个迭代算法最终可以找到最优策略 每次迭代都会使得策略进行提升，那么\",\"vπ0​​≤vπ1​​≤vπ2​​⋯≤vπk​​≤⋯≤v∗\",\"我们需要保证策略是不断提升，且最终会收敛到最优策略v∗\",\"policy iteration algorithm 与 value iteration algorithm 之间存在什么关系.\"]},\"142\":{\"h\":\"3. Truncated policy iteration algorithm\",\"t\":[\"该算法是 value iteration 以及 policy iteration 一般化的推广\"]},\"143\":{\"h\":\"3.1 value iteration 与 policy iteration 算法比较\",\"t\":[\"Policy iteration: 需要初始化策略π0​, 之后进行迭代\",\"Policy evaluation (PE): 通过 Bellman equation 求解当前策略的 state value.\",\"vπk​​=rπk​​+γPπk​​vπk​​\",\"内嵌迭代算法求解.\",\"Policy improvement (PI): 考虑 greedy 策略求解, 选取当前状态下最大的 action value.\",\"πk+1​=πargmax​(rπ​+γPπ​vπk​​)\",\"Value iteration: 需要初始化猜测的 state value v0​\",\"Policy update (PU): 考虑 greedy 策略求解, 选取当前状态下最大的 action value.\",\"πk+1​=πargmax​(rπ​+γPπ​vk​)\",\"Value update (VU): 进行迭代\",\"vk+1​=rπk+1​​+γPπk+1​​vk​\",\"两个算法迭代过程十分类似: Policy iteration:\",\"π0​PE​vπ0​​PI​π1​PE​vπ1​​PI​π2​PE​vπ2​​PI​…\",\"Value iteration:\",\"u0​PU​π1′​VU​u1​PU​π2′​VU​u2​PU​…\",\"Policy iteration algorithm\",\"Value iteration algorithm\",\"Comments\",\"1) Policy:\",\"π0​\",\"N/A\",\"2) Value:\",\"vπ0​​=rπ0​​+γPπ0​​vπ0​​\",\"v0​:=vπ0​​\",\"对于 policy iteration，vπ0​​是通过迭代算法来求的; 而 value iteration 我们这里强行初始化为vπ0​​，方便后续比较\",\"3) Policy:\",\"π1​=argmaxπ​(rπ​+γPπ​vπ0​​)\",\"π1​=argmaxπ​(rπ​+γPπ​vπ0​​)\",\"在策略更新上，这两个算法是一致的。\",\"4) Value:\",\"vπ1​​=rπ1​​+γPπ1​​vπ1​​\",\"v1​=rπ1​​+γPπ1​​v0​\",\"对于 Policy iteration 而言, 这里需要通过迭代算法来精确求出 vπ1​​; 对于 Value iteration，则只是进行一次带入求解。\",\"5) Policy:\",\"π2​=argmaxπ​(rπ​+γPπ​vπ1​​)\",\"π2′​=argmaxπ​(rπ​+γPπ​v1​)\",\"⋮\",\"⋮\",\"⋮\",\"⋮\"]},\"144\":{\"h\":\"3.2 Truncated policy iteration algorithm\",\"t\":[\"20240811010933\",\"显然，在求解 Bellman equation 中，Value iteration 只是进行了一步求解，而 Policy iteration 进行了无穷多步来进行了真实的求解 state value，显然在现实运行算法中是无法做到的。 因此 Truncated policy iteration algorithm 就是进行迭代 n 步来求解。\"]},\"145\":{\"h\":\"truncated policy iteration algorithm 是否是收敛的\",\"t\":[\"20240811011334\"]},\"146\":{\"c\":[\"academic\"]},\"147\":{\"c\":[\"强化学习\"]},\"148\":{\"h\":\"RL5 - 蒙特卡洛方法 (Monte Carlo) model-free\",\"t\":[\"如何在没有模型 (即p(r∣s,a),p(s′∣s,a)等均未知) 的情况下进行估计 通过 Monte Carlo estimation. 其核心思想是： 若有一系列(i.i.d)样本采样，得到一个样本序列x1​,x2​,…,xN​ 那么对于随机变量X的估计可以为：\",\"E[x]≈xˉ=N1​j=1∑N​xj​\",\"该方法成立的数学依据是 大数定理 (Law of Large Numbers)样本必须是独立同分布(iid, independent and identically distributed)\",\"为什么考虑 mean estimation. 因为无论是 state value 还是 action value 其原始定义都是从期望出发的。\",\"vπ​(s)=E[Gt​∣St​=s];qπ​(s,a)=E[Gt​∣St​=s,At​=a]\"]},\"149\":{\"h\":\"1. MC Basic\",\"t\":[\"最简单的示例算法，用于解释 MC 的原理，但现实场景中不太经常使用，效率过低。\",\"核心思想：如何将 Policy iteration algorithm 转换为 model-free 的情况。\"]},\"150\":{\"h\":\"1.1 算法思路\",\"t\":[\"Policy iteration 算法的核心是 先根据当前策略计算出各个状态的 state value， 再将 state value 转换为 action value，更新策略的步骤就是选择此时 action value 最大的 action.\",\"{Policyevaluation:vπk​​=rπk​​+γPπk​​vπk​​Policyimprovement:πk+1​=argmaxπ​(rπ​+γPπ​vπk​​)​\",\"显然其核心关键就是在 PE 中 通过迭代算法求解 Bellman equation 的 state value后：\",\"对于 model-based 的情况, 因为 p(r∣s,a),p(s′∣s,a) 已知，我们可以很轻松的求出各个情况下的q(s,a)，从而选择每个状态下最大的 action value 即可。\",\"qπk​​(s,a)=r∑​p(r∣s,a)+γs′∑​p(s′∣s,a)vπk​​(s)\",\"对于 model-free 的情况，此时 p(r∣s,a),p(s′∣s,a) 未知，我们不能通过之前的方法来求出q(s,a)，需要从 action value 的定义出发，即：\",\"qπk​​(s,a)=E[Gt​∣St​=s,At​=a]\",\"从此可以发现，我们可以通过前面所引入的 mean estimation 方法，来进行求解 q(s,a).\"]},\"151\":{\"h\":\"1.2 如何估计\",\"t\":[\"从指定的 (s,a) 出发，根据策略 πk​, 我们可以生成一个 episode.\",\"这个 episode 的 return 为 g(s,a).\",\"显然，g(s,a) 就是前面 Gt​ 的一个 sample.\",\"假设我们有了一系列 从状态 s 出发, 采取动作 a 的 episodes, 即 g(j)(s,a). 那么我们可以对 qπk​​(s,a) 进行估计，即\",\"qπk​​(s,a)=E[Gt​∣St​=s,At​=a]≈N1​i=1∑N​g(i)(s,a).\"]},\"152\":{\"h\":\"1.3 具体算法\",\"t\":[\"与 Policy iteration algorithm 步骤类似 首先初始化一个随机的策略π0​，然后进行迭代，对于 kth 迭代，有：\",\"Step 1: Policy evaluation. 求在策略πk​下所有的 action value, q(s,a). 具体求解方法，如 1.2 节所述，只不过我们此时需要遍历所有的 action-state pair. 为什么不去求 state value，因为最终策略更新的核心仍然是 action value, 即使先估计了 state value, 我们仍需要估计 action value.\",\"Step 2: Policy improvement. 这是来求解 πk+1​(s)=argmaxπ​∑a​π(a∣s)qπk​​(s,a),foralls∈S 这个仍然与之前一致，采用 greedy policy，即对于每个状态，我们选取其 action value 最大的 action.πk+1​(ak∗​∣s)=1，其中ak∗​=argmaxa​qπk​​(s,a)\",\"20240811233346\"]},\"153\":{\"h\":\"2. MC Exploring Starts\",\"t\":[\"MC Exploring Starts 是针对 MC Basic 的一些改进，即对于数据(experience)更加高效利用。\"]},\"154\":{\"h\":\"2.1 Episode 的高效利用\",\"t\":[\"Visit: every time a state-action pair appears in the episode, it is called a visit of that state-action pair.\",\"考虑一个 episode, 跟随策略π,\",\"s1​a2​​s2​a4​​s1​a2​​s2​a3​​s5​a1​​…\",\"对于 MC-Basic, 这一条 episode 仅用作估计 state-action pair (s1​,a2​) 的 action value q(s1​,a2​)，但存在一定的浪费, 对于一个 episode, 可以拆分为多个 episode, 从而进行多次利用.\",\"s1​a2​​s2​a4​​s1​a2​​s2​a3​​s5​a1​​…s2​a4​​s1​a2​​s2​a3​​s5​a1​​…s1​a2​​s2​a3​​s5​a1​​…s2​a3​​s5​a1​​…s5​a1​​…​[originalepisode][episodestartingfrom(s2​,a4​)][episodestartingfrom(s1​,a2​)][episodestartingfrom(s2​,a3​)][episodestartingfrom(s5​,a1​)]​\",\"这样，我们不仅可以用来估计q(s1​,a2​), 还可以估计q(s2​,a4​),q(s2​,a3​)…\",\"Data-efficient methods:\",\"first-visit method 记录在 episode 中第一次出现的 state-action pair, 如果该 state-action pair 再次出现, 不记录 action value 估计中.\",\"every-visit method 对于每个 state-action pair, 都记录 action value 估计中.\"]},\"155\":{\"h\":\"2.2 高效地更新 Policy\",\"t\":[\"什么时候更新策略也是一个影响效率的因素。\",\"方法1：如 MC Based 一样，在收集到了足够多的 从给定的 state-action pair 出发的 episodes 后, 通过 mean estimation 估计了q(s,a)后, 才进行更新。 缺点，等候时间过长，只有当所有 episodes 均收集完，才能进行 策略更新。\",\"方法2：直接 uses the return of a single episode to approximate the action value. 这类算法统称为：Generalized policy iteration (GPI). 它会在 Policy-evaluation 和 policy-improvement 中不断切换，即不需要完全精确地求出 action value，就直接去更新策略。\"]},\"156\":{\"h\":\"2.3 MC Exploring Starts\",\"t\":[\"20240812004534\"]},\"157\":{\"h\":\"2.4 Exploring Statrts的解释\",\"t\":[\"Exploring 表示对于每一个 action-state pair (s,a), 都需要有多个 episodes, 这样才能去估计相应的qπ​(s,a). 如果存在一个 action value 未能访问，就不能确保所选择的 action 是最优的。\",\"Starts 表示对于对应 action-state pair (s,a) 的 episodes，每次都是从对应的状态 s 出发，选择对应的动作 a 进行的采样。 如果从其他状态出发，得到的 episode，如果经过了 (s,a)，那么这称为 visit , 但目前无法保证 visit 一定可以遍历所给定的 (s,a).\",\"据目前而言，Exploring Starts 是一个必要条件.\"]},\"158\":{\"h\":\"3. MC Eplison-Greedy\",\"t\":[\"将 Exploring Starts 条件转换掉，通过采取 Soft Policies 的方法。\"]},\"159\":{\"h\":\"3.1 Soft Policy\",\"t\":[\"A policy is called soft if the probability to take any action is positive. 显然 soft policy 是 stochastic 的，并且如果按照这样一个策略，在 episode 足够长的情况下，我们可以确保其可以遍历所有的 state-action pair.\"]},\"160\":{\"h\":\"3.2 -greedy policy\",\"t\":[\"在这里，我们采用的是 ϵ-greedy policies, 其属于 soft policies.\",\"π(a∣s)={1−∣A(s)∣ϵ​(∣A(s)∣−1),∣A(s)∣ϵ​,​forthegreedyaction,fortheother∣A(s)∣−1action,​\",\"其中 ϵ∈[0,1] 且 ∣A(s)∣ 为状态 s 的动作数量.ϵ-greedy policy 可以平衡 exploitation 和 exploration. 显然ϵ=0, policy 就是 greedy 的; 如果ϵ=1, 此时就是随机策略，其探索性就很强.\"]},\"161\":{\"h\":\"3.3 -greedy policy 引入 MC-based 算法中\",\"t\":[\"对于 MC Basic 以及 MC Exploring 中的 policy improvement 中，找的是在所有可能策略中的最优策略，因此是一个确定的贪心策略。\",\"20240812011140\"]},\"162\":{\"h\":\"3.3 算法流程\",\"t\":[\"20240812010538\"]},\"163\":{\"c\":[\"academic\"]},\"164\":{\"c\":[\"强化学习\"]},\"165\":{\"h\":\"RL6 - 随机近似理论与随机梯度下降算法\",\"t\":[\"针对 mean estimation 问题进行研究，因为在 RL 中 无论是 state value 还是 action value 其定义都是一个均值 (means)\",\"Stochastic approximation(SA): SA refers to a broad class of stochastic iterative algorithms soloving root finding or optimization problems.\"]},\"166\":{\"h\":\"1. 引言\"},\"167\":{\"h\":\"1.1 求均值的方法\",\"t\":[\"第一种：直接通过 E[x]≈xˉ:=N1​∑i=1N​xi​，进行估计，只有当样本全部收集完才能估计.\",\"第二种: 增量式的迭代算法. 假设:\",\"wk+1​=k1​i=1∑k​xi​,k=1,2,…\",\"对应的\",\"wk​=k−11​i=1∑k−1​xi​,k=2,3,…\",\"那么，wk+1​可以由wk​推导出来，即\",\"wk+1​​=k1​∑i=1k​xi​​=k1​(∑i=1k−1​xi​+xk​)=k1​((k−1)wk​+xk​)​=wk​−k1​(wk​−xk​)​\",\"因此，wk+1​=wk​−k1​(wk​−xk​)\"]},\"168\":{\"h\":\"2. Robbins-Monto(RM) algorithm\"},\"169\":{\"h\":\"2.1 问题引入\",\"t\":[\"假设我们需要求解如下方程:\",\"g(w)=0\",\"其中, w∈R 且需要被求解出来，g:R→R 为一个函数方程. 显然，如果对于 g(w) 已知的情况，我们可以通过一些特定的算法进行求解。 如果 g(w) 未知，就需要新的算法进行解决。\"]},\"170\":{\"h\":\"2.2 算法介绍\",\"t\":[\"RM 算法就可以用来求解当 g(w) 未知时的情况，即函数 g(w) 是一个黑盒，我们只能通过 输入序列: wk​, 得到含有噪音的观测值序列: g​(wk​,ηk​) 具体解决如下:\",\"wk+1​=wk​−ak​g​(wk​,ηk​),k=1,2,3,…\",\"其中:\",\"wk​ 是第 k 次方程根的估计.\",\"g​(wk​,ηk​)=g(wk​)+ηk​ 是第 k 次的观测值(含噪音).\",\"ak​ 是一个 positive coefficient.\"]},\"171\":{\"h\":\"2.3 收敛性分析\",\"t\":[\"Robbins-Monro Theorem In the Robbins-Monro algorithm, if\",\"0<c1​≤▽w​g(w)≤c2​,forallw; 要求g(w)必须是递增的，确保根是存在且唯一的。\",\"∑k=1∞​ak​=∞ 且 ∑k=1∞​ak2​<∞;∑k=1∞​ak2​=∞ 保证 ak​→0,k→0∑k=1∞​ak​=∞ 保证 ak​→0不要过快.\",\"E[ηk​∣Hk​]=0 且 E[ηk2​∣Hk​]<∞; 其中Hk​=wk​,wk−1​,…, 那么 wk​ converges with probability 1 (w.p.1) to the root w∗ satisfying g(w∗)=0.\",\"ak​=k1​是满足上面三个条件的. 但实际上我们往往是选择一个非常小的常数。\"]},\"172\":{\"h\":\"2.4 应用于 mean estimation 中\",\"t\":[\"比如我们要估计某个随机变量X的 E[X] 我们可以设计如下方程:\",\"g(w)≐w−E[X].\",\"那么只要求解 g(w)=0, 我们就可以得到 E[X] 的值。 同样，我们不能直接得到随机变量的值，而是对应的样本 x，sample of X. 即，我们得到的观测值是:\",\"g​(w,x)≐w−x\",\"我们可以修改为噪音 η 的形式，\",\"g​(w,η)​=w−x​=w−x+E[X]−E[X]=(w−E[X])+(E[X]−x)​≐g(w)+η​\",\"因此我们可以通过 RM 算法来进行求解\",\"wk+1​=wk​−αk​g​(wk​,ηk​)=wk​−αk​(wk​−xk​)\"]},\"173\":{\"h\":\"3. Stochastic gradient descent\"},\"174\":{\"h\":\"3.1 问题引入\",\"t\":[\"需要求解一个优化问题:\",\"wargmin​J(w)=E[f(w,X)]\",\"其中，\",\"w 是需要被优化的参数\",\"X 是一个随机变量\",\"w 和 X 可以是标量，也可以是向量. 对于函数 f(⋅) 输出为标量.\",\"对于这个问题，我们有以下几种方法:\",\"Method 1: 梯度下降法 (gradient descent, GD)\",\"wk+1​=wk​−αk​▽w​E[f(wk​,X)]=wk​−αk​E[▽w​f(wk​,X)]\",\"但由于 j(w) 是一个期望值，我们很难直接获得.\",\"Method 2: batch gradient descent (BGD) 借用 MC 的思想，我们可以将:\",\"E[▽w​f(wk​,X)]≈n1​i=1∑n​▽w​f(wk​,xi​).\",\"因此\",\"wk+1​=wk​−αk​n1​i=1∑n​▽w​f(wk​,xi​)\",\"但需要大量的 samples 收集完毕才能进行一次迭代.\",\"Method 3: 随机梯度下降(SGD) 考虑能否仅用一次 sample 进行迭代.\",\"wk+1​=wk​−αk​▽w​f(wk​,xk​)\",\"但能否保证其精确度，以及是否可以到最后优化的成果。\"]},\"175\":{\"h\":\"3.2 SGD 分析\"},\"176\":{\"h\":\"mean estimation 问题转化\",\"t\":[\"我们可以将 均值估计 问题 转化为 一个 优化问题 进行求解：\",\"20240814014058\"]},\"177\":{\"h\":\"SGD 正确性和收敛性分析\",\"t\":[\"从 GD 到 SGD:\",\"wk+1​=wk​−αk​E[▽w​f(wk​,X)]⇓wk+1​=wk​−αk​▽w​f(wk​,x)​\",\"显然我们可以将 ▽w​f(wk​,x) 视为 E[▽w​f(wk​,x)] 的一个观测值(含噪声):\",\"▽w​f(wk​,x)=E[▽w​f(wk​,x)]+η▽w​f(wk​,x)−E[▽w​f(wk​,x)]​​\",\"因为\",\"▽w​f(wk​,x)=E[▽w​f(wk​,x)]\",\"因此，我们需要思考使用 SGD 时wk​→w∗ as k→∞ 是否成立。\",\"我们可以将 SGD 视为一个特殊情况下的 RM 算法 SGD的目标是 minimize\",\"J(w)=E[f(w,X)]\",\"而最小值问题，往往可以转化为导数为 0 的情况,\",\"▽w​J(w)=E[▽w​f(w,X)]=0\",\"显然，可以参考 RM 算法, 让\",\"g(w)=▽w​J(w)=E[▽w​f(w,X)]\",\"从而转换为一个 root-finding 问题. 相应的，对于观测值g​(w,η),\",\"g~​(w,η)​=∇w​f(w,x)=g(w)E[∇w​f(w,X)]​​+η∇w​f(w,x)−E[∇w​f(w,X)]​​.​\",\"因此，我们就可以通过 RM 算法进行求解g(w)=0,\",\"wk+1​=wk​−αk​g​(wk​,ηk​)=wk​−αk​▽w​f(wk​,xk​)\",\"对应收敛性证明\"]},\"178\":{\"h\":\"3.3 SGD 另一种问题描述方法 (deterministic formulation)\",\"t\":[\"在之前关于使用 SGD 算法的问题描述中，我们是引入了 随机变量 和 期望的情况. 我们可以将这个问题可以转化为一个随机变量的方法，从而引入 SGD 算法.\"]},\"179\":{\"h\":\"3.4 BGD MBGD SGDw\",\"t\":[\"20240814230747\"]},\"180\":{\"c\":[\"academic\"]},\"181\":{\"c\":[\"强化学习\"]},\"182\":{\"h\":\"RL7 - Temporal-Difference Learning\"},\"183\":{\"h\":\"1. 引入\",\"t\":[\"考虑一个复杂的均值估计问题: 计算\",\"ω=E[R+γv(X)],\",\"其中, R, X 均是随机变量，γ 是常数，v(⋅) 表示一个函数。 显然我们仍然可以通过 RM 算法进行求解，假设我们可以得到有关随机变量 R, X 的采样 {x},{r}\",\"g(w)g~​(w,η)​=w−E[R+γv(X)]=w−[r+γv(x)]=(w−E[R+γv(X)])+(E[R+γv(X)]−[r+γv(x)])≐g(w)+η​\",\"因此，我们可以将该问题定义为一个 root-finding 问题: g(w)=0. 相应的 RM 算法为:\",\"wk+1​=wk​−αk​g~​(wk​,ηk​)=wk​−αk​[wk​−[rk​+γv(xk​)]]\"]},\"184\":{\"h\":\"2. TD Learning of state value\",\"t\":[\"求解给定策略 π 的 state value，这样就可以与 policy improvement 结合去寻找最优策略。\"]},\"185\":{\"h\":\"2.1 算法描述\",\"t\":[\"算法所需的数据(experience): 根据给定的策略 π 所生成的数据 (s0​,r1​,s1​,…,st​,rt+1​,st+1​,…) or {(st​,rt+1​,st+1​)}\",\"相应的算法是:\",\"vt+1​(st​)vt+1​(s)​=vt​(st​)−αt​(st​)[vt​(st​)−[rt+1​+γvt​(st+1​)]]=vt​(s),∀s=st​,​\",\"其中 t=0,1,2,…, vt​(st​)是关于 vπ​(st​) 的估计。\",\"newestimatevt+1​(st​)​​=currentestimatevt​(st​)​​−αt​(st​)[vt​(st​)−TDtargetvt​ˉ​[rt+1​+γvt​(st+1​)]​​]​TDerrorδt​​\"]},\"186\":{\"h\":\"2.2 算法分析\",\"t\":[\"TD 算法是用来求解一个 给定策略 π 的 Bellman equation.\",\"根据 state value 的定义，对于策略 π 的 state value\",\"vπ​(s)=E[R+γG∣S=s],s∈S\",\"其中 G 是 discounted return。\",\"E[G∣S=s]=a∑​π(a∣s)s′∑​p(s′∣s,a)vπ​(s)=E[vπ​(S′)∣S=s]\",\"因此，我们可以写出 Bellman equation 的新形式，称为 Bellman expection equation\",\"vπ​(s)=E[E+γvπ​(S′)∣S=s],s∈S\"]},\"187\":{\"h\":\"2.3 TD 算法 与 MC 算法的比较\"},\"188\":{\"h\":\"3. TD Learning of action value\",\"t\":[\"Sarsa (state-action-reward-state-action) Sarsa 算法其目的是用于直接估计 action value, 从而可以在 policy improvement 中直接根据 action value 进行更新即可。\",\"Sarsa 算法同样是来求解 Bellman equation:\",\"qπ​(s,a)=E[R+γqπ​(S′,A′)∣s,a],∀s,a\"]},\"189\":{\"h\":\"3.1 Sarsa\",\"t\":[\"假设我们具有 some experience {(st​,at​,rt+1​,st+1​,at+1​)} 对应的 Sarsa 算法如下来进行估计 action value:\",\"qt+1​(st​,at​)qt+1​(s,a)​=qt​(sa​,at​)−αt​(st​,at​)[qt​(sa​,at​)−[rt+1​+γqt​(st+1​,at+1​)]]=qt​(s,a),∀(s,a)=(st​,at​)​\",\"其中 t=0,1,2,…, qt​(st​,at​) 是qπ​(st​,at​)的估计。\",\"收敛性情况\",\"20240817000114\",\"伪代码\",\"20240817000134\",\"20240817000230\"]},\"190\":{\"h\":\"3.2 n-step Sarsa\",\"t\":[\"20240817000500\",\"20240817000601\",\"20240817000642\"]},\"191\":{\"h\":\"3.3 Expected Sarsa\",\"t\":[\"20240817000331\",\"20240817000409\"]},\"192\":{\"h\":\"4. TD Learning of optimal action value\",\"t\":[\"Q-learning 算法是用来解决 action value 形式下的贝尔曼最优公式 (Bellman optimality equation in terms of action value)\",\"q(s,a)=E[Rt+1​+γamax​q(St+1​,a)∣St​=s,At​=a],∀s,a\"]},\"193\":{\"h\":\"4.1 Q-learning\",\"t\":[\"Q-learning 直接估计的是 optimal action value，因此不需要进行 policy improvement。\",\"qt+1​(st​,at​)qt+1​(s,a)​=qt​(st​,at​)−αt​(st​,at​)[qt​(st​,at​)−[rt+1​+γa∈Amax​qt​(st+1​,a)]]=qt​(s,a),∀(s,a)=(st​,at​)​\"]},\"194\":{\"h\":\"4.2. off-policy | on-policy\",\"t\":[\"behavior policy: 是用来与环境进行交互，从而生成经验数据的策略\",\"target policy: 是我们不断进行更新的策略，最终优化的策略\"]},\"195\":{\"h\":\"on - policy\",\"t\":[\"该算法中 behavior policy 和 target policy 是一致的，即我通过这个策略与环境进行交互生成一系列经验，在通过经验来更新这个策略。\"]},\"196\":{\"h\":\"off - policy\",\"t\":[\"该算法中 behavior policy 和 target policy 是不同的，即我通过一个策略与环境进行交互生成一系列经验。再通过这些经验来不断改进更新另一个策略，这另一个策略会更新到最优的策略。\",\"Sarsa，MC 是 on-policy 的 Q-learning 是 off-policy 的\"]},\"197\":{\"h\":\"4.3 Q-learning 伪代码\",\"t\":[\"因为 Q-learning 是 off-policy 的，因此，如果我们强制让 target policy 与 behavior ppolicy 一致也是可以的，此时也可以是 on-policy 的。\"]},\"198\":{\"h\":\"off-poicy 版本\",\"t\":[\"20240818182057\",\"此时 target policy 就不需要是 ϵ−greedy 策略了，因为不需要 target policy 进行生成数据。\"]},\"199\":{\"h\":\"on-policy 版本\",\"t\":[\"20240818181917\"]},\"200\":{\"h\":\"5. TD 算法的统一形式和总结\",\"t\":[\"20240818182301\",\"20240818182231\"]},\"201\":{\"c\":[\"academic\"]},\"202\":{\"c\":[\"强化学习\"]},\"203\":{\"h\":\"RL8 - 值函数近似(Value Function Approximation)\",\"t\":[\"对于 q-value 的估计从 基于表格的 (tabular representation) 转换到 基于函数的 (function representation)\"]},\"204\":{\"h\":\"1. 引入\",\"t\":[\"通过使用一个函数来进行拟合 state values 或者 action values: v^(s,w)≈vπ​(s)， 其中w∈Rm是参数向量。\",\"可以提高存储效率\",\"提高泛化能力\"]},\"205\":{\"h\":\"2. Alogorithm of state value estimation\",\"t\":[\"目标: 寻找一个最优的参数w，使得v^(s,w)最接近真实的vπ​(s).\",\"共两步:\",\"定义目标函数\",\"优化目标函数的算法\"]},\"206\":{\"h\":\"2.1 Obejctive function\",\"t\":[\"J(w)=E[(vπ​(S)−v^(S,w))2]\",\"分析随机变量 S 的 probability distribution (即对于损失函数中的 expection 需要考虑怎样对状态进行平均):\"]},\"207\":{\"h\":\"uniform distributon\",\"t\":[\"认为所有状态都是同等重要的，即各个状态的可能性为∣S∣1​ 因此这种情况下的 objective function 可以写成:\",\"J(w)=E[(vπ​(S)−v^(S,w))2]=∣S∣1​s∈S∑​(vπ​(s)−v^(s,w))2\",\"但实际情况可能并不是所有状态的概率都是一致的，基于给定策略下，一些状态可能很少被访问，另一些则频繁被访问，因此采用这种 objective function 就不太可行。\"]},\"208\":{\"h\":\"stationary distribution\",\"t\":[\"stationary: 表示是一种长时间的交互行为\",\"distributon: 表示是 状态 的分布\",\"通常也称为 steady-state distributon or limiting distributon.\",\"describes the long-run behavior of a Markov process. 即基于一个策略，我们不断地与环境进行交互，最终会达到一个平稳的状态，此时可以分析每一个状态在这个策略下的概率。\",\"设 {dπ​(s)}s∈S​ 表示 基于策略 π 下的 stationary distribution。其中 dπ​(s)≥0 且 ∑s∈S​dπ​(s)=1\",\"那么此时的 objective function 可以表示为：\",\"J(w)=E[(vπ​(S)−v^(S,w))2]=s∈S∑​dπ​(s)(vπ​(s)−v^(s,w))2\",\"20240820181406\",\"20240820181718\"]},\"209\":{\"h\":\"2.2 Optimization algorithms 优化算法\",\"t\":[\"目前的优化算法只是在估计给定策略的 statevalue\",\"minisize obejctive function J(w), 采用 梯度下降 算法:\",\"wk+1​=wk​−αk​▽w​J(wk​)\",\"对应目标函数的真实梯度是：\",\"▽w​j(w)​=▽w​E[(vπ​(S)−v^(S,w))2]=E[▽w​(vπ​(S)−v^(S,w))2]=−2E[(vπ​(S)−v^(S,w))▽w​v^(S,w)]​\",\"这里包含了一个 Expection，因此可以考虑 SGD 方法进行求解：\",\"wk+1​=wk​+αk​(vπ​(st​)−v^(st​,wt​))▽w​vt​^​(st​,wt​)\",\"其中st​是随机变量S的一个样本。 但这里还有一个难点，vπ​(st​) 我们是无法估计的，这是我们所求的量，因此需要用近似算法来进行替代，从而使得算法可行。\"]},\"210\":{\"h\":\"Monte Carlo learning with function approximation\",\"t\":[\"设 gt​ 表示在一个 episode 中，从状态 st​ 出发的 discounted return。因此我们用 gt​ 来近似 vπ​(st​), 即：\",\"wk+1​=wk​+αk​(gt​−v^(st​,wt​))▽w​vt​^​(st​,wt​)\"]},\"211\":{\"h\":\"TD Learning with function approximation\",\"t\":[\"在 TD 算法中，我们将 rt+1​+γv^(st+1​,wt​) 来近似 vπ​(st​), 因此对应算法为：\",\"wk+1​=wk​+αk​(rt+1​+γv^(st+1​,wt​)−v^(st​,wt​))▽w​vt​^​(st​,wt​)\"]},\"212\":{\"h\":\"3. Sarsa with function approximation\",\"t\":[\"wk+1​=wk​+αk​(rt+1​+γq^​(st+1​,at+1​,wt​)−q^​(st​,at​,wt​))▽w​qt​^​(st​,at​,wt​)\",\"20240820184127\"]},\"213\":{\"h\":\"4. Q-learning with function approximation\",\"t\":[\"wk+1​=wk​+αk​(rt+1​+γa∈A(st+1​)max​q^​(st+1​,at+1​,wt​)−q^​(st​,at​,wt​))▽w​qt​^​(st​,at​,wt​)\",\"on-policy版本：\",\"20240820184405\"]},\"214\":{\"h\":\"5. Deep Q-learning (DQN)\",\"t\":[\"Deep Q-learning 目的是最小化目标函数(objective/loss function):\",\"J(w)=E[(R+γa∈A(S′)max​q^​(S′,a,w)−q^​(S,A,w))2]\",\"其中 (S,A,R,S′) 均是随机变量。\"]},\"215\":{\"h\":\"优化方法\",\"t\":[\"采用梯度下降。\",\"J(w)=E[(R+γa∈A(S′)max​q^​(S′,a,w)−q^​(S,A,w))2]\",\"对于 q^​(S,A,w) 求解梯度还是很好求的。 但对于 maxa∈A(S′)​q^​(S′,a,w) 其求解梯度比较难求，在 DQN 中采用一个 固定 的方法进行解决。 尝试将 y≐R+γmaxa∈A(S′)​q^​(S′,a,w) 中的 w 进行固定求解，具体如下：\",\"引入两个网络：\",\"main network q^​(s,a,w)w 会一直进行更新，根据梯度下降的公式。\",\"target network q^​(s′,a,wT​) 并不是一直进行更新，而是等 main network 更新一定次数后，将该网络的 w 复制到 wT​ 中\",\"将 objective function 修改为：\",\"J(w)=E[(R+γa∈A(S′)max​q^​(S′,a,wT​)−q^​(S,A,w))2]\",\"在计算 main network q^​(s,a,w) 的梯度时，将 q^​(S′,a,wT​) 中的 wT​ 固定不动，因此左侧那个类似 TD target 的就不是有关 w 的函数，不用进行求导，从而方便计算。 然后在更新了一定次数之后，在将 wT​=w 进行赋值。\",\"因此对应的损失函数的梯度可以修改为：\",\"▽w​J​=E[−2(R+γa∈A(S′)max​q^​(S′,a,wT​)−q^​(S,A,w))▽w​q^​(S,A,w)]=E[−2(YT​−q^​(S,A,w))▽w​q^​(S,A,w)]​\",\"一些细节:\",\"w 和 wT​ 表示 the main and target networks 的参数，在初始化的时候是设为相同的。\",\"在每一次迭代时，我们需要从经验池 (the replay buffer) 中取出一定数量的样本 (a mini-batch of samples {(s,a,r,s')}) 进行训练。\",\"网络的输入包括 状态 s 和 动作 a. 在训练求解梯度时，我们先直接求解 target network 的输出，视为 yT​≐r+γmaxa∈A(s′)​q^​(s′,a,wT​)。 然后我们通过 mini-batch 样本 {(s,a,yT​)}, 通过梯度的算法来最小化对应的损失函数, 假设有 N 个样本，那么对应的损失函数求解为：\",\"J(w)=i=1∑N​yT​−q^​(s,a,w) 即可以通过梯度下降，来更新参数值\",\"wt+1​=wt​+αt​N1​i=1∑N​(yT​−q^​(si​,ai​,wt​))⋅▽w​q^​(si​,ai​,wt​)\"]},\"216\":{\"h\":\"经验回放 (replay buffer)\",\"t\":[\"20240820230827\",\"20240820230920\",\"20240820230944\"]},\"217\":{\"h\":\"伪代码\",\"t\":[\"20240820231024\",\"但在发表 DQN 的文章中，不太一样，在原文是 on-policy 且 main network 的输出是不一样的。\",\"20240820231205\"]},\"218\":{\"c\":[\"academic\"]},\"219\":{\"c\":[\"强化学习\"]},\"220\":{\"h\":\"RL9 - 策略梯度法(Policy gradient)\",\"t\":[\"之前介绍的方法都是 value-based 的方法，从这章开始时基于 policy-based 的方法。\",\"policy function approximation 是直接建立一个基于策略的目标函数来进行梯度上升的优化。\"]},\"221\":{\"h\":\"1. 基本思路\",\"t\":[\"将基于表格表示的策略 转换为 基于函数表示的策略。 即此时策略 π 可以描述为：\",\"π(a∣s,θ)\",\"其中，θ∈Rm表示参数向量，是我们需要进行优化的。\",\"当策略是以表格的形式保存时，我们定义最优的策略为 在该策略下的所有 state value 都是最大的。\",\"当策略是以函数的形式存在时，我们定义 最优的策略 为 可以最大化一个确定的常数指标(certain scalar metrics).\",\"Policy gradient 的基本步骤：\",\"确定 metrics/objective function，来定义最优的策略：J(θ)\",\"进行优化，如梯度上升算法\",\"θt+1​=θt​+α▽θ​J(θt​)\"]},\"222\":{\"h\":\"2. 目标函数定义\"},\"223\":{\"h\":\"2.1 average state value\",\"t\":[\"vˉπ​=s∈S∑​d(s)vπ​(s)=dTvπ​\",\"vˉπ​ 显然是 state value 的加权平均。\",\"d(s)≥0 是各个 state 的权重\",\"∑s∈S​d(s)=1, 我们可以认为 d(s) 是 概率分布，因此该指标可以描述为:\",\"vˉπ​=ES∼d​[vπ​(S)]\"]},\"224\":{\"h\":\"另一种表达\",\"t\":[\"J(θ)=E[t=0∑∞​γtRt+1​]\",\"20240826173749\"]},\"225\":{\"h\":\"d(s)的选择\",\"t\":[\"d 与策略 π 无关 这种情况我们将 d 表示为 d0​, vˉπ​ 表示为 vˉπ0​. 这种情况下的 d 可以根据对各个状态的重要程度进行选择： 一种是将所有状态视为同等重要，一种则是有所偏向。\",\"d 与策略 π 有关 d 表示为 dπ​(s), 即在策略 π 下的 stationary distribution。\"]},\"226\":{\"h\":\"2.2 average return value\",\"t\":[\"rˉπ​=s∈S∑​dπ​(s)rπ​(s)=ES∼d​[rπ​(s)]\",\"其中:\",\"rπ​(s)r(s,a)​≐a∈A∑​π(a∣s)r(s,a)=E[R∣s,a]=r∑​rp(r∣s,a)​\",\"rπ​(s)表示在策略π下 状态s时可以得到的平均reward。r(s,a)表示在单步情况下(在状态s采用动作a)时的平均reward。\"]},\"227\":{\"h\":\"另一种表达\",\"t\":[\"假设 agent 跟随一个 给定的策略 然后生成了一个 trajectory以及对应的 rewards (Rt+1​,Rt+2​,…)\",\"对应 average single-step reward along this trajectory is\",\"====​n→∞lim​n1​E[Rt+1​+Rt+2​+⋯+Rt+n​∣St​=s0​]n→∞lim​n1​E[k=1∑n​Rt+k​∣St​=s0​]n→∞lim​n1​E[k=1∑n​Rt+k​]s∑​dπ​(s)rπ​(s)rˉπ​​\"]},\"228\":{\"h\":\"3. 目标函数梯度求解\",\"t\":[\"这里在视频没有详细介绍，只给出了梯度的公式：\",\"▽θ​J(θ)​=s∈S∑​η(s)a∈A∑​▽θ​π(a∣s,θ)qπ​(s,a)=E[▽θ​lnπ(A∣S,θ)qπ​(S,A)]​\",\"其中\",\"J(θ) 可以是 vˉπ​,rˉπ​,vˉπ0​ 任何一种。\",\"\\\"=\\\" 有表示 严格等于 近似 以及 成比例等于\",\"η 表示 state 的权重或者分布\",\"具体推导过程:\",\"▽θ​lnπ(a∣s,θ)▽θ​J(θ)​=π(a∣s,θ)▽θ​π(a∣s,θ)​=s∈S∑​d(s)a∈A∑​▽θ​π(a∣s,θ)qπ​(s,a)=s∈S∑​d(s)a∈A∑​π(a∣s,θ)▽θ​lnπ(a∣s,θ)qπ​(s,a)=ES∼d​[a∑​π(s∣S,θ)▽θ​lnπ(a∣S,θ)qπ​(S,a)]=ES∼d,A∼π​[▽θ​lnπ(A∣S,θ)qπ​(S,A)]≐E[▽θ​lnπ(A∣S,θ)qπ​(S,A)]​\",\"根据这个式子我们就可以通过 SGD 方法，从而可以进行近似求解：\",\"▽θ​J(θ)≈▽θ​lnπ(a∣s,θ)qπ​(s,a)\",\"一些特性 这里的策略是随机性 的，因为我们需要计算的是 lnπ(a∣s,θ), 因此我们需要保证对于所有的 s,a,θ\",\"π(a∣s,θ)≥0\",\"20240826180244\"]},\"229\":{\"h\":\"4. REINFORCE 梯度上升算法\",\"t\":[\"梯度上升算法的本质就是最大化目标函数 J(θ)\",\"θt+1​​=θt​+α▽θ​J(θ)=θt​+αE[▽θ​lnπ(A∣S,θt​)qπ​(S,A)]​\",\"而对应的真实梯度可以用一个估计的梯度来替代:\",\"θt+1​=θt​+α▽θ​lnπ(at​∣st​,θt​)qπ​(s,a)\",\"但还存在 qπ​(s,a) 是未知的，我们也可以进行近似：\",\"θt+1​=θt​+α▽θ​lnπ(a∣s,θt​)qt​(st​,at​)\",\"这里可以用不同的方法来近似 qπ​(s,a).\",\"Monte-Carlo based method， 我们便称为 REINFORCE\",\"也可以采用基于 TD 的算法 或者 其他的算法。\",\"一些细节\",\"20240826181340\",\"20240826181538\",\"20240826181638\"]},\"230\":{\"h\":\"REINFORCE 算法\",\"t\":[\"20240826181712\"]},\"231\":{\"c\":[\"academic\"]},\"232\":{\"c\":[\"强化学习\"]},\"233\":{\"h\":\"\",\"t\":[\"404 Not Found\"]},\"234\":{\"h\":\"Daily\"},\"235\":{\"h\":\"Java\"},\"236\":{\"h\":\"Code\"},\"237\":{\"h\":\"UAV\"},\"238\":{\"h\":\"Academic\"},\"239\":{\"h\":\"强化学习\"}},\"dirtCount\":0,\"index\":[[\"梯度上升算法的本质就是最大化目标函数\",{\"1\":{\"229\":1}}],[\"梯度上升算法\",{\"0\":{\"229\":1}}],[\"梯度下降\",{\"1\":{\"209\":1}}],[\"梯度下降法\",{\"1\":{\"174\":1}}],[\"成比例等于\",{\"1\":{\"228\":1}}],[\"成员变量一样\",{\"1\":{\"61\":1}}],[\"成员变量的初始化\",{\"1\":{\"9\":1}}],[\"成员内部类的name\",{\"1\":{\"61\":2}}],[\"成员内部类的类型名称就是\",{\"1\":{\"61\":1}}],[\"成员内部类可以访问到外部的成员变量\",{\"1\":{\"61\":1}}],[\"成员内部类也可以使用访问权限控制\",{\"1\":{\"61\":1}}],[\"成员内部类和成员方法\",{\"1\":{\"61\":1}}],[\"成员内部类其实在某些情况下使用起来比较麻烦\",{\"1\":{\"61\":1}}],[\"成员内部类\",{\"0\":{\"61\":1}}],[\"成员\",{\"1\":{\"59\":1}}],[\"成员方法因为需要具体对象使用\",{\"1\":{\"68\":1}}],[\"成员方法只能通过\",{\"1\":{\"68\":1}}],[\"成员方法中不能用\",{\"1\":{\"25\":1}}],[\"成员方法\",{\"1\":{\"17\":1}}],[\"近似\",{\"1\":{\"228\":1}}],[\"严格等于\",{\"1\":{\"228\":1}}],[\"任何一种\",{\"1\":{\"228\":1}}],[\"任何方法都可以通过方法引用作为实现\",{\"1\":{\"68\":1}}],[\"给定的策略\",{\"1\":{\"227\":1}}],[\"给定策略\",{\"1\":{\"186\":1}}],[\"概率分布\",{\"1\":{\"223\":1}}],[\"概念上的定义\",{\"1\":{\"4\":1}}],[\"确定\",{\"1\":{\"221\":1}}],[\"确保根是存在且唯一的\",{\"1\":{\"171\":1}}],[\"经验回放\",{\"0\":{\"216\":1}}],[\"网络的输入包括\",{\"1\":{\"215\":1}}],[\"复制到\",{\"1\":{\"215\":1}}],[\"尝试将\",{\"1\":{\"215\":1}}],[\"固定不动\",{\"1\":{\"215\":1}}],[\"固定\",{\"1\":{\"215\":1}}],[\"≥0\",{\"1\":{\"208\":1,\"223\":1,\"228\":1}}],[\"≥vπ​\",{\"1\":{\"126\":1}}],[\"认为所有状态都是同等重要的\",{\"1\":{\"207\":1}}],[\"寻找一个最优的参数w\",{\"1\":{\"205\":1}}],[\"版本\",{\"0\":{\"198\":1,\"199\":1}}],[\"形式下的贝尔曼最优公式\",{\"1\":{\"192\":1}}],[\"称为\",{\"1\":{\"186\":1}}],[\"称为策略评估\",{\"1\":{\"118\":1}}],[\"结合去寻找最优策略\",{\"1\":{\"184\":1}}],[\"结果是\",{\"1\":{\"64\":1}}],[\"≐e\",{\"1\":{\"228\":1}}],[\"≐g\",{\"1\":{\"183\":1}}],[\"≐w−x\",{\"1\":{\"172\":1}}],[\"≐w−e\",{\"1\":{\"172\":1}}],[\"ω=e\",{\"1\":{\"183\":1}}],[\"期望的情况\",{\"1\":{\"178\":1}}],[\"另一种表达\",{\"0\":{\"224\":1,\"227\":1}}],[\"另一种问题描述方法\",{\"0\":{\"178\":1}}],[\"另一些则频繁被访问\",{\"1\":{\"207\":1}}],[\"另一分布下\",{\"1\":{\"102\":1}}],[\"∇w​f\",{\"1\":{\"177\":2}}],[\"往往可以转化为导数为\",{\"1\":{\"177\":1}}],[\"=\",{\"1\":{\"189\":1,\"193\":1}}],[\"=e\",{\"1\":{\"177\":1}}],[\"=n\",{\"1\":{\"81\":1,\"84\":1,\"86\":1}}],[\"含噪声\",{\"1\":{\"177\":1}}],[\"含噪音\",{\"1\":{\"170\":1}}],[\"视为一个特殊情况下的\",{\"1\":{\"177\":1}}],[\"视为\",{\"1\":{\"177\":1,\"215\":1}}],[\"⇓wk+1​=wk​−αk​▽w​f\",{\"1\":{\"177\":1}}],[\"到\",{\"1\":{\"177\":1}}],[\"转化为\",{\"1\":{\"176\":1}}],[\"转换到\",{\"1\":{\"203\":1}}],[\"转换到状态s\",{\"1\":{\"93\":1}}],[\"转换为\",{\"1\":{\"131\":1,\"149\":1,\"150\":1,\"221\":1}}],[\"转换为数字666\",{\"1\":{\"37\":1}}],[\"借用\",{\"1\":{\"174\":1}}],[\"▽w​q^​\",{\"1\":{\"215\":2}}],[\"▽w​qt​^​\",{\"1\":{\"212\":1,\"213\":1}}],[\"▽w​vt​^​\",{\"1\":{\"209\":1,\"210\":1,\"211\":1}}],[\"▽w​v^\",{\"1\":{\"209\":1}}],[\"▽w​\",{\"1\":{\"209\":1}}],[\"▽w​j​=e\",{\"1\":{\"215\":1}}],[\"▽w​j\",{\"1\":{\"177\":1,\"209\":1}}],[\"▽w​f\",{\"1\":{\"174\":2,\"177\":10}}],[\"▽θ​π\",{\"1\":{\"228\":1}}],[\"▽θ​lnπ\",{\"1\":{\"228\":6,\"229\":1}}],[\"▽θ​ln\",{\"1\":{\"99\":3}}],[\"▽θ​j\",{\"1\":{\"99\":1,\"228\":3}}],[\"⋅▽w​q^​\",{\"1\":{\"215\":1}}],[\"⋅\",{\"1\":{\"174\":1,\"183\":1}}],[\"η\",{\"1\":{\"172\":2,\"177\":2,\"183\":1,\"228\":1}}],[\"ηk2​∣hk​\",{\"1\":{\"171\":1}}],[\"ηk​∣hk​\",{\"1\":{\"171\":1}}],[\"ηk​\",{\"1\":{\"170\":3,\"172\":1,\"177\":1,\"183\":1}}],[\"应用于\",{\"0\":{\"172\":1}}],[\"应该是无人机最终停的位置即是部署的最佳位置\",{\"1\":{\"86\":1}}],[\"保证\",{\"1\":{\"171\":2}}],[\"保证选取最大的\",{\"1\":{\"130\":1}}],[\"收敛性情况\",{\"1\":{\"189\":1}}],[\"收敛性分析\",{\"0\":{\"171\":1}}],[\"收集完毕才能进行一次迭代\",{\"1\":{\"174\":1}}],[\"收益最大\",{\"1\":{\"121\":1}}],[\"输入序列\",{\"1\":{\"170\":1}}],[\"输出为标量\",{\"1\":{\"174\":1}}],[\"输出为\",{\"1\":{\"64\":1}}],[\"问题\",{\"1\":{\"176\":1,\"177\":1,\"183\":1}}],[\"问题转化\",{\"0\":{\"176\":1}}],[\"问题引入\",{\"0\":{\"169\":1,\"174\":1}}],[\"问题进行研究\",{\"1\":{\"165\":1}}],[\"增量式的迭代算法\",{\"1\":{\"167\":1}}],[\"第二种\",{\"1\":{\"167\":1}}],[\"第一种\",{\"1\":{\"167\":1}}],[\"引言\",{\"0\":{\"166\":1}}],[\"引入两个网络\",{\"1\":{\"215\":1}}],[\"引入\",{\"0\":{\"161\":1,\"183\":1,\"204\":1},\"1\":{\"121\":1}}],[\"引入随机变量后对应的discounted\",{\"1\":{\"112\":1}}],[\"引入discount\",{\"1\":{\"92\":1}}],[\"针对\",{\"1\":{\"165\":1}}],[\"随机变量\",{\"1\":{\"178\":1}}],[\"随机梯度下降\",{\"1\":{\"174\":1}}],[\"随机近似理论与随机梯度下降算法\",{\"0\":{\"165\":1}}],[\"随着无人机与用户之间距离和发射功率的变化\",{\"1\":{\"82\":1}}],[\"随着我们的程序不断变大\",{\"1\":{\"16\":1}}],[\"找的是在所有可能策略中的最优策略\",{\"1\":{\"161\":1}}],[\"ϵ−greedy\",{\"1\":{\"198\":1}}],[\"ϵ∈\",{\"1\":{\"160\":1}}],[\"ϵ\",{\"1\":{\"160\":2}}],[\"足够长的情况下\",{\"1\":{\"159\":1}}],[\"条件转换掉\",{\"1\":{\"158\":1}}],[\"据目前而言\",{\"1\":{\"157\":1}}],[\"未能访问\",{\"1\":{\"157\":1}}],[\"未知时的情况\",{\"1\":{\"170\":1}}],[\"未知\",{\"1\":{\"9\":1,\"150\":1,\"169\":1}}],[\"均是随机变量\",{\"1\":{\"183\":1,\"214\":1}}],[\"均值估计\",{\"1\":{\"176\":1}}],[\"均收集完\",{\"1\":{\"155\":1}}],[\"均为非负整数\",{\"1\":{\"55\":1}}],[\"缺点\",{\"1\":{\"155\":1}}],[\"估计了q\",{\"1\":{\"155\":1}}],[\"估计中\",{\"1\":{\"154\":2}}],[\"什么时候更新策略也是一个影响效率的因素\",{\"1\":{\"155\":1}}],[\"什么都不写\",{\"1\":{\"17\":1}}],[\"高效地更新\",{\"0\":{\"155\":1}}],[\"高度h=hn​\",{\"1\":{\"84\":1}}],[\"高度的上界是最大发射功率pmax​的函数\",{\"1\":{\"82\":1}}],[\"记录在\",{\"1\":{\"154\":1}}],[\"节所述\",{\"1\":{\"152\":1}}],[\"迭代\",{\"1\":{\"152\":1}}],[\"≈▽θ​lnπ\",{\"1\":{\"228\":1}}],[\"≈vπ​\",{\"1\":{\"204\":1}}],[\"≈n1​i=1∑n​▽w​f\",{\"1\":{\"174\":1}}],[\"≈n1​i=1∑n​g\",{\"1\":{\"151\":1}}],[\"≈xˉ\",{\"1\":{\"167\":1}}],[\"≈xˉ=n1​j=1∑n​xj​\",{\"1\":{\"148\":1}}],[\"假设有\",{\"1\":{\"215\":1}}],[\"假设我们具有\",{\"1\":{\"189\":1}}],[\"假设我们可以得到有关随机变量\",{\"1\":{\"183\":1}}],[\"假设我们需要求解如下方程\",{\"1\":{\"169\":1}}],[\"假设我们有了一系列\",{\"1\":{\"151\":1}}],[\"假设\",{\"1\":{\"167\":1,\"227\":1}}],[\"假设功率q=qn​\",{\"1\":{\"84\":1}}],[\"出发的\",{\"1\":{\"155\":1,\"210\":1}}],[\"出发\",{\"1\":{\"151\":2,\"157\":1}}],[\"出现重名时\",{\"1\":{\"7\":1}}],[\"已知的情况\",{\"1\":{\"169\":1}}],[\"已知\",{\"1\":{\"150\":1}}],[\"效率过低\",{\"1\":{\"149\":1}}],[\"样本\",{\"1\":{\"215\":1}}],[\"样本必须是独立同分布\",{\"1\":{\"148\":1}}],[\"样本采样\",{\"1\":{\"148\":1}}],[\"大数定理\",{\"1\":{\"148\":1}}],[\"大体内容其实普通类差不多\",{\"1\":{\"27\":1}}],[\"若有一系列\",{\"1\":{\"148\":1}}],[\"等候时间过长\",{\"1\":{\"155\":1}}],[\"等均未知\",{\"1\":{\"148\":1}}],[\"等价于\",{\"1\":{\"55\":6}}],[\"蒙特卡洛方法\",{\"0\":{\"148\":1}}],[\"⋮\",{\"1\":{\"143\":4}}],[\"方便后续比较\",{\"1\":{\"143\":1}}],[\"方法进行求解\",{\"1\":{\"209\":1}}],[\"方法2\",{\"1\":{\"155\":1}}],[\"方法1\",{\"1\":{\"155\":1}}],[\"方法引用其实本质上就相当于将其他方法的实现\",{\"1\":{\"68\":1}}],[\"方法引用\",{\"0\":{\"68\":1},\"1\":{\"68\":2}}],[\"方法名\",{\"1\":{\"63\":1,\"68\":4}}],[\"方法名称\",{\"1\":{\"5\":2}}],[\"方法名称同样可以随便起\",{\"1\":{\"5\":1}}],[\"方法参数的name\",{\"1\":{\"61\":1}}],[\"方法等\",{\"1\":{\"61\":1}}],[\"方法重写\",{\"0\":{\"25\":1}}],[\"方法\",{\"0\":{\"45\":1,\"96\":1},\"1\":{\"24\":1,\"150\":1,\"228\":1}}],[\"方法体\",{\"1\":{\"5\":1}}],[\"方法的返回类型\",{\"1\":{\"8\":1}}],[\"方法的重载是为某个方法提供更多种类\",{\"1\":{\"25\":1}}],[\"方法的重载\",{\"0\":{\"8\":1}}],[\"方法的进阶使用\",{\"0\":{\"6\":1}}],[\"方法的调用\",{\"1\":{\"5\":1}}],[\"方法的定义如下\",{\"1\":{\"5\":1}}],[\"方法的创建与使用\",{\"0\":{\"5\":1}}],[\"方法是语句的集合\",{\"1\":{\"5\":1}}],[\"两个算法迭代过程十分类似\",{\"1\":{\"143\":1}}],[\"首先初始化一个随机的策略π0​\",{\"1\":{\"152\":1}}],[\"首先随机设计一个初始的策略π0​\",{\"1\":{\"139\":1}}],[\"首先实现cloneable接口\",{\"1\":{\"29\":1}}],[\"步骤类似\",{\"1\":{\"152\":1}}],[\"步骤中\",{\"1\":{\"141\":2}}],[\"步来求解\",{\"1\":{\"144\":1}}],[\"步\",{\"1\":{\"136\":1}}],[\"动态规划\",{\"0\":{\"134\":1}}],[\"动作\",{\"1\":{\"92\":1,\"215\":1}}],[\"动作空间包含两个部分\",{\"1\":{\"87\":1}}],[\"动作空间\",{\"1\":{\"86\":1}}],[\"满足f\",{\"1\":{\"131\":1}}],[\"区别于贝尔曼公式\",{\"1\":{\"129\":1}}],[\"选择对应的动作\",{\"1\":{\"157\":1}}],[\"选择移动方向和选择关联用户\",{\"1\":{\"87\":1}}],[\"选取当前状态下最大的\",{\"1\":{\"143\":2}}],[\"选取状态中最大的\",{\"1\":{\"126\":1}}],[\"唯一\",{\"1\":{\"126\":1}}],[\"存在不动点x∗\",{\"1\":{\"131\":1}}],[\"存在\",{\"1\":{\"126\":1}}],[\"存在lemma1\",{\"1\":{\"82\":1}}],[\"贝尔曼最优公式\",{\"0\":{\"125\":1},\"1\":{\"134\":1}}],[\"贝尔曼公式\",{\"0\":{\"109\":1}}],[\"总结\",{\"0\":{\"122\":1}}],[\"总用户的mos取决于无人机的发射功率\",{\"1\":{\"84\":1}}],[\"采取动作\",{\"1\":{\"151\":1}}],[\"采取一个指定的action可以得到的平均return\",{\"1\":{\"121\":1}}],[\"采用梯度下降\",{\"1\":{\"215\":1}}],[\"采用\",{\"1\":{\"152\":1,\"209\":1}}],[\"采用的是the\",{\"1\":{\"87\":1}}],[\"采用q\",{\"1\":{\"86\":1}}],[\"采用基于遗传算法的gak\",{\"1\":{\"86\":1}}],[\"采用mos作为用户qos衡量的标准\",{\"1\":{\"83\":1}}],[\"k→0∑k=1∞​ak​=∞\",{\"1\":{\"171\":1}}],[\"k→∞\",{\"1\":{\"120\":1,\"131\":1,\"177\":1}}],[\"k\",{\"1\":{\"170\":2}}],[\"k−1\",{\"1\":{\"167\":1}}],[\"k=2\",{\"1\":{\"167\":1}}],[\"k=1∑n​rt+k​\",{\"1\":{\"227\":1}}],[\"k=1∑n​rt+k​∣st​=s0​\",{\"1\":{\"227\":1}}],[\"k=1\",{\"1\":{\"135\":1,\"167\":1,\"170\":1}}],[\"kth\",{\"1\":{\"152\":1}}],[\"kn​∩kn\",{\"1\":{\"81\":1,\"84\":1,\"86\":1}}],[\"kn​\",{\"1\":{\"81\":1,\"82\":2}}],[\"然后生成了一个\",{\"1\":{\"227\":1}}],[\"然后我们通过\",{\"1\":{\"215\":1}}],[\"然后在更新了一定次数之后\",{\"1\":{\"215\":1}}],[\"然后进行迭代\",{\"1\":{\"152\":1}}],[\"然后进行不断迭代\",{\"1\":{\"120\":1}}],[\"然后创建子类对象\",{\"1\":{\"65\":1}}],[\"进行优化\",{\"1\":{\"221\":1}}],[\"进行训练\",{\"1\":{\"215\":1}}],[\"进行赋值\",{\"1\":{\"215\":1}}],[\"进行固定求解\",{\"1\":{\"215\":1}}],[\"进行生成数据\",{\"1\":{\"198\":1}}],[\"进行更新即可\",{\"1\":{\"188\":1}}],[\"进行的采样\",{\"1\":{\"157\":1}}],[\"进行估计\",{\"1\":{\"151\":1,\"167\":1}}],[\"进行了无穷多步来进行了真实的求解\",{\"1\":{\"144\":1}}],[\"进行迭代\",{\"1\":{\"143\":1,\"174\":1}}],[\"进行求解\",{\"1\":{\"120\":1,\"139\":1,\"176\":1}}],[\"进行拼接操作\",{\"1\":{\"54\":1}}],[\"整合\",{\"1\":{\"119\":1}}],[\"整个内部类中都处于静态上下文\",{\"1\":{\"62\":1}}],[\"求均值的方法\",{\"0\":{\"167\":1}}],[\"求在策略πk​下所有的\",{\"1\":{\"152\":1}}],[\"求解梯度还是很好求的\",{\"1\":{\"215\":1}}],[\"求解给定策略\",{\"1\":{\"184\":1}}],[\"求解当前策略的\",{\"1\":{\"143\":1}}],[\"求解下一步的vk+1​\",{\"1\":{\"136\":1}}],[\"求解方法\",{\"1\":{\"131\":1}}],[\"求解\",{\"0\":{\"131\":1},\"1\":{\"122\":1}}],[\"求解bellman\",{\"1\":{\"118\":1}}],[\"求出其对应状态的\",{\"1\":{\"120\":1}}],[\"求\",{\"1\":{\"114\":1}}],[\"核心内容\",{\"0\":{\"110\":1}}],[\"核心思想\",{\"1\":{\"98\":1,\"149\":1}}],[\"伪代码\",{\"0\":{\"105\":1,\"137\":1,\"140\":1,\"197\":1,\"217\":1},\"1\":{\"189\":1}}],[\"作为下一步的\",{\"1\":{\"126\":1}}],[\"作为附加功能存在\",{\"1\":{\"28\":1}}],[\"作用\",{\"1\":{\"99\":1}}],[\"证明\",{\"1\":{\"99\":1}}],[\"θt+1​=θt​+α▽θ​lnπ\",{\"1\":{\"229\":2}}],[\"θt+1​=θt​+α▽θ​j\",{\"1\":{\"221\":1}}],[\"θt+1​​=θt​+α▽θ​j\",{\"1\":{\"229\":1}}],[\"θt​\",{\"1\":{\"221\":1,\"229\":3}}],[\"θ∈rm表示参数向量\",{\"1\":{\"221\":1}}],[\"θ\",{\"1\":{\"99\":4,\"221\":2,\"224\":1,\"228\":20,\"229\":2}}],[\"θkn​​\",{\"1\":{\"82\":1}}],[\"算法是用来解决\",{\"1\":{\"192\":1}}],[\"算法是用来求解一个\",{\"1\":{\"186\":1}}],[\"算法如下来进行估计\",{\"1\":{\"189\":1}}],[\"算法同样是来求解\",{\"1\":{\"188\":1}}],[\"算法其目的是用于直接估计\",{\"1\":{\"188\":1}}],[\"算法分析\",{\"0\":{\"186\":1}}],[\"算法所需的数据\",{\"1\":{\"185\":1}}],[\"算法为\",{\"1\":{\"183\":1}}],[\"算法进行求解\",{\"1\":{\"183\":1}}],[\"算法进行求解g\",{\"1\":{\"177\":1}}],[\"算法\",{\"0\":{\"187\":1,\"230\":1},\"1\":{\"177\":2,\"178\":1,\"209\":1}}],[\"算法来进行求解\",{\"1\":{\"172\":1}}],[\"算法就可以用来求解当\",{\"1\":{\"170\":1}}],[\"算法介绍\",{\"0\":{\"170\":1}}],[\"算法流程\",{\"0\":{\"162\":1}}],[\"算法中\",{\"0\":{\"161\":1},\"1\":{\"211\":1}}],[\"算法的统一形式和总结\",{\"0\":{\"200\":1}}],[\"算法的比较\",{\"0\":{\"187\":1}}],[\"算法的问题描述中\",{\"1\":{\"178\":1}}],[\"算法的核心是\",{\"1\":{\"150\":1}}],[\"算法的基础上\",{\"1\":{\"96\":1}}],[\"算法思路\",{\"0\":{\"150\":1}}],[\"算法比较\",{\"0\":{\"143\":1}}],[\"算法描述\",{\"0\":{\"139\":1,\"185\":1}}],[\"算法迭代示意图\",{\"1\":{\"138\":1}}],[\"算法1\",{\"1\":{\"86\":1}}],[\"策略梯度法\",{\"0\":{\"220\":1}}],[\"策略梯度上升\",{\"1\":{\"96\":1}}],[\"策略了\",{\"1\":{\"198\":1}}],[\"策略求解\",{\"1\":{\"143\":2}}],[\"策略提升\",{\"1\":{\"139\":1}}],[\"策略评估\",{\"1\":{\"139\":1}}],[\"策略更新\",{\"1\":{\"102\":1,\"155\":1}}],[\"策略为贪心策略\",{\"1\":{\"86\":1}}],[\"获得reward\",{\"1\":{\"93\":1}}],[\"获取长度\",{\"1\":{\"53\":1}}],[\"获取封装的成员变量\",{\"1\":{\"30\":1}}],[\"获取对象的哈希值\",{\"1\":{\"24\":1}}],[\"获取当前的类型class对象\",{\"1\":{\"24\":1}}],[\"∣\",{\"1\":{\"160\":1}}],[\"∣−1action\",{\"1\":{\"160\":1}}],[\"∣−1\",{\"1\":{\"160\":1}}],[\"∣a\",{\"1\":{\"160\":3}}],[\"∣ϵ​\",{\"1\":{\"160\":2}}],[\"∣∣≤δ\",{\"1\":{\"139\":1}}],[\"∣∣≤γ∣∣x1​−x2​∣∣\",{\"1\":{\"131\":1}}],[\"∣∣vπk+1​\",{\"1\":{\"139\":1}}],[\"∣∣f\",{\"1\":{\"131\":1}}],[\"∣st​=s\",{\"1\":{\"192\":1}}],[\"∣s=s\",{\"1\":{\"186\":2}}],[\"∣s\",{\"1\":{\"93\":1,\"114\":1,\"117\":4,\"118\":3,\"119\":1,\"121\":2,\"128\":1,\"136\":2,\"139\":2,\"148\":1,\"150\":3,\"186\":1,\"188\":1}}],[\"∣kn​∣为第n个簇的用户总数\",{\"1\":{\"87\":1}}],[\"控制其一直待在target\",{\"1\":{\"92\":1}}],[\"控制符\",{\"0\":{\"26\":1}}],[\"通常也称为\",{\"1\":{\"208\":1}}],[\"通常是具有有限步长的trajectory\",{\"1\":{\"92\":1}}],[\"通过梯度的算法来最小化对应的损失函数\",{\"1\":{\"215\":1}}],[\"通过使用一个函数来进行拟合\",{\"1\":{\"204\":1}}],[\"通过采取\",{\"1\":{\"158\":1}}],[\"通过迭代算法求解\",{\"1\":{\"150\":1}}],[\"通过求解\",{\"1\":{\"121\":1}}],[\"通过\",{\"1\":{\"102\":1,\"118\":1,\"120\":1,\"143\":1,\"148\":1,\"155\":1}}],[\"通过k\",{\"1\":{\"86\":1}}],[\"通过gak\",{\"1\":{\"78\":1}}],[\"通过优化无人机的部署和动态移动来解决总用户mos最大化问题\",{\"1\":{\"78\":1}}],[\"通过自动装箱转换的integer对象\",{\"1\":{\"36\":1}}],[\"通过添加abstract关键字\",{\"1\":{\"27\":1}}],[\"更新一定次数后\",{\"1\":{\"215\":1}}],[\"更新策略的步骤就是选择此时\",{\"1\":{\"150\":1}}],[\"更加高效利用\",{\"1\":{\"153\":1}}],[\"更注重长远的reward\",{\"1\":{\"92\":1}}],[\"更远视\",{\"1\":{\"92\":1}}],[\"更多地是考虑多架无人机的二维部署或单架无人机在地面用户保持静止情况下的部署\",{\"1\":{\"77\":1}}],[\"显然是\",{\"1\":{\"223\":1}}],[\"显然是可以通过一个\",{\"1\":{\"139\":1}}],[\"显然我们仍然可以通过\",{\"1\":{\"183\":1}}],[\"显然我们可以将\",{\"1\":{\"177\":1}}],[\"显然ϵ=0\",{\"1\":{\"160\":1}}],[\"显然其核心关键就是在\",{\"1\":{\"150\":1}}],[\"显然在现实运行算法中是无法做到的\",{\"1\":{\"144\":1}}],[\"显然\",{\"1\":{\"92\":1,\"96\":1,\"136\":1,\"144\":1,\"151\":1,\"159\":1,\"169\":1,\"177\":1}}],[\"γa∑​π\",{\"1\":{\"118\":1}}],[\"γ\",{\"1\":{\"112\":1,\"121\":1,\"183\":1}}],[\"γ接近1\",{\"1\":{\"92\":1}}],[\"γ∈\",{\"1\":{\"92\":1}}],[\"γkn​\",{\"1\":{\"84\":1,\"86\":1}}],[\"γkn​​\",{\"1\":{\"82\":1}}],[\"γk0​σ2μlos​pmax​​\",{\"1\":{\"82\":1}}],[\"指导agent在当前状态下选择哪个动作\",{\"1\":{\"92\":1}}],[\"π2\",{\"1\":{\"143\":1}}],[\"π2​=argmaxπ​\",{\"1\":{\"143\":1}}],[\"π1​=argmaxπ​\",{\"1\":{\"143\":2}}],[\"π180​θkn​​−ζ\",{\"1\":{\"82\":1}}],[\"π0​\",{\"1\":{\"143\":1}}],[\"π0​pe​vπ0​​pi​π1​pe​vπ1​​pi​π2​pe​vπ2​​pi​\",{\"1\":{\"138\":1,\"143\":1}}],[\"πk​的\",{\"1\":{\"141\":1}}],[\"πk​\",{\"1\":{\"139\":2,\"151\":1}}],[\"πk+1​=πargmax​\",{\"1\":{\"139\":1,\"143\":2}}],[\"πk+1​=argmaxπ​\",{\"1\":{\"136\":1,\"150\":1}}],[\"πk+1​\",{\"1\":{\"136\":2,\"139\":2,\"141\":1,\"152\":2}}],[\"π∗\",{\"1\":{\"126\":2}}],[\"π\",{\"1\":{\"92\":1,\"93\":1,\"113\":1,\"114\":1,\"117\":1,\"118\":1,\"121\":1,\"126\":1,\"130\":1,\"160\":1,\"184\":1,\"185\":1,\"186\":2,\"208\":1,\"221\":2,\"225\":3,\"228\":1}}],[\"强化学习\",{\"0\":{\"239\":1},\"2\":{\"95\":1,\"108\":1,\"124\":1,\"133\":1,\"147\":1,\"164\":1,\"181\":1,\"202\":1,\"219\":1,\"232\":1}}],[\"强化学习框架图\",{\"0\":{\"91\":1}}],[\"强制类型转换\",{\"1\":{\"28\":1}}],[\"测试阶段\",{\"1\":{\"87\":1}}],[\"训练阶段\",{\"1\":{\"87\":1}}],[\"与策略\",{\"1\":{\"225\":2}}],[\"与\",{\"0\":{\"114\":1,\"143\":1,\"187\":1},\"1\":{\"114\":1,\"141\":1,\"152\":1,\"197\":1}}],[\"与基于q\",{\"1\":{\"87\":1}}],[\"与k\",{\"1\":{\"78\":1}}],[\"基于函数表示的策略\",{\"1\":{\"221\":1}}],[\"基于函数的\",{\"1\":{\"203\":1}}],[\"基于策略\",{\"1\":{\"208\":1}}],[\"基于给定策略下\",{\"1\":{\"207\":1}}],[\"基于表格的\",{\"1\":{\"203\":1}}],[\"基于q\",{\"1\":{\"87\":1}}],[\"基本思路\",{\"0\":{\"221\":1}}],[\"基本形式\",{\"0\":{\"128\":1}}],[\"基本概念\",{\"0\":{\"90\":1,\"92\":1}}],[\"基本设置\",{\"0\":{\"81\":1}}],[\"基本定义是\",{\"1\":{\"63\":1}}],[\"基本上不会用到\",{\"1\":{\"63\":1}}],[\"基本类型\",{\"1\":{\"52\":1}}],[\"基本类型包装类\",{\"0\":{\"35\":1}}],[\"基本类型的比较跟之前一样\",{\"1\":{\"25\":1}}],[\"前\",{\"1\":{\"87\":1}}],[\"前进\",{\"1\":{\"86\":1}}],[\"右\",{\"1\":{\"87\":1}}],[\"右转\",{\"1\":{\"86\":1}}],[\"需考虑用户的移动性\",{\"1\":{\"87\":1}}],[\"需要考虑怎样对状态进行平均\",{\"1\":{\"206\":1}}],[\"需要求解一个优化问题\",{\"1\":{\"174\":1}}],[\"需要从\",{\"1\":{\"150\":1}}],[\"需要初始化猜测的\",{\"1\":{\"143\":1}}],[\"需要初始化策略π0​\",{\"1\":{\"143\":1}}],[\"需要确定几件事\",{\"1\":{\"126\":1}}],[\"需要通过采样解决\",{\"1\":{\"118\":1}}],[\"需要通过关键字\",{\"1\":{\"16\":2}}],[\"需要推导e\",{\"1\":{\"115\":1}}],[\"需要调整相应无人机的高度\",{\"1\":{\"82\":1}}],[\"需要合理选择无人机n的垂直高度hn​\",{\"1\":{\"82\":1}}],[\"需要由子类来完成\",{\"1\":{\"27\":1}}],[\"需要先进行导入才可以\",{\"1\":{\"16\":1}}],[\"需要在类的最上面添加package关键字来指明当前类所处的包\",{\"1\":{\"16\":1}}],[\"∑s∈s​d\",{\"1\":{\"223\":1}}],[\"∑s∈s​dπ​\",{\"1\":{\"208\":1}}],[\"∑k=1∞​ak2​=∞\",{\"1\":{\"171\":1}}],[\"∑k=1∞​ak2​<∞\",{\"1\":{\"171\":1}}],[\"∑k=1∞​ak​=∞\",{\"1\":{\"171\":1}}],[\"∑kn​=1kn​​pkn​​\",{\"1\":{\"84\":1,\"86\":1}}],[\"∑i=1k−1​xi​+xk​\",{\"1\":{\"167\":1}}],[\"∑​pπ​\",{\"1\":{\"119\":1}}],[\"∑​p\",{\"1\":{\"117\":2,\"118\":2,\"121\":1,\"128\":1,\"136\":1,\"139\":2,\"150\":1,\"186\":1}}],[\"∑​vπ​\",{\"1\":{\"117\":2}}],[\"∑​e\",{\"1\":{\"117\":3}}],[\"∑n=1n​∣kn​∣是总用户数\",{\"1\":{\"87\":1}}],[\"设\",{\"1\":{\"208\":1,\"210\":1}}],[\"设置为\",{\"1\":{\"100\":1}}],[\"设无人机总数为n\",{\"1\":{\"87\":1}}],[\"设计一种基于\",{\"1\":{\"78\":2}}],[\"仅用作估计\",{\"1\":{\"154\":1}}],[\"仅需要考虑无人机的7个移动方向即可\",{\"1\":{\"87\":1}}],[\"仅支持接口\",{\"1\":{\"67\":1}}],[\"值函数近似\",{\"0\":{\"203\":1}}],[\"值迭代和策略迭代\",{\"0\":{\"134\":1}}],[\"值的估计通过一个网络来进行描述\",{\"1\":{\"96\":1}}],[\"值\",{\"1\":{\"86\":1}}],[\"值还是可以修改\",{\"1\":{\"48\":1}}],[\"奖励\",{\"1\":{\"86\":1,\"92\":1}}],[\"静止\",{\"1\":{\"86\":1}}],[\"静态内部类编译特性\",{\"0\":{\"64\":1}}],[\"静态内部类由于是静态的\",{\"1\":{\"62\":1}}],[\"静态内部类的类名同样是之前的格式\",{\"1\":{\"62\":1}}],[\"静态内部类就像静态方法和静态变量一样\",{\"1\":{\"62\":1}}],[\"静态内部类\",{\"0\":{\"62\":1}}],[\"静态\",{\"1\":{\"59\":1}}],[\"静态初始化\",{\"1\":{\"44\":1}}],[\"静态导入test方法\",{\"1\":{\"17\":1}}],[\"静态导入\",{\"1\":{\"17\":1}}],[\"静态方法使用\",{\"1\":{\"68\":1}}],[\"静态方法甚至是类指定访问权限\",{\"1\":{\"17\":1}}],[\"静态方法同样是属于类的\",{\"1\":{\"13\":1}}],[\"静态变量\",{\"1\":{\"17\":1}}],[\"静态变量初始化\",{\"0\":{\"14\":1}}],[\"静态变量和静态方法\",{\"0\":{\"13\":1}}],[\"静态的内容\",{\"1\":{\"13\":1}}],[\"下的\",{\"1\":{\"208\":1,\"225\":1}}],[\"下行\",{\"1\":{\"86\":1}}],[\"下划线\",{\"1\":{\"55\":1}}],[\"后\",{\"1\":{\"121\":1,\"155\":2}}],[\"后续可能是未知的\",{\"1\":{\"118\":1}}],[\"后四个方向\",{\"1\":{\"87\":1}}],[\"后退\",{\"1\":{\"86\":1}}],[\"后面加上花括号\",{\"1\":{\"65\":1}}],[\"后面的\",{\"1\":{\"16\":1}}],[\"左转\",{\"1\":{\"86\":1}}],[\"共两步\",{\"1\":{\"205\":1}}],[\"共分为\",{\"1\":{\"136\":1}}],[\"共考虑7个方向\",{\"1\":{\"86\":1}}],[\"共划分5个等级\",{\"1\":{\"83\":1}}],[\"按照所给定策略j来执行一个动作at​∈a从而获得奖励rt​以及下一个状态st+1​\",{\"1\":{\"86\":1}}],[\"个样本\",{\"1\":{\"215\":1}}],[\"个人推导\",{\"1\":{\"117\":1}}],[\"个人感觉是这个\",{\"1\":{\"86\":1}}],[\"个人理解\",{\"1\":{\"86\":1,\"92\":1}}],[\"个\",{\"1\":{\"86\":1}}],[\"个类型的实参\",{\"1\":{\"50\":1}}],[\"×\",{\"1\":{\"86\":2}}],[\"智能体\",{\"1\":{\"86\":1}}],[\"目的是最小化目标函数\",{\"1\":{\"214\":1}}],[\"目标函数梯度求解\",{\"0\":{\"228\":1}}],[\"目标函数定义\",{\"0\":{\"222\":1}}],[\"目标\",{\"1\":{\"205\":1}}],[\"目标是获得无人机的最佳3d位置\",{\"1\":{\"86\":1}}],[\"目前的优化算法只是在估计给定策略的\",{\"1\":{\"209\":1}}],[\"目前各位小伙伴就暂时理解为会返回对象存放的内存地址\",{\"1\":{\"24\":1}}],[\"目前暂时不会用到\",{\"1\":{\"24\":3}}],[\"划分完毕\",{\"1\":{\"86\":1}}],[\"直到设置的收敛条件为止\",{\"1\":{\"139\":1}}],[\"直到各个簇的成员没有太大变化\",{\"1\":{\"86\":1}}],[\"直接估计的是\",{\"1\":{\"193\":1}}],[\"直接通过\",{\"1\":{\"167\":1}}],[\"直接\",{\"1\":{\"155\":1}}],[\"直接作为接口中抽象方法的实现\",{\"1\":{\"68\":2}}],[\"直接创建一个匿名的接口实现类\",{\"1\":{\"65\":1}}],[\"直接在方法中创建局部内部类\",{\"1\":{\"63\":1}}],[\"直接在类中定义变量\",{\"1\":{\"4\":1}}],[\"直接指定值和大小\",{\"1\":{\"44\":1}}],[\"直接将字符串的666\",{\"1\":{\"37\":1}}],[\"直接写每个状态的名字即可\",{\"1\":{\"30\":1}}],[\"直接判断引用的对象是不是teacher类型\",{\"1\":{\"28\":1}}],[\"直接int\",{\"1\":{\"28\":1}}],[\"直接==\",{\"1\":{\"25\":1}}],[\"直接使用integer为我们通过好的求和方法\",{\"1\":{\"68\":1}}],[\"直接使用就可以\",{\"1\":{\"17\":1}}],[\"直接使用即可\",{\"1\":{\"16\":1}}],[\"直接赋值\",{\"1\":{\"9\":1}}],[\"再通过这些经验来不断改进更新另一个策略\",{\"1\":{\"196\":1}}],[\"再次出现\",{\"1\":{\"154\":1}}],[\"再将\",{\"1\":{\"150\":1}}],[\"再将用户划分给距离最近的无人机\",{\"1\":{\"86\":1}}],[\"再求平均值\",{\"1\":{\"114\":1}}],[\"再根据欧几里得距离重新划分\",{\"1\":{\"86\":1}}],[\"再找到新的簇的各中心\",{\"1\":{\"86\":1}}],[\"重新修改为\",{\"1\":{\"119\":1}}],[\"重要性采样\",{\"0\":{\"103\":1},\"1\":{\"102\":1}}],[\"重复步骤\",{\"1\":{\"86\":1}}],[\"重写方法可以添加\",{\"1\":{\"25\":1}}],[\"重写方法要求与父类的定义完全一致\",{\"1\":{\"25\":2}}],[\"解决方案\",{\"0\":{\"85\":1}}],[\"水平位置和高度\",{\"1\":{\"84\":1}}],[\"∀\",{\"1\":{\"189\":1,\"193\":1}}],[\"∀s\",{\"1\":{\"188\":1,\"192\":1}}],[\"∀s=st​\",{\"1\":{\"185\":1}}],[\"∀s∈s​\",{\"1\":{\"128\":1}}],[\"∀s∈s=πmax​a∑​π\",{\"1\":{\"128\":1}}],[\"∀s∈s\",{\"1\":{\"118\":1,\"131\":1}}],[\"∀kn​\",{\"1\":{\"84\":3,\"86\":3}}],[\"∀t\",{\"1\":{\"84\":4,\"86\":4}}],[\"∀n\",{\"1\":{\"84\":2,\"86\":2}}],[\"用来描述所有状态的state\",{\"1\":{\"115\":1}}],[\"用户的速度设为\",{\"1\":{\"87\":1}}],[\"用户漫游模型\",{\"1\":{\"87\":1}}],[\"用户区域划分算法\",{\"1\":{\"86\":1}}],[\"用户关联策略\",{\"1\":{\"86\":1}}],[\"用户是保持静态的\",{\"1\":{\"86\":1}}],[\"用户rkn​​在一段时间ts​内的mos总和为\",{\"1\":{\"83\":1}}],[\"用于解释\",{\"1\":{\"149\":1}}],[\"用于规定给定组件必须要出现多少次才能满足匹配的\",{\"1\":{\"55\":1}}],[\"用于计算超大数字\",{\"1\":{\"38\":1}}],[\"用于指定当前类所处的包的\",{\"1\":{\"16\":1}}],[\"发现在该位置静止是最优的\",{\"1\":{\"86\":1}}],[\"发现对于\",{\"1\":{\"27\":1}}],[\"发送端的时间\",{\"1\":{\"83\":1}}],[\"分析随机变量\",{\"1\":{\"206\":1}}],[\"分析\",{\"0\":{\"175\":1}}],[\"分别设为1\",{\"1\":{\"83\":1}}],[\"分割字符串\",{\"1\":{\"53\":1}}],[\"分割操作\",{\"1\":{\"53\":1}}],[\"取值范围从1−4\",{\"1\":{\"83\":1}}],[\"根据这个式子我们就可以通过\",{\"1\":{\"228\":1}}],[\"根据梯度下降的公式\",{\"1\":{\"215\":1}}],[\"根据给定的策略\",{\"1\":{\"185\":1}}],[\"根据策略\",{\"1\":{\"151\":1}}],[\"根据策略π\",{\"1\":{\"121\":1}}],[\"根据对应的\",{\"1\":{\"139\":1}}],[\"根据\",{\"1\":{\"126\":2,\"135\":1,\"136\":1,\"139\":1,\"141\":1,\"186\":1}}],[\"根据一个\",{\"1\":{\"115\":1}}],[\"根据所给定的用户划分情况\",{\"1\":{\"86\":1}}],[\"根据遗传算法找到cn​个最优个体作为簇的中心\",{\"1\":{\"86\":1}}],[\"根据n个用户\",{\"1\":{\"86\":1}}],[\"根据mos数值\",{\"1\":{\"83\":1}}],[\"根据香农定理\",{\"1\":{\"82\":1}}],[\"≤c2​\",{\"1\":{\"171\":1}}],[\"≤pmax​\",{\"1\":{\"84\":1,\"86\":1}}],[\"≤hmax​\",{\"1\":{\"84\":1,\"86\":1}}],[\"≤hn​\",{\"1\":{\"82\":1}}],[\"≤\",{\"1\":{\"82\":1}}],[\"ζ2​是系数\",{\"1\":{\"83\":1}}],[\"ζ1​\",{\"1\":{\"83\":1}}],[\"ζ+em\",{\"1\":{\"82\":1}}],[\"ζ是由环境决定的常数\",{\"1\":{\"82\":1}}],[\"信道容量c=b∗log\",{\"1\":{\"82\":1}}],[\"信号模型\",{\"0\":{\"82\":1}}],[\"由无人机的位置和它们在最后时隙采取的动作决定\",{\"1\":{\"87\":1}}],[\"由用户的初始位置和运动模型决定\",{\"1\":{\"87\":1}}],[\"由此进行迭代\",{\"1\":{\"139\":1}}],[\"由此可以根据contraction\",{\"1\":{\"131\":1}}],[\"由此可以推导出一个多步的trajectory\",{\"1\":{\"112\":1}}],[\"由此\",{\"1\":{\"82\":1}}],[\"由于πk+1​是\",{\"1\":{\"136\":1}}],[\"由于\",{\"1\":{\"121\":1,\"136\":1}}],[\"由于用户在每个时隙都处于漫游状态\",{\"1\":{\"87\":1}}],[\"由于gak\",{\"1\":{\"86\":1}}],[\"由于特定用户的mos与该用户与无人机之间的距离有关\",{\"1\":{\"86\":1}}],[\"由于不同用户对于传输速率的需求是不同的\",{\"1\":{\"83\":1}}],[\"由于不同集群的频谱不同\",{\"1\":{\"82\":1}}],[\"由于string的构造方法在创建对象时也会得到一个string类型的结果\",{\"1\":{\"68\":1}}],[\"由于基本数据类型和引用类型不同\",{\"1\":{\"47\":1}}],[\"由于类本身都是继承\",{\"1\":{\"28\":1}}],[\"由于默认导入了系统自带的string类\",{\"1\":{\"16\":1}}],[\"且需要被求解出来\",{\"1\":{\"169\":1}}],[\"且最终会收敛到最优策略v∗\",{\"1\":{\"141\":1}}],[\"且\",{\"1\":{\"126\":1,\"160\":1,\"171\":2,\"208\":1,\"217\":1}}],[\"且ζ1​+ζ2​=1\",{\"1\":{\"83\":1}}],[\"且传输率永远都不可能超过信道容量c\",{\"1\":{\"82\":1}}],[\"且无人机向关联用户的发射功率是恒定的\",{\"1\":{\"82\":1}}],[\"且同样不能使用\",{\"1\":{\"27\":1}}],[\"μnlos​\",{\"1\":{\"82\":1}}],[\"μnlos​是表示los和nlos链路的衰减因子\",{\"1\":{\"82\":1}}],[\"μlos​−μnlos​\",{\"1\":{\"82\":1}}],[\"μlos​\",{\"1\":{\"82\":1}}],[\"常数\",{\"1\":{\"82\":1}}],[\"α是表示路径损耗指数\",{\"1\":{\"82\":1}}],[\"​≐a∈a∑​π\",{\"1\":{\"226\":1}}],[\"​≐g\",{\"1\":{\"172\":1}}],[\"​q^​\",{\"1\":{\"215\":3}}],[\"​tderrorδt​​\",{\"1\":{\"185\":1}}],[\"​forthegreedyaction\",{\"1\":{\"160\":1}}],[\"​vu​u2​pu​\",{\"1\":{\"143\":1}}],[\"​vu​u1​pu​π2\",{\"1\":{\"143\":1}}],[\"​p\",{\"1\":{\"121\":1}}],[\"​​−αt​\",{\"1\":{\"185\":1}}],[\"​​=currentestimatevt​\",{\"1\":{\"185\":1}}],[\"​​=a∑​qπ​\",{\"1\":{\"121\":1}}],[\"​​+η∇w​f\",{\"1\":{\"177\":1}}],[\"​​π\",{\"1\":{\"121\":1}}],[\"​​\",{\"1\":{\"118\":1,\"139\":1,\"177\":2,\"185\":1}}],[\"​=π\",{\"1\":{\"228\":1}}],[\"​=πmax​a∑​π\",{\"1\":{\"128\":1}}],[\"​=▽w​e\",{\"1\":{\"209\":1}}],[\"​=qt​\",{\"1\":{\"189\":1,\"193\":1}}],[\"​=vt​\",{\"1\":{\"185\":1}}],[\"​=∇w​f\",{\"1\":{\"177\":1}}],[\"​=w−e\",{\"1\":{\"183\":1}}],[\"​=w−x​=w−x+e\",{\"1\":{\"172\":1}}],[\"​=wk​−k1​\",{\"1\":{\"167\":1}}],[\"​=argmaxπ​\",{\"1\":{\"143\":1}}],[\"​=a∑​π\",{\"1\":{\"116\":1,\"117\":1}}],[\"​=rπk​​+γpπk​​vπk​\",{\"1\":{\"141\":1}}],[\"​=s∈s∑​d\",{\"1\":{\"228\":1}}],[\"​=s∈s∑​η\",{\"1\":{\"228\":1}}],[\"​=s\",{\"1\":{\"117\":1}}],[\"​=e\",{\"1\":{\"115\":1,\"118\":1}}],[\"​=es∼η\",{\"1\":{\"99\":1}}],[\"​=ϕ\",{\"1\":{\"81\":1,\"84\":1,\"86\":1}}],[\"​ifmosnew​>mosold​ifmosnew​=mosold​ifmosnew​<mosold​​\",{\"1\":{\"86\":1}}],[\"​≥0\",{\"1\":{\"84\":1,\"86\":1}}],[\"​≥γkn​​\",{\"1\":{\"84\":1,\"86\":1}}],[\"​−μlos​−μnlos​μnlos​​​s\",{\"1\":{\"82\":1}}],[\"​\",{\"1\":{\"82\":3,\"83\":1,\"84\":1,\"86\":1,\"99\":1,\"115\":1,\"117\":2,\"118\":1,\"119\":1,\"121\":2,\"128\":1,\"136\":1,\"139\":5,\"141\":1,\"150\":1,\"154\":2,\"160\":1,\"167\":1,\"177\":2,\"185\":1,\"189\":1,\"193\":1,\"209\":1,\"215\":1,\"226\":1,\"228\":2,\"229\":1}}],[\"​hn​\",{\"1\":{\"82\":1}}],[\"−2\",{\"1\":{\"215\":2}}],[\"−q^​\",{\"1\":{\"212\":1,\"213\":1,\"214\":1,\"215\":3}}],[\"−v^\",{\"1\":{\"206\":1,\"207\":2,\"208\":2,\"209\":4,\"211\":1}}],[\"−vπk​\",{\"1\":{\"139\":1}}],[\"−tdtargetvt​ˉ​\",{\"1\":{\"185\":1}}],[\"−αt​\",{\"1\":{\"185\":1,\"189\":1,\"193\":1}}],[\"−\",{\"1\":{\"183\":1,\"185\":1,\"189\":1,\"193\":1}}],[\"−x\",{\"1\":{\"172\":1}}],[\"−xkn​​\",{\"1\":{\"81\":1}}],[\"−e\",{\"1\":{\"172\":1,\"177\":2}}],[\"−f\",{\"1\":{\"131\":1}}],[\"−b\",{\"1\":{\"99\":1}}],[\"−0\",{\"1\":{\"86\":1}}],[\"−1rπk​​\",{\"1\":{\"141\":1}}],[\"−1rπ​\",{\"1\":{\"120\":1}}],[\"−1rπ​​\",{\"1\":{\"120\":1}}],[\"−1\",{\"1\":{\"82\":1,\"83\":2,\"86\":4}}],[\"−ykn​​\",{\"1\":{\"81\":1}}],[\"yt​\",{\"1\":{\"215\":1}}],[\"yt​≐r+γmaxa∈a\",{\"1\":{\"215\":1}}],[\"yt​−q^​\",{\"1\":{\"215\":2}}],[\"y≐r+γmaxa∈a\",{\"1\":{\"215\":1}}],[\"yuser​\",{\"1\":{\"87\":2}}],[\"yuav​\",{\"1\":{\"86\":2,\"87\":2}}],[\"yd​+1\",{\"1\":{\"86\":1}}],[\"yd​\",{\"1\":{\"86\":1}}],[\"yn​\",{\"1\":{\"81\":2}}],[\"ykn​​\",{\"1\":{\"81\":1}}],[\"∈\",{\"1\":{\"81\":1}}],[\"飞行速度恒定\",{\"1\":{\"81\":1}}],[\"xi​\",{\"1\":{\"174\":2}}],[\"xk+1​=f\",{\"1\":{\"131\":1}}],[\"xk​→x∗\",{\"1\":{\"131\":1}}],[\"xk​\",{\"1\":{\"131\":2,\"174\":1,\"177\":1,\"183\":1}}],[\"xkn​​\",{\"1\":{\"81\":1}}],[\"x∗\",{\"1\":{\"131\":1}}],[\"x=f\",{\"1\":{\"131\":1}}],[\"x2​\",{\"1\":{\"131\":1,\"148\":1}}],[\"x1​\",{\"1\":{\"131\":1}}],[\"x\",{\"1\":{\"131\":2,\"148\":1,\"167\":1,\"172\":10,\"174\":6,\"177\":17,\"183\":9}}],[\"xuser​\",{\"1\":{\"87\":2}}],[\"xuav​\",{\"1\":{\"86\":1,\"87\":2}}],[\"xt​=⎩⎨⎧​1\",{\"1\":{\"86\":1}}],[\"xd​+1\",{\"1\":{\"86\":1}}],[\"xd​\",{\"1\":{\"86\":1}}],[\"xn​\",{\"1\":{\"81\":2,\"148\":1}}],[\"xxxxx\",{\"1\":{\"51\":1}}],[\"考虑能否仅用一次\",{\"1\":{\"174\":1}}],[\"考虑一个复杂的均值估计问题\",{\"1\":{\"183\":1}}],[\"考虑一个\",{\"1\":{\"154\":1}}],[\"考虑\",{\"1\":{\"143\":2}}],[\"考虑用户在每个时隙移动的情况\",{\"1\":{\"87\":1}}],[\"考虑以下场景\",{\"1\":{\"86\":1}}],[\"考虑无人机辅助无线网络的下行链路传输\",{\"1\":{\"81\":1}}],[\"考虑qoe\",{\"1\":{\"77\":1}}],[\"系统结构\",{\"0\":{\"80\":1}}],[\"为一个函数方程\",{\"1\":{\"169\":1}}],[\"为状态\",{\"1\":{\"160\":1}}],[\"为什么不去求\",{\"1\":{\"152\":1}}],[\"为什么考虑\",{\"1\":{\"148\":1}}],[\"为什么这个迭代算法最终可以找到最优策略\",{\"1\":{\"141\":1}}],[\"为discounted\",{\"1\":{\"112\":1}}],[\"为t时刻的mos评分\",{\"1\":{\"83\":1}}],[\"为\",{\"1\":{\"82\":1,\"115\":2,\"119\":1,\"151\":1,\"221\":1}}],[\"为指标\",{\"1\":{\"78\":1}}],[\"为了让右边取到最大值的情况\",{\"1\":{\"130\":1}}],[\"为了进行\",{\"1\":{\"120\":1}}],[\"为了应对具有无限步的trajectory的return=∞的情况\",{\"1\":{\"92\":1}}],[\"为了保证所有用户都能连接到网络\",{\"1\":{\"82\":1}}],[\"为了满足不同用户传输速率要求\",{\"1\":{\"82\":1}}],[\"为了在los信道概率和路径损耗之间取得平衡\",{\"1\":{\"82\":1}}],[\"为了在创建匿名对象时进行属性初始化\",{\"1\":{\"65\":1}}],[\"为了优化效率\",{\"1\":{\"53\":1}}],[\"为了支持小数加法\",{\"1\":{\"8\":1}}],[\"以向用户提供可靠的服务\",{\"1\":{\"82\":1}}],[\"以\",{\"1\":{\"78\":1}}],[\"以及是否可以到最后优化的成果\",{\"1\":{\"174\":1}}],[\"以及\",{\"1\":{\"55\":2,\"142\":1,\"161\":1,\"228\":1}}],[\"以及object的方法\",{\"1\":{\"28\":1}}],[\"过去研究主要考虑的是2d部署\",{\"1\":{\"77\":1}}],[\"过去研究大多没有基于用户的移动\",{\"1\":{\"77\":1}}],[\"q^​\",{\"1\":{\"215\":5}}],[\"qt​\",{\"1\":{\"189\":2,\"193\":1,\"229\":1}}],[\"qt+1​\",{\"1\":{\"189\":2,\"193\":2}}],[\"qπk​​\",{\"1\":{\"139\":1,\"150\":2,\"151\":2,\"152\":1}}],[\"qπ​\",{\"1\":{\"99\":2,\"121\":3,\"122\":1,\"148\":1,\"188\":1,\"228\":8,\"229\":4}}],[\"qac\",{\"0\":{\"97\":1},\"1\":{\"98\":1}}],[\"q\",{\"0\":{\"193\":1,\"197\":1,\"213\":1,\"214\":1},\"1\":{\"78\":2,\"84\":1,\"86\":2,\"87\":1,\"96\":1,\"122\":1,\"128\":1,\"130\":1,\"131\":1,\"150\":1,\"152\":1,\"154\":2,\"192\":2,\"193\":1,\"196\":1,\"197\":1,\"203\":1,\"214\":1}}],[\"qoe\",{\"1\":{\"77\":1}}],[\"quality\",{\"0\":{\"83\":1},\"1\":{\"77\":1}}],[\"主要框架\",{\"1\":{\"91\":1}}],[\"主要内容\",{\"0\":{\"79\":1}}],[\"主要贡献\",{\"0\":{\"78\":1}}],[\"主要动机\",{\"0\":{\"77\":1}}],[\"主方法的string参数是java\",{\"1\":{\"16\":1}}],[\"工具类\",{\"0\":{\"71\":1}}],[\"工人\",{\"1\":{\"27\":1}}],[\"异常\",{\"0\":{\"71\":1,\"73\":1}}],[\"反正只要是符合接口中方法的定义的\",{\"1\":{\"68\":1}}],[\"卡布奇诺今犹在\",{\"1\":{\"68\":1}}],[\"待实现的求和方法\",{\"1\":{\"68\":1}}],[\"返回值一样\",{\"1\":{\"68\":1}}],[\"返回值类型\",{\"1\":{\"5\":1}}],[\"该算法中\",{\"1\":{\"195\":1,\"196\":1}}],[\"该算法是\",{\"1\":{\"142\":1}}],[\"该方法成立的数学依据是\",{\"1\":{\"148\":1}}],[\"该步骤是根据\",{\"1\":{\"139\":1}}],[\"该步骤是用来计算当前策略\",{\"1\":{\"139\":1}}],[\"该式子针对状态空间中的所有状态均成立\",{\"1\":{\"118\":1}}],[\"该论文中在精度和模型复杂型上作出平衡\",{\"1\":{\"86\":1}}],[\"该问题依然是np\",{\"1\":{\"86\":1}}],[\"该优化问题是一个non\",{\"1\":{\"84\":1}}],[\"该文中不同集群所利用的频谱是不同的\",{\"1\":{\"82\":1}}],[\"该文中表示为\",{\"1\":{\"82\":1}}],[\"该文提出的算法具较快的收敛性\",{\"1\":{\"78\":1}}],[\"该文基于q\",{\"1\":{\"78\":1}}],[\"该文设计的是3d部署\",{\"1\":{\"77\":1}}],[\"该框架将无人机部署在三维空间内\",{\"1\":{\"78\":1}}],[\"该咋写咋写\",{\"1\":{\"67\":1}}],[\"该怎么处理\",{\"1\":{\"38\":1}}],[\"今天学会了\",{\"1\":{\"67\":5}}],[\"今年\",{\"1\":{\"5\":1}}],[\"代码语句\",{\"1\":{\"67\":1}}],[\"代码块中的内容会在对象创建时仅执行一次\",{\"1\":{\"9\":1}}],[\"代码块同样会在对象构造之前进行\",{\"1\":{\"9\":1}}],[\"标准格式为\",{\"1\":{\"67\":1}}],[\"标记为native的方法是本地方法\",{\"1\":{\"24\":1}}],[\"标记为公共的内容\",{\"1\":{\"17\":1}}],[\"标记为受保护的内容可以能被类本身和同包中的其他类访问\",{\"1\":{\"17\":1}}],[\"标记为私有的内容无法被除当前类以外的任何位置访问\",{\"1\":{\"17\":1}}],[\"普通的类也可以\",{\"1\":{\"65\":1}}],[\"匿名类只能访问其父类的属性或方法\",{\"1\":{\"65\":1}}],[\"匿名对象通常不能直接给属性赋值\",{\"1\":{\"65\":1}}],[\"匿名对象的类是在创建时匿名生成的\",{\"1\":{\"65\":1}}],[\"匿名对象本身不能定义新的属性\",{\"1\":{\"65\":1}}],[\"匿名内部类特性\",{\"0\":{\"66\":1}}],[\"匿名内部类中同样可以使用类中的属性\",{\"1\":{\"65\":1}}],[\"匿名内部类是我们使用频率非常高的一种内部类\",{\"1\":{\"65\":1}}],[\"匿名内部类\",{\"0\":{\"65\":1},\"1\":{\"66\":1}}],[\"把未实现的方法实现了\",{\"1\":{\"65\":1}}],[\"把对象的属性和方法结合成一个独立的整体\",{\"1\":{\"21\":1}}],[\"先根据当前策略计算出各个状态的\",{\"1\":{\"150\":1}}],[\"先实现未实现的方法\",{\"1\":{\"65\":1}}],[\"先转换为当前类型\",{\"1\":{\"25\":1}}],[\"才进行更新\",{\"1\":{\"155\":1}}],[\"才会初始化外部类\",{\"1\":{\"64\":1}}],[\"才能进行\",{\"1\":{\"155\":1}}],[\"才能进行比较\",{\"1\":{\"25\":1}}],[\"才能去\",{\"1\":{\"61\":1}}],[\"说明这种情况下\",{\"1\":{\"64\":1}}],[\"局部内部类直接使用类名就行\",{\"1\":{\"63\":1}}],[\"局部内部类名\",{\"1\":{\"63\":1}}],[\"局部内部类就像局部变量一样\",{\"1\":{\"63\":1}}],[\"局部内部类\",{\"0\":{\"63\":1}}],[\"依赖于当前状态和所采取的动作\",{\"1\":{\"92\":1}}],[\"依附任何对象我们可以直接创建使用\",{\"1\":{\"62\":1}}],[\"依附于b创建的对象\",{\"1\":{\"61\":1}}],[\"依附于a创建的对象\",{\"1\":{\"61\":1}}],[\"依然是就近原则\",{\"1\":{\"61\":1}}],[\"套娃了属于是\",{\"1\":{\"61\":1}}],[\"外\",{\"1\":{\"128\":1}}],[\"外部类初始化\",{\"1\":{\"64\":2}}],[\"外部类父类的tostring方法\",{\"1\":{\"61\":1}}],[\"外部类的tosrting方法\",{\"1\":{\"61\":1}}],[\"外部是无法访问到这个内部类的\",{\"1\":{\"61\":1}}],[\"外层\",{\"1\":{\"61\":1}}],[\"甚至连名字都没有\",{\"1\":{\"65\":1}}],[\"甚至还可以继续套娃一个成员内部类\",{\"1\":{\"61\":1}}],[\"甚至都不是一个类了\",{\"1\":{\"28\":1}}],[\"甚至都不是一个类\",{\"1\":{\"28\":1}}],[\"属于\",{\"0\":{\"61\":1,\"62\":1},\"1\":{\"59\":2}}],[\"属性默认就是这个值\",{\"1\":{\"9\":1}}],[\"9\",{\"1\":{\"55\":1}}],[\"数量和位置\",{\"1\":{\"84\":1}}],[\"数据包从发送端\",{\"1\":{\"83\":1}}],[\"数字\",{\"1\":{\"55\":1}}],[\"数组同样支持向上转型\",{\"1\":{\"47\":1}}],[\"数组的\",{\"1\":{\"45\":1}}],[\"数组大小\",{\"1\":{\"44\":2}}],[\"数组类型比较特殊\",{\"1\":{\"44\":1}}],[\"数组可以代表任何相同类型的一组内容\",{\"1\":{\"43\":1}}],[\"数组是相同类型数据的有序集合\",{\"1\":{\"43\":1}}],[\"数组\",{\"0\":{\"41\":1,\"43\":1}}],[\"非空白符\",{\"1\":{\"55\":1}}],[\"^\",{\"1\":{\"55\":1}}],[\"^aeiou\",{\"1\":{\"55\":1}}],[\"^abc\",{\"1\":{\"55\":1}}],[\"相应的算法是\",{\"1\":{\"185\":1}}],[\"相应的\",{\"1\":{\"177\":1,\"183\":1}}],[\"相等于\",{\"1\":{\"55\":1}}],[\"相当于\",{\"1\":{\"28\":1}}],[\"字母的所有字母\",{\"1\":{\"55\":1}}],[\"字母\",{\"1\":{\"55\":1}}],[\"字符\",{\"1\":{\"55\":2}}],[\"字符数组转字符串\",{\"1\":{\"53\":1}}],[\"字符数组和字符串之间是可以快速进行相互转换的字符串转字符数组\",{\"1\":{\"53\":1}}],[\"字符串支持使用\",{\"1\":{\"54\":1}}],[\"字符串类中提供了很多方便我们操作的方法\",{\"1\":{\"53\":1}}],[\"字符串类是一个比较特殊的类\",{\"1\":{\"52\":1}}],[\"字符串的内容比较\",{\"1\":{\"53\":1}}],[\"字符串中的字符一旦确定\",{\"1\":{\"52\":1}}],[\"字符串\",{\"0\":{\"41\":1,\"52\":1}}],[\"字符串转integer有多个方法\",{\"1\":{\"37\":1}}],[\"字符串内容的比较\",{\"1\":{\"25\":1}}],[\"uses\",{\"1\":{\"155\":1}}],[\"users\",{\"1\":{\"77\":2}}],[\"u0​pu​π1\",{\"1\":{\"143\":1}}],[\"uniform\",{\"0\":{\"207\":1}}],[\"uniqueness\",{\"1\":{\"131\":1}}],[\"unmanned\",{\"1\":{\"77\":1}}],[\"update\",{\"1\":{\"96\":1,\"136\":3,\"143\":2}}],[\"uavn\",{\"1\":{\"86\":1}}],[\"uavs\",{\"1\":{\"77\":1,\"78\":1}}],[\"uav\",{\"0\":{\"76\":1,\"237\":1},\"1\":{\"77\":1},\"2\":{\"89\":1}}],[\"u\",{\"1\":{\"55\":2}}],[\"g∣s=s\",{\"1\":{\"186\":1}}],[\"g~​\",{\"1\":{\"177\":1,\"183\":1}}],[\"gd\",{\"1\":{\"174\":1,\"177\":1}}],[\"gradient\",{\"0\":{\"173\":1,\"220\":1},\"1\":{\"174\":2,\"221\":1}}],[\"greedy\",{\"0\":{\"158\":1,\"160\":1,\"161\":1},\"1\":{\"126\":1,\"136\":2,\"139\":1,\"143\":2,\"152\":1,\"160\":3}}],[\"g​\",{\"1\":{\"170\":2,\"172\":2}}],[\"gpi\",{\"1\":{\"155\":1}}],[\"generalized\",{\"1\":{\"155\":1}}],[\"get\",{\"1\":{\"121\":2}}],[\"getstatus\",{\"1\":{\"30\":2}}],[\"getclass\",{\"1\":{\"24\":2}}],[\"getname\",{\"1\":{\"7\":1,\"24\":1,\"30\":2}}],[\"g\",{\"1\":{\"151\":3,\"169\":4,\"170\":2,\"171\":1,\"172\":2,\"177\":1,\"183\":2,\"186\":1}}],[\"gt+1​∣st+1​=s\",{\"1\":{\"117\":2}}],[\"gt+1​∣st​=s\",{\"1\":{\"115\":2,\"117\":5,\"118\":1}}],[\"gt​−v^\",{\"1\":{\"210\":1}}],[\"gt​​=rt+1​+γrt+2​+γ2rt+3​+\",{\"1\":{\"115\":1}}],[\"gt​∣st​=s\",{\"1\":{\"113\":1,\"115\":1,\"121\":3,\"122\":2,\"148\":2,\"150\":1,\"151\":1}}],[\"gt​\",{\"1\":{\"113\":2,\"115\":1,\"151\":1,\"210\":2}}],[\"gt​也是一个随机变量\",{\"1\":{\"112\":1}}],[\"gt​=rt+1​+γrt+2​+γ2rt+3​+\",{\"1\":{\"112\":1}}],[\"gt​=e\",{\"1\":{\"86\":1}}],[\"good\",{\"1\":{\"83\":1}}],[\"google\",{\"1\":{\"55\":2}}],[\"gkn​​\",{\"1\":{\"82\":1}}],[\"gain\",{\"1\":{\"82\":1}}],[\"请注意在逗号和两个数之间不能有空格\",{\"1\":{\"55\":1}}],[\"则只是进行一次带入求解\",{\"1\":{\"143\":1}}],[\"则是对多个\",{\"1\":{\"114\":1}}],[\"则总动作空间的大小为7+2n∑n=1n​∣kn​∣会导致动作空间过大\",{\"1\":{\"87\":1}}],[\"则用户的关联动作数为2n∑n=1n​∣kn​∣\",{\"1\":{\"87\":1}}],[\"则无人机将获得正奖励\",{\"1\":{\"86\":1}}],[\"则等价于\",{\"1\":{\"55\":1}}],[\"则表示只能赋一次值\",{\"1\":{\"26\":1}}],[\"至少匹配n\",{\"1\":{\"55\":1}}],[\"次的观测值\",{\"1\":{\"170\":1}}],[\"次方程根的估计\",{\"1\":{\"170\":1}}],[\"次且最多匹配\",{\"1\":{\"55\":1}}],[\"次\",{\"1\":{\"55\":4}}],[\"z0\",{\"1\":{\"55\":1}}],[\"za\",{\"1\":{\"55\":1}}],[\"z\",{\"1\":{\"55\":5}}],[\"zo+\",{\"1\":{\"55\":1}}],[\"zoo\",{\"1\":{\"55\":2}}],[\"zo\",{\"1\":{\"55\":2}}],[\"能匹配\",{\"1\":{\"55\":2}}],[\"例如\",{\"1\":{\"55\":8}}],[\"匹配字母\",{\"1\":{\"55\":1}}],[\"匹配字符串\",{\"1\":{\"55\":2}}],[\"匹配所有\",{\"1\":{\"55\":1}}],[\"匹配所有大写字母\",{\"1\":{\"55\":1}}],[\"匹配除换行符\",{\"1\":{\"55\":1}}],[\"匹配除了\",{\"1\":{\"55\":1}}],[\"匹配\",{\"1\":{\"55\":1}}],[\"匹配确定的\",{\"1\":{\"55\":1}}],[\"匹配前面的子表达式零次或一次\",{\"1\":{\"55\":1}}],[\"匹配前面的子表达式零次或多次\",{\"1\":{\"55\":1}}],[\"匹配前面的子表达式一次或多次\",{\"1\":{\"55\":1}}],[\"匹配成功返回true\",{\"1\":{\"55\":1}}],[\"描述如下\",{\"1\":{\"86\":1}}],[\"描述\",{\"1\":{\"55\":2}}],[\"描述了一种字符串匹配的模式\",{\"1\":{\"55\":1}}],[\"限定符表如下\",{\"1\":{\"55\":1}}],[\"行不行\",{\"1\":{\"54\":2}}],[\"做滴\",{\"1\":{\"54\":2}}],[\"做题我并不擅长\",{\"1\":{\"25\":1,\"27\":1}}],[\"汉堡\",{\"1\":{\"54\":2}}],[\"你看\",{\"1\":{\"54\":2}}],[\"删除2到4这个范围内的字符\",{\"1\":{\"54\":1}}],[\"内嵌迭代算法求解\",{\"1\":{\"143\":1}}],[\"内容\",{\"1\":{\"63\":1}}],[\"内部类静态方法\",{\"1\":{\"64\":2}}],[\"内部类初始化\",{\"1\":{\"64\":2}}],[\"内部类父类的tostring方法\",{\"1\":{\"61\":1}}],[\"内部类自己的tostring方法\",{\"1\":{\"61\":1}}],[\"内部类名称\",{\"1\":{\"61\":1}}],[\"内部类也是类\",{\"1\":{\"61\":1}}],[\"内部类顾名思义\",{\"1\":{\"60\":1}}],[\"内部类\",{\"0\":{\"58\":1,\"60\":1}}],[\"内部什么都没有\",{\"1\":{\"54\":1}}],[\"内存也会消耗更多\",{\"1\":{\"36\":1}}],[\"弥补了字符串不能修改的不足\",{\"1\":{\"54\":1}}],[\"裁剪等操作\",{\"1\":{\"54\":1}}],[\"添加点内容的话\",{\"1\":{\"51\":1}}],[\"函数的\",{\"0\":{\"51\":1}}],[\"哥们在这跟你说唱\",{\"1\":{\"50\":1}}],[\"遍历打印数组中每一个元素\",{\"1\":{\"50\":1}}],[\"参数一样\",{\"1\":{\"68\":1}}],[\"参数直接写成lambda表达式\",{\"1\":{\"67\":1}}],[\"参数名称\",{\"1\":{\"50\":1,\"67\":1}}],[\"参数类型\",{\"1\":{\"50\":1,\"67\":1}}],[\"参数类型可能会多种多样\",{\"1\":{\"8\":1}}],[\"6746\",{\"1\":{\"83\":1}}],[\"6\",{\"1\":{\"49\":1}}],[\"666\",{\"1\":{\"37\":1}}],[\"5个变量连续加\",{\"1\":{\"54\":1}}],[\"5\",{\"0\":{\"122\":1,\"200\":1,\"214\":1},\"1\":{\"49\":1,\"50\":1,\"83\":3,\"143\":1}}],[\"5555\",{\"1\":{\"37\":1}}],[\"答案是可以的\",{\"1\":{\"49\":1}}],[\"7\",{\"1\":{\"48\":1}}],[\"8\",{\"1\":{\"48\":1}}],[\"允许\",{\"1\":{\"48\":1}}],[\"允许在任何地方被访问\",{\"1\":{\"17\":1}}],[\"404\",{\"1\":{\"233\":1}}],[\"4\",{\"0\":{\"72\":1,\"106\":1,\"119\":1,\"121\":1,\"157\":1,\"172\":1,\"179\":1,\"192\":1,\"193\":1,\"194\":1,\"197\":1,\"213\":1,\"229\":1},\"1\":{\"48\":3,\"49\":1,\"54\":1,\"83\":1,\"143\":1}}],[\"性质\",{\"0\":{\"48\":1}}],[\"性别呢\",{\"1\":{\"9\":1}}],[\"性别\",{\"1\":{\"4\":1}}],[\"特性\",{\"0\":{\"47\":1}}],[\"特殊包装类\",{\"0\":{\"38\":1}}],[\"每次都是从对应的状态\",{\"1\":{\"157\":1}}],[\"每次迭代都会使得策略进行提升\",{\"1\":{\"141\":1}}],[\"每次无人机会根据当前状态st​∈s\",{\"1\":{\"86\":1}}],[\"每次运算都会生成一个新的对象\",{\"1\":{\"54\":1}}],[\"每架无人机的带宽和发射功率都均匀分配给每个用户\",{\"1\":{\"86\":1}}],[\"每个集群中无人机的最优位置也会发生变化\",{\"1\":{\"87\":1}}],[\"每个用户的移动方向均匀分布在左\",{\"1\":{\"87\":1}}],[\"每个用户都需要判断是否与每个无人机关联\",{\"1\":{\"87\":1}}],[\"每个用户只能属于一个集群\",{\"1\":{\"81\":1}}],[\"每个用双引号括起来的字符串\",{\"1\":{\"53\":1}}],[\"每个对象中都有一个单独的类定义\",{\"1\":{\"61\":1}}],[\"每个对象都有这样的一个类定义\",{\"1\":{\"61\":1}}],[\"每个对象都有一个自己的空间\",{\"1\":{\"4\":1}}],[\"每个类可以创建一个对象\",{\"1\":{\"61\":1}}],[\"每一轮循环\",{\"1\":{\"46\":1}}],[\"<∞\",{\"1\":{\"171\":1}}],[\"<\",{\"1\":{\"46\":1}}],[\"<=\",{\"1\":{\"36\":1,\"55\":1}}],[\"之前介绍的方法都是\",{\"1\":{\"220\":1}}],[\"之前就是直接使用的\",{\"1\":{\"16\":1}}],[\"之后进行迭代\",{\"1\":{\"143\":1}}],[\"之间存在什么关系\",{\"1\":{\"141\":1}}],[\"之外的任何单个字符\",{\"1\":{\"55\":1}}],[\"之外\",{\"1\":{\"45\":1}}],[\"除了需要求解\",{\"1\":{\"128\":1}}],[\"除了clone\",{\"1\":{\"45\":1}}],[\"除非我们手动重载一个无参构造\",{\"1\":{\"9\":1}}],[\"很遗憾\",{\"1\":{\"45\":1}}],[\"长度是在一开始创建数组的时候就确定好的\",{\"1\":{\"45\":1}}],[\"支持c语言样式\",{\"1\":{\"44\":1}}],[\"跟随一个\",{\"1\":{\"227\":1}}],[\"跟随策略π\",{\"1\":{\"154\":1}}],[\"跟对象成员变量的默认值是一样的\",{\"1\":{\"44\":1}}],[\"跟普通的类一样\",{\"1\":{\"28\":1}}],[\"跟普通方法是一样的\",{\"1\":{\"9\":1}}],[\"或者\",{\"1\":{\"46\":1,\"66\":1,\"96\":1,\"139\":1,\"204\":1,\"229\":1}}],[\"或者是false\",{\"1\":{\"44\":1}}],[\"或者为静态变量赋值\",{\"1\":{\"14\":1}}],[\"肯定是继承自object的\",{\"1\":{\"44\":1}}],[\"底层c++写的\",{\"1\":{\"44\":1}}],[\"底层是由c++实现的\",{\"1\":{\"24\":1}}],[\"它会在\",{\"1\":{\"155\":1}}],[\"它是局部内部类的简化版\",{\"1\":{\"65\":1}}],[\"它就像一个字符串编辑器\",{\"1\":{\"54\":1}}],[\"它用于保存字符串\",{\"1\":{\"52\":1}}],[\"它本身也是类\",{\"1\":{\"44\":1}}],[\"它的属性没有进行赋值\",{\"1\":{\"4\":1}}],[\"面向对象高级篇\",{\"0\":{\"42\":1,\"59\":1,\"72\":1}}],[\"面向对象高级篇1\",{\"0\":{\"34\":1}}],[\"|\",{\"0\":{\"41\":2,\"71\":1,\"194\":1}}],[\"精确到小数点后100位\",{\"1\":{\"38\":1}}],[\"计算\",{\"1\":{\"183\":1}}],[\"计算10\",{\"1\":{\"38\":1}}],[\"计算的话也只能通过\",{\"1\":{\"38\":1}}],[\"3d部署和移动问题\",{\"1\":{\"78\":1}}],[\"3d位置进行优化处理\",{\"1\":{\"78\":1}}],[\"3的结果\",{\"1\":{\"38\":1}}],[\"3\",{\"0\":{\"59\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"105\":2,\"114\":1,\"118\":1,\"120\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":1,\"152\":1,\"156\":1,\"158\":1,\"159\":1,\"160\":1,\"161\":2,\"162\":2,\"171\":1,\"173\":1,\"174\":1,\"175\":1,\"178\":2,\"179\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":2,\"197\":1,\"212\":1,\"228\":1},\"1\":{\"38\":1,\"48\":1,\"49\":1,\"53\":1,\"55\":1,\"135\":2,\"143\":1,\"167\":1,\"170\":1,\"174\":1}}],[\"浮点类型精度有限\",{\"1\":{\"38\":1}}],[\"mini\",{\"1\":{\"215\":2}}],[\"minisize\",{\"1\":{\"209\":1}}],[\"minimize\",{\"1\":{\"177\":1}}],[\"mbgd\",{\"0\":{\"179\":1}}],[\"metrics\",{\"1\":{\"221\":2}}],[\"method\",{\"1\":{\"154\":2,\"174\":3,\"229\":1}}],[\"methods\",{\"1\":{\"154\":1}}],[\"means\",{\"1\":{\"165\":1}}],[\"means来划分各个无人机所管理的用户簇\",{\"1\":{\"86\":1}}],[\"means的优化目标是最小化无人机与对应集群用户的欧氏距离\",{\"1\":{\"86\":1}}],[\"means可以视为获得无人机部署的低复杂度方案\",{\"1\":{\"86\":1}}],[\"means算法\",{\"1\":{\"86\":1}}],[\"means和igk算法比具有较低的复杂度\",{\"1\":{\"78\":1}}],[\"mean算法获得初始单元划分\",{\"1\":{\"78\":1}}],[\"mean\",{\"0\":{\"116\":1,\"117\":1,\"172\":1,\"176\":1},\"1\":{\"78\":1,\"113\":1,\"114\":1,\"148\":1,\"150\":1,\"155\":1,\"165\":1}}],[\"mc\",{\"0\":{\"149\":1,\"153\":1,\"156\":1,\"158\":1,\"161\":1,\"187\":1},\"1\":{\"149\":1,\"153\":2,\"154\":1,\"155\":1,\"161\":2,\"174\":1,\"196\":1}}],[\"mdp就变为mp\",{\"1\":{\"93\":1}}],[\"mdp\",{\"0\":{\"93\":1}}],[\"mssrkn​​rtt​+1\",{\"1\":{\"83\":1}}],[\"mss\",{\"1\":{\"83\":1}}],[\"monro\",{\"1\":{\"171\":2}}],[\"monto\",{\"0\":{\"168\":1}}],[\"monte\",{\"0\":{\"148\":1,\"210\":1},\"1\":{\"148\":1,\"229\":1}}],[\"moreover\",{\"1\":{\"131\":1}}],[\"mobility\",{\"1\":{\"87\":1}}],[\"modles可选择\",{\"1\":{\"87\":1}}],[\"model|environment\",{\"1\":{\"118\":1}}],[\"model\",{\"0\":{\"83\":1,\"148\":1},\"1\":{\"87\":3,\"118\":2,\"149\":1,\"150\":2}}],[\"mos主要是有关传输速率rkn​​的函数\",{\"1\":{\"86\":1}}],[\"mosrkn​​​=t=0∑ts​​moskn​​\",{\"1\":{\"83\":1}}],[\"moskn​​\",{\"1\":{\"83\":3}}],[\"mos\",{\"1\":{\"78\":1}}],[\"movement\",{\"0\":{\"76\":1},\"1\":{\"77\":2}}],[\"multiple\",{\"0\":{\"76\":1},\"1\":{\"77\":1}}],[\"multiply\",{\"1\":{\"38\":1}}],[\"m\",{\"1\":{\"55\":3,\"82\":1}}],[\"mapping\",{\"1\":{\"126\":2,\"131\":5,\"135\":1}}],[\"markov\",{\"0\":{\"93\":1},\"1\":{\"93\":2,\"208\":1}}],[\"markovian\",{\"1\":{\"87\":1}}],[\"may\",{\"1\":{\"92\":1}}],[\"matrix\",{\"0\":{\"119\":1},\"1\":{\"119\":1,\"122\":1,\"128\":1}}],[\"matches\",{\"1\":{\"55\":2}}],[\"matches方法用于对给定正则表达式进行匹配\",{\"1\":{\"55\":1}}],[\"match\",{\"1\":{\"55\":1}}],[\"math\",{\"1\":{\"38\":3}}],[\"maxa∈a\",{\"1\":{\"215\":1}}],[\"max​q^​\",{\"1\":{\"213\":1,\"214\":1,\"215\":3}}],[\"max\",{\"1\":{\"38\":1}}],[\"male\",{\"1\":{\"28\":1}}],[\"main\",{\"1\":{\"4\":3,\"5\":1,\"13\":1,\"16\":4,\"17\":2,\"28\":3,\"29\":1,\"30\":1,\"36\":4,\"37\":1,\"38\":2,\"44\":1,\"45\":1,\"46\":1,\"47\":2,\"48\":1,\"49\":1,\"50\":1,\"51\":2,\"53\":3,\"54\":3,\"55\":2,\"61\":2,\"62\":1,\"64\":2,\"65\":5,\"66\":1,\"67\":3,\"68\":9,\"215\":4,\"217\":1}}],[\"乘法\",{\"1\":{\"38\":1}}],[\"轻轻松松\",{\"1\":{\"38\":1}}],[\"提高泛化能力\",{\"1\":{\"204\":1}}],[\"提升采样的效率\",{\"1\":{\"98\":1}}],[\"提升clone方法的访问权限\",{\"1\":{\"29\":1}}],[\"提出解决总用户mos最大化问题的三步骤\",{\"1\":{\"78\":1}}],[\"提出了一个理想的由qoe驱动的多无人机协助通信框架\",{\"1\":{\"78\":1}}],[\"提供了一种特殊的语法\",{\"1\":{\"65\":1}}],[\"提供的方法进行计算\",{\"1\":{\"38\":1}}],[\"没错\",{\"1\":{\"68\":1}}],[\"没啥意义\",{\"1\":{\"38\":1}}],[\"没有方法体\",{\"1\":{\"27\":1}}],[\"没有返回值\",{\"1\":{\"5\":1}}],[\"和动作\",{\"1\":{\"121\":1}}],[\"和e\",{\"1\":{\"115\":1}}],[\"和匿名内部类不同\",{\"1\":{\"67\":1}}],[\"和\",{\"1\":{\"38\":1,\"54\":1,\"55\":1,\"121\":1,\"155\":1,\"160\":1,\"174\":1,\"178\":1,\"195\":1,\"196\":1,\"215\":2}}],[\"得到含有噪音的观测值序列\",{\"1\":{\"170\":1}}],[\"得到的\",{\"1\":{\"157\":1}}],[\"得到的会是同一个对象\",{\"1\":{\"36\":1}}],[\"得到\",{\"1\":{\"141\":1}}],[\"得到一个样本序列x1​\",{\"1\":{\"148\":1}}],[\"得到一个字符串数组\",{\"1\":{\"53\":1}}],[\"得到一个克隆的对象\",{\"1\":{\"29\":1}}],[\"虽然是继承于\",{\"1\":{\"45\":1}}],[\"虽然我们目前还没有学习数组\",{\"1\":{\"36\":1}}],[\"虽然a和b的值相同\",{\"1\":{\"36\":1}}],[\"虽然java语言是一个面向对象的语言\",{\"1\":{\"35\":1}}],[\"既然是在方法中声明的类\",{\"1\":{\"63\":1}}],[\"既然数组可以是任何类型的\",{\"1\":{\"49\":1}}],[\"既然能装箱\",{\"1\":{\"36\":1}}],[\"既然都有\",{\"1\":{\"28\":1}}],[\"运用到\",{\"1\":{\"102\":1}}],[\"运用了封装的思想\",{\"1\":{\"36\":1}}],[\"运算符\",{\"1\":{\"5\":1}}],[\"覆盖原有构造方法\",{\"1\":{\"30\":1}}],[\"枚举的成员变量\",{\"1\":{\"30\":1}}],[\"枚举类型是普通的类\",{\"1\":{\"30\":1}}],[\"枚举类型使用起来就非常方便了\",{\"1\":{\"30\":1}}],[\"枚举类的语法稍微有一些不一样\",{\"1\":{\"30\":1}}],[\"枚举类\",{\"0\":{\"30\":1}}],[\"本文目的是优化无人机在每个时隙的位置\",{\"1\":{\"84\":1}}],[\"本质就是调用的构造方法\",{\"1\":{\"30\":1}}],[\"本地方法不是我们se中需要学习的内容\",{\"1\":{\"24\":1}}],[\"睡觉\",{\"1\":{\"30\":2}}],[\"睡觉这三个之中的其中一种\",{\"1\":{\"30\":1}}],[\"vˉπ0​\",{\"1\":{\"225\":1,\"228\":1}}],[\"vˉπ​=es∼d​\",{\"1\":{\"223\":1}}],[\"vˉπ​=s∈s∑​d\",{\"1\":{\"223\":1}}],[\"vˉπ​\",{\"1\":{\"223\":1,\"225\":1,\"228\":1}}],[\"v^\",{\"1\":{\"204\":1}}],[\"vt​\",{\"1\":{\"185\":3}}],[\"vt+1​\",{\"1\":{\"185\":2}}],[\"visit\",{\"1\":{\"154\":4,\"157\":2}}],[\"vu\",{\"1\":{\"143\":1}}],[\"v0​\",{\"1\":{\"143\":2}}],[\"vπ1​​\",{\"1\":{\"143\":1}}],[\"vπ1​​=rπ1​​+γpπ1​​vπ1​​\",{\"1\":{\"143\":1}}],[\"vπ0​​是通过迭代算法来求的\",{\"1\":{\"143\":1}}],[\"vπ0​​=rπ0​​+γpπ0​​vπ0​​\",{\"1\":{\"143\":1}}],[\"vπ0​​≤vπ1​​≤vπ2​​⋯≤vπk​​≤⋯≤v∗\",{\"1\":{\"141\":1}}],[\"vπk​​=\",{\"1\":{\"141\":1}}],[\"vπk​​=rπk​​+γpπk​​vπk​​policyimprovement\",{\"1\":{\"150\":1}}],[\"vπk​​=rπk​​+γpπk​​vπk​​\",{\"1\":{\"139\":1,\"143\":1}}],[\"vπk​​\",{\"1\":{\"139\":1,\"141\":1,\"150\":1}}],[\"vπk​\",{\"1\":{\"139\":2,\"141\":1}}],[\"vπ​=\",{\"1\":{\"119\":1,\"120\":1}}],[\"vπ​=rπ​+γpπ​vπ​​\",{\"1\":{\"119\":1}}],[\"vπ​\",{\"1\":{\"100\":1,\"113\":2,\"115\":1,\"117\":1,\"118\":3,\"119\":4,\"121\":5,\"122\":1,\"128\":2,\"130\":1,\"148\":1,\"185\":1,\"186\":4,\"206\":1,\"207\":2,\"208\":2,\"209\":5,\"210\":1,\"211\":1,\"223\":2}}],[\"v=f\",{\"1\":{\"131\":1,\"134\":1}}],[\"v=πmax​\",{\"1\":{\"128\":1}}],[\"vk​\",{\"1\":{\"135\":1}}],[\"vk​→vπ​=\",{\"1\":{\"120\":1}}],[\"vk+1​=rπk+1​​+γpπk+1​​vk​\",{\"1\":{\"143\":1}}],[\"vk+1​=rπk+1​​+γpπk+1​​vk​这里的vk​并不是\",{\"1\":{\"136\":1}}],[\"vk+1​=rπ​+γpπ​vk​​\",{\"1\":{\"120\":1}}],[\"vk+1​=f\",{\"1\":{\"135\":1}}],[\"v2​\",{\"1\":{\"120\":1}}],[\"v1​=rπ1​​+γpπ1​​v0​\",{\"1\":{\"143\":1}}],[\"v1​\",{\"1\":{\"120\":1}}],[\"vector\",{\"0\":{\"119\":1},\"1\":{\"119\":1,\"122\":1,\"128\":1}}],[\"very\",{\"1\":{\"83\":1}}],[\"vehicles\",{\"1\":{\"77\":1}}],[\"vehicular\",{\"1\":{\"76\":1},\"2\":{\"89\":1}}],[\"v\",{\"1\":{\"38\":1,\"131\":5,\"134\":1,\"136\":2,\"183\":1}}],[\"value后\",{\"1\":{\"150\":1}}],[\"value的关系\",{\"1\":{\"115\":1}}],[\"value\",{\"0\":{\"111\":1,\"113\":1,\"114\":1,\"120\":1,\"121\":1,\"131\":1,\"135\":1,\"143\":1,\"184\":1,\"188\":1,\"192\":1,\"203\":1,\"205\":1,\"223\":1,\"226\":1},\"1\":{\"38\":1,\"96\":1,\"110\":1,\"113\":5,\"114\":3,\"115\":1,\"118\":1,\"120\":1,\"121\":10,\"122\":4,\"125\":1,\"126\":1,\"128\":1,\"129\":1,\"130\":2,\"135\":1,\"136\":3,\"139\":3,\"141\":2,\"142\":1,\"143\":12,\"144\":2,\"148\":2,\"150\":6,\"152\":6,\"154\":3,\"155\":2,\"157\":1,\"165\":2,\"184\":1,\"186\":2,\"188\":2,\"189\":1,\"192\":2,\"193\":1,\"203\":1,\"220\":1,\"221\":1,\"223\":1}}],[\"valueof\",{\"1\":{\"30\":1,\"36\":1,\"37\":1,\"38\":4}}],[\"values\",{\"1\":{\"30\":1,\"204\":2}}],[\"void类\",{\"1\":{\"38\":1}}],[\"void\",{\"1\":{\"4\":3,\"5\":2,\"7\":2,\"13\":2,\"16\":2,\"17\":2,\"24\":7,\"25\":1,\"27\":2,\"28\":7,\"29\":2,\"30\":2,\"36\":4,\"37\":1,\"38\":3,\"44\":1,\"45\":1,\"46\":1,\"47\":2,\"48\":1,\"49\":1,\"50\":4,\"51\":1,\"53\":3,\"54\":3,\"55\":2,\"61\":6,\"62\":2,\"63\":2,\"64\":3,\"65\":8,\"66\":2,\"67\":4,\"68\":5}}],[\"学习\",{\"1\":{\"30\":2}}],[\"学生\",{\"1\":{\"28\":1,\"29\":1}}],[\"学生和老师来说\",{\"1\":{\"28\":1}}],[\"状态s时可以得到的平均reward\",{\"1\":{\"226\":1}}],[\"状态转换\",{\"1\":{\"92\":1}}],[\"状态转换模型\",{\"1\":{\"86\":1}}],[\"状态除了要考虑无人机的3d位置外\",{\"1\":{\"87\":1}}],[\"状态空间\",{\"1\":{\"86\":1,\"92\":1}}],[\"状态\",{\"1\":{\"30\":1,\"86\":1,\"92\":1,\"208\":1,\"215\":1}}],[\"最优的策略\",{\"1\":{\"221\":1}}],[\"最优策略\",{\"0\":{\"130\":1}}],[\"最优策略是\",{\"1\":{\"126\":1}}],[\"最优策略是否唯一\",{\"1\":{\"126\":1}}],[\"最优策略是否存在\",{\"1\":{\"126\":1}}],[\"最优策略的定义\",{\"1\":{\"126\":1}}],[\"最接近真实的vπ​\",{\"1\":{\"205\":1}}],[\"最大的\",{\"1\":{\"150\":1,\"152\":1}}],[\"最大的q\",{\"1\":{\"86\":1}}],[\"最简单的示例算法\",{\"1\":{\"149\":1}}],[\"最终会达到一个平稳的状态\",{\"1\":{\"208\":1}}],[\"最终优化的策略\",{\"1\":{\"194\":1}}],[\"最终可以证明\",{\"1\":{\"120\":1}}],[\"最终输出结果\",{\"1\":{\"86\":1}}],[\"最终输出的结果\",{\"1\":{\"86\":1}}],[\"最好的\",{\"0\":{\"100\":1}}],[\"最好是一个域名的格式\",{\"1\":{\"16\":1}}],[\"最近的是参数\",{\"1\":{\"61\":1}}],[\"最少匹配\",{\"1\":{\"55\":1}}],[\"最后面分号可以不打\",{\"1\":{\"30\":1}}],[\"rˉπ​\",{\"1\":{\"228\":1}}],[\"rˉπ​​\",{\"1\":{\"227\":1}}],[\"rˉπ​=s∈s∑​dπ​\",{\"1\":{\"226\":1}}],[\"r1​\",{\"1\":{\"185\":1}}],[\"rk​+γv\",{\"1\":{\"183\":1}}],[\"rkn​​mss​\",{\"1\":{\"83\":1}}],[\"rkn​​\",{\"1\":{\"83\":3}}],[\"r→r\",{\"1\":{\"169\":1}}],[\"rm\",{\"0\":{\"168\":1},\"1\":{\"170\":1,\"172\":1,\"177\":3,\"183\":2}}],[\"robbins\",{\"0\":{\"168\":1},\"1\":{\"171\":2}}],[\"root\",{\"1\":{\"165\":1,\"171\":1,\"177\":1,\"183\":1}}],[\"roundingmode是舍入模式\",{\"1\":{\"38\":1}}],[\"roundingmode\",{\"1\":{\"38\":1}}],[\"r+s\",{\"1\":{\"139\":2}}],[\"r+γa∈a\",{\"1\":{\"214\":1,\"215\":3}}],[\"r+γqπ​\",{\"1\":{\"188\":1}}],[\"r+γg∣s=s\",{\"1\":{\"186\":1}}],[\"r+γv\",{\"1\":{\"183\":6}}],[\"r+γ∑s\",{\"1\":{\"121\":1}}],[\"r+γs\",{\"1\":{\"118\":1,\"121\":1,\"128\":1,\"136\":1}}],[\"rπ​+γpπ​v1​\",{\"1\":{\"143\":1}}],[\"rπ​+γpπ​vπ1​​\",{\"1\":{\"143\":1}}],[\"rπ​+γpπ​vπ0​​\",{\"1\":{\"143\":2}}],[\"rπ​+γpπ​vπk​​\",{\"1\":{\"139\":1,\"143\":1,\"150\":1}}],[\"rπ​+γpπ​vk​\",{\"1\":{\"135\":1,\"136\":1,\"143\":1}}],[\"rπ​+γpπ​v\",{\"1\":{\"128\":1,\"131\":1,\"134\":1}}],[\"rπ​\",{\"1\":{\"119\":2,\"226\":4,\"227\":1}}],[\"rπ​=\",{\"1\":{\"119\":1}}],[\"r​​+mean\",{\"1\":{\"118\":1}}],[\"r​\",{\"1\":{\"116\":1}}],[\"r∑​p\",{\"1\":{\"116\":1,\"118\":2,\"121\":1,\"128\":1,\"136\":1,\"139\":2}}],[\"rl9\",{\"0\":{\"220\":1}}],[\"rl8\",{\"0\":{\"203\":1}}],[\"rl7\",{\"0\":{\"182\":1}}],[\"rl\",{\"1\":{\"165\":1}}],[\"rl6\",{\"0\":{\"165\":1}}],[\"rl5\",{\"0\":{\"148\":1}}],[\"rl4\",{\"0\":{\"134\":1}}],[\"rl3\",{\"0\":{\"125\":1}}],[\"rl2\",{\"0\":{\"109\":1}}],[\"rl10\",{\"0\":{\"96\":1}}],[\"rl1\",{\"0\":{\"90\":1}}],[\"r∣s\",{\"1\":{\"93\":1,\"114\":1,\"116\":1,\"118\":3,\"121\":2,\"128\":1,\"136\":2,\"139\":2,\"148\":1,\"150\":3,\"226\":2}}],[\"rate\",{\"1\":{\"92\":1,\"112\":1,\"131\":1}}],[\"random\",{\"1\":{\"87\":2,\"115\":1}}],[\"r=1∣s1​\",{\"1\":{\"92\":1}}],[\"rt+2​\",{\"1\":{\"227\":1}}],[\"rt+2​+γrt+3​+\",{\"1\":{\"115\":1}}],[\"rt+1​+rt+2​+⋯+rt+n​∣st​=s0​\",{\"1\":{\"227\":1}}],[\"rt+1​+γq^​\",{\"1\":{\"212\":1}}],[\"rt+1​+γqt​\",{\"1\":{\"189\":1}}],[\"rt+1​+γv^\",{\"1\":{\"211\":2}}],[\"rt+1​+γvt​\",{\"1\":{\"185\":2}}],[\"rt+1​+γa∈a\",{\"1\":{\"213\":1}}],[\"rt+1​+γa∈amax​qt​\",{\"1\":{\"193\":1}}],[\"rt+1​+γamax​q\",{\"1\":{\"192\":1}}],[\"rt+1​+γgt+1​∣st​=s\",{\"1\":{\"115\":1}}],[\"rt+1​\",{\"1\":{\"185\":2,\"189\":1,\"227\":1}}],[\"rt+1​∣st​=s\",{\"1\":{\"115\":2,\"116\":2,\"118\":1}}],[\"rt+1​∣at+1​\",{\"1\":{\"93\":1}}],[\"rt​∣st​\",{\"1\":{\"86\":1}}],[\"rtt\",{\"1\":{\"83\":1}}],[\"r\",{\"1\":{\"55\":2,\"93\":3,\"183\":3,\"214\":1,\"215\":1,\"226\":3}}],[\"run\",{\"1\":{\"208\":1}}],[\"runoob\",{\"1\":{\"55\":3}}],[\"running\",{\"1\":{\"30\":5}}],[\"reinforce\",{\"0\":{\"229\":1,\"230\":1},\"1\":{\"229\":1}}],[\"reinforcement\",{\"0\":{\"76\":1}}],[\"replay\",{\"0\":{\"216\":1},\"1\":{\"215\":1}}],[\"representation\",{\"1\":{\"203\":2}}],[\"refers\",{\"1\":{\"165\":1}}],[\"respect\",{\"1\":{\"92\":1}}],[\"resulting\",{\"1\":{\"92\":1}}],[\"result\",{\"1\":{\"54\":2}}],[\"rewards\",{\"0\":{\"116\":1,\"117\":1},\"1\":{\"93\":1,\"118\":2,\"227\":1}}],[\"reward\",{\"1\":{\"86\":1,\"92\":2,\"93\":2,\"188\":1,\"227\":1}}],[\"regexp\",{\"1\":{\"55\":2}}],[\"regular\",{\"1\":{\"55\":1}}],[\"registernatives\",{\"1\":{\"24\":2}}],[\"returns\",{\"1\":{\"114\":1}}],[\"return为\",{\"1\":{\"112\":1}}],[\"return的描述\",{\"1\":{\"112\":1}}],[\"return越短视\",{\"1\":{\"92\":1}}],[\"return\",{\"0\":{\"114\":1,\"226\":1},\"1\":{\"7\":1,\"8\":2,\"24\":2,\"25\":3,\"29\":1,\"30\":2,\"36\":2,\"67\":2,\"68\":1,\"92\":2,\"112\":1,\"114\":3,\"115\":1,\"121\":2,\"151\":1,\"155\":1,\"186\":1,\"210\":1}}],[\"还可以估计q\",{\"1\":{\"154\":1}}],[\"还可以定义一些方法来描述同一类的行为\",{\"1\":{\"5\":1}}],[\"还需要求解最优策略π\",{\"1\":{\"128\":1}}],[\"还需要考虑所有用户的2d位置\",{\"1\":{\"87\":1}}],[\"还是\",{\"1\":{\"126\":1,\"148\":1,\"165\":1}}],[\"还是同一张q\",{\"1\":{\"86\":1}}],[\"还与los的概率有关\",{\"1\":{\"86\":1}}],[\"还没学异常\",{\"1\":{\"29\":1}}],[\"全部拷贝为一个新的对象\",{\"1\":{\"29\":1}}],[\"无关\",{\"1\":{\"225\":1}}],[\"无论是\",{\"1\":{\"165\":1}}],[\"无论是基本类型还是引用类型\",{\"1\":{\"29\":1}}],[\"无记忆性\",{\"1\":{\"117\":1}}],[\"无人机需要进行移动\",{\"1\":{\"87\":1}}],[\"无人机的动态移动设计\",{\"0\":{\"87\":1}}],[\"无人机的位置初始化也是随机部署的\",{\"1\":{\"86\":1}}],[\"无人机的3d部署\",{\"0\":{\"86\":1}}],[\"无人机3d部署算法\",{\"1\":{\"86\":1}}],[\"无人机\",{\"1\":{\"86\":1}}],[\"无人机n以可变高度悬停在用户上方\",{\"1\":{\"86\":1}}],[\"无人机n的高度需满足\",{\"1\":{\"82\":1}}],[\"无人机n与用户kn​在时间t的距离表示为\",{\"1\":{\"81\":1}}],[\"无人机往往有更高的los链接概率\",{\"1\":{\"82\":1}}],[\"无参构造方法被覆盖\",{\"1\":{\"30\":1}}],[\"无法显式定义构造函数或初始化块\",{\"1\":{\"65\":1}}],[\"无法直接定义新的属性\",{\"1\":{\"65\":1}}],[\"无法进行修改\",{\"1\":{\"52\":1}}],[\"无法表示一个非常大的数\",{\"1\":{\"38\":1}}],[\"无法使用this关键字\",{\"1\":{\"13\":1}}],[\"无法获取成员变量的值\",{\"1\":{\"13\":1}}],[\"深拷贝会将引用类型的所有内容\",{\"1\":{\"29\":1}}],[\"深拷贝\",{\"1\":{\"29\":1}}],[\"拷贝个基莫\",{\"1\":{\"29\":1}}],[\"浅拷贝\",{\"1\":{\"29\":1}}],[\"克隆实现\",{\"1\":{\"29\":1}}],[\"克隆操作可以完全复制一个对象的所有属性\",{\"1\":{\"29\":1}}],[\"克隆出来的与原来的对象不是一个对象\",{\"1\":{\"29\":1}}],[\"克隆方法\",{\"0\":{\"29\":1}}],[\"克隆当前对象\",{\"1\":{\"24\":1}}],[\"从这章开始时基于\",{\"1\":{\"220\":1}}],[\"从\",{\"1\":{\"177\":1}}],[\"从给定的\",{\"1\":{\"155\":1}}],[\"从状态\",{\"1\":{\"151\":1,\"210\":1}}],[\"从状态st​到st+1​\",{\"1\":{\"86\":1}}],[\"从指定的\",{\"1\":{\"151\":1}}],[\"从此可以发现\",{\"1\":{\"150\":1}}],[\"从而可以进行近似求解\",{\"1\":{\"228\":1}}],[\"从而可以在\",{\"1\":{\"188\":1}}],[\"从而方便计算\",{\"1\":{\"215\":1}}],[\"从而使得算法可行\",{\"1\":{\"209\":1}}],[\"从而生成经验数据的策略\",{\"1\":{\"194\":1}}],[\"从而引入\",{\"1\":{\"178\":1}}],[\"从而转换为一个\",{\"1\":{\"177\":1}}],[\"从而进行多次利用\",{\"1\":{\"154\":1}}],[\"从而选择每个状态下最大的\",{\"1\":{\"150\":1}}],[\"从而减小方差\",{\"1\":{\"98\":1}}],[\"从而最大化所有用户的总mos值\",{\"1\":{\"84\":1}}],[\"从无人机n到用户kn​的信道功率增益\",{\"1\":{\"82\":1}}],[\"从java8开始\",{\"1\":{\"28\":1}}],[\"从已知的一个类中派生出一个新的类\",{\"1\":{\"21\":1}}],[\"2种方法\",{\"1\":{\"122\":1}}],[\"2l−1\",{\"1\":{\"83\":1}}],[\"2mssfs​+1\",{\"1\":{\"83\":1}}],[\"2mss\",{\"1\":{\"83\":1}}],[\"2~3\",{\"1\":{\"83\":1}}],[\"2​\",{\"1\":{\"81\":1}}],[\"2+\",{\"1\":{\"81\":1}}],[\"2019\",{\"1\":{\"76\":1}}],[\"20\",{\"1\":{\"66\":1,\"68\":2}}],[\"20240826181712\",{\"1\":{\"230\":1}}],[\"20240826181638\",{\"1\":{\"229\":1}}],[\"20240826181538\",{\"1\":{\"229\":1}}],[\"20240826181340\",{\"1\":{\"229\":1}}],[\"20240826180244\",{\"1\":{\"228\":1}}],[\"20240826173749\",{\"1\":{\"224\":1}}],[\"20240820231205\",{\"1\":{\"217\":1}}],[\"20240820231024\",{\"1\":{\"217\":1}}],[\"20240820230944\",{\"1\":{\"216\":1}}],[\"20240820230920\",{\"1\":{\"216\":1}}],[\"20240820230827\",{\"1\":{\"216\":1}}],[\"20240820184405\",{\"1\":{\"213\":1}}],[\"20240820184127\",{\"1\":{\"212\":1}}],[\"20240820181718\",{\"1\":{\"208\":1}}],[\"20240820181406\",{\"1\":{\"208\":1}}],[\"20240818182231\",{\"1\":{\"200\":1}}],[\"20240818182301\",{\"1\":{\"200\":1}}],[\"20240818182057\",{\"1\":{\"198\":1}}],[\"20240818181917\",{\"1\":{\"199\":1}}],[\"20240817000409\",{\"1\":{\"191\":1}}],[\"20240817000331\",{\"1\":{\"191\":1}}],[\"20240817000642\",{\"1\":{\"190\":1}}],[\"20240817000601\",{\"1\":{\"190\":1}}],[\"20240817000500\",{\"1\":{\"190\":1}}],[\"20240817000230\",{\"1\":{\"189\":1}}],[\"20240817000134\",{\"1\":{\"189\":1}}],[\"20240817000114\",{\"1\":{\"189\":1}}],[\"20240814230747\",{\"1\":{\"179\":1}}],[\"20240814014058\",{\"1\":{\"176\":1}}],[\"20240812010538\",{\"1\":{\"162\":1}}],[\"20240812011140\",{\"1\":{\"161\":1}}],[\"20240812004534\",{\"1\":{\"156\":1}}],[\"20240811233346\",{\"1\":{\"152\":1}}],[\"20240811011334\",{\"1\":{\"145\":1}}],[\"20240811010933\",{\"1\":{\"144\":1}}],[\"20240811002219\",{\"1\":{\"140\":1}}],[\"20240810190018\",{\"1\":{\"137\":1}}],[\"20240815234719\",{\"1\":{\"16\":1}}],[\"20240830200624\",{\"1\":{\"106\":1}}],[\"20240830200608\",{\"1\":{\"106\":1}}],[\"20240830200406\",{\"1\":{\"105\":1}}],[\"20240830200343\",{\"1\":{\"104\":1}}],[\"20240830200320\",{\"1\":{\"104\":1}}],[\"20240830200305\",{\"1\":{\"104\":1}}],[\"20240830200248\",{\"1\":{\"104\":1}}],[\"20240830200138\",{\"1\":{\"103\":1}}],[\"20240830200118\",{\"1\":{\"103\":1}}],[\"20240830200056\",{\"1\":{\"103\":1}}],[\"20240830185629\",{\"1\":{\"101\":1}}],[\"20240830185556\",{\"1\":{\"101\":1}}],[\"20240830185537\",{\"1\":{\"101\":1}}],[\"20240830185324\",{\"1\":{\"100\":1}}],[\"20240830185127\",{\"1\":{\"99\":1}}],[\"20240830184424\",{\"1\":{\"97\":1}}],[\"20240830184330\",{\"1\":{\"97\":1}}],[\"20240830184312\",{\"1\":{\"96\":1}}],[\"20240830184236\",{\"1\":{\"96\":1}}],[\"20241027015718\",{\"1\":{\"64\":1}}],[\"20241027015244\",{\"1\":{\"64\":1}}],[\"20241027012950\",{\"1\":{\"61\":1}}],[\"20241017002218\",{\"1\":{\"36\":1}}],[\"2\",{\"0\":{\"42\":1,\"93\":1,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"104\":1,\"113\":1,\"115\":1,\"116\":1,\"117\":2,\"118\":1,\"119\":1,\"127\":1,\"128\":1,\"129\":2,\"130\":2,\"131\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":2,\"141\":1,\"144\":1,\"151\":1,\"153\":1,\"154\":1,\"155\":2,\"156\":1,\"157\":1,\"160\":1,\"168\":1,\"169\":1,\"170\":2,\"171\":1,\"172\":1,\"175\":1,\"184\":1,\"185\":1,\"186\":2,\"187\":1,\"190\":1,\"194\":1,\"205\":1,\"206\":1,\"209\":2,\"222\":1,\"223\":1,\"226\":2},\"1\":{\"48\":2,\"49\":2,\"54\":1,\"55\":2,\"81\":1,\"82\":1,\"86\":1,\"87\":1,\"135\":1,\"136\":1,\"139\":1,\"141\":1,\"143\":1,\"152\":2,\"167\":1,\"170\":1,\"174\":1,\"185\":1,\"189\":1,\"206\":1,\"207\":2,\"208\":2,\"209\":2,\"214\":1,\"215\":2}}],[\"27\",{\"1\":{\"28\":1}}],[\"都需要有多个\",{\"1\":{\"157\":1}}],[\"都记录\",{\"1\":{\"154\":1}}],[\"都有可选择的动作\",{\"1\":{\"92\":1}}],[\"都有学习的能力\",{\"1\":{\"28\":1}}],[\"都可以直接进行方法引用\",{\"1\":{\"68\":1}}],[\"都是最大的\",{\"1\":{\"221\":1}}],[\"都是string类型的一个实例对象\",{\"1\":{\"53\":1}}],[\"都是直接使用new关键字就能直接搞定了\",{\"1\":{\"9\":1}}],[\"都在\",{\"1\":{\"38\":1}}],[\"自动装箱|拆箱机制\",{\"1\":{\"38\":1}}],[\"自其他接口的\",{\"1\":{\"28\":1}}],[\"自我介绍需要用到当前对象的名字和年龄\",{\"1\":{\"5\":1}}],[\"自我介绍只需要完成就行\",{\"1\":{\"5\":1}}],[\"的算法\",{\"1\":{\"229\":1}}],[\"的选择\",{\"0\":{\"225\":1}}],[\"的权重或者分布\",{\"1\":{\"228\":1}}],[\"的权重\",{\"1\":{\"223\":1}}],[\"的加权平均\",{\"1\":{\"223\":1}}],[\"的加权均值\",{\"1\":{\"121\":1}}],[\"的基本步骤\",{\"1\":{\"221\":1}}],[\"的基础上来引入偏置量\",{\"1\":{\"98\":1}}],[\"的文章中\",{\"1\":{\"217\":1}}],[\"的输出是不一样的\",{\"1\":{\"217\":1}}],[\"的输出\",{\"1\":{\"215\":1}}],[\"的参数\",{\"1\":{\"215\":1}}],[\"的就不是有关\",{\"1\":{\"215\":1}}],[\"的梯度时\",{\"1\":{\"215\":1}}],[\"的分布\",{\"1\":{\"208\":1}}],[\"的新形式\",{\"1\":{\"186\":1}}],[\"的定义\",{\"1\":{\"186\":1}}],[\"的定义出发\",{\"1\":{\"150\":1}}],[\"的估计从\",{\"1\":{\"203\":1}}],[\"的估计\",{\"1\":{\"185\":1,\"189\":1}}],[\"的采样\",{\"1\":{\"183\":1}}],[\"的思想\",{\"1\":{\"174\":1}}],[\"的值\",{\"1\":{\"172\":1}}],[\"的动作数量\",{\"1\":{\"160\":1}}],[\"的高效利用\",{\"0\":{\"154\":1}}],[\"的一些改进\",{\"1\":{\"153\":1}}],[\"的一个观测值\",{\"1\":{\"177\":1}}],[\"的一个\",{\"1\":{\"151\":1}}],[\"的情况\",{\"1\":{\"149\":1,\"150\":2,\"177\":1}}],[\"的情况下进行估计\",{\"1\":{\"148\":1}}],[\"的原理\",{\"1\":{\"149\":1}}],[\"的策略来进行选择\",{\"1\":{\"139\":1}}],[\"的策略πk+1​\",{\"1\":{\"136\":1}}],[\"的策略所采集的数据来\",{\"1\":{\"102\":1}}],[\"的根据策略π加权平均\",{\"1\":{\"122\":1}}],[\"的转到下一个状态的\",{\"1\":{\"121\":1}}],[\"的过程\",{\"1\":{\"120\":1}}],[\"的计算即可\",{\"1\":{\"115\":1}}],[\"的区别\",{\"0\":{\"114\":1}}],[\"的期望\",{\"1\":{\"113\":1}}],[\"的方法进行解决\",{\"1\":{\"215\":1}}],[\"的方法\",{\"1\":{\"102\":1,\"141\":1,\"158\":1,\"220\":2}}],[\"的方式去创建一个抽象类或是接口对象\",{\"1\":{\"65\":1}}],[\"的方式使用静态内容\",{\"1\":{\"28\":1}}],[\"的概率\",{\"1\":{\"93\":2}}],[\"的s是有范围的\",{\"1\":{\"86\":1}}],[\"的函数\",{\"1\":{\"82\":1,\"215\":1}}],[\"的功率谱密度\",{\"1\":{\"82\":1}}],[\"的无人机3d动态运动设计算法\",{\"1\":{\"78\":1}}],[\"的部署方法\",{\"1\":{\"78\":1}}],[\"的形式\",{\"1\":{\"68\":2,\"131\":1,\"172\":1}}],[\"的变量\",{\"1\":{\"66\":1}}],[\"的匿名类\",{\"1\":{\"65\":1}}],[\"的\",{\"1\":{\"28\":1,\"45\":1,\"136\":1,\"139\":1,\"150\":1,\"151\":2,\"154\":1,\"157\":1,\"159\":1,\"160\":1,\"184\":1,\"186\":2,\"196\":2,\"197\":2,\"206\":1,\"228\":1}}],[\"的使用\",{\"0\":{\"7\":1}}],[\"顶多说是多继承的一种替代方案\",{\"1\":{\"28\":1}}],[\"隔开即可\",{\"1\":{\"28\":1}}],[\"逗号\",{\"1\":{\"28\":1}}],[\"实际不常用\",{\"1\":{\"141\":1}}],[\"实际意义的解释\",{\"1\":{\"121\":1}}],[\"实际意义是\",{\"1\":{\"121\":1}}],[\"实际上这里面就是方法体\",{\"1\":{\"67\":1}}],[\"实际上正则表达式内容非常多\",{\"1\":{\"55\":1}}],[\"实际上实现接口更像是一个类的功能列表\",{\"1\":{\"28\":1}}],[\"实际上接口的目标就是将类所具有某些的行为抽象出来\",{\"1\":{\"28\":1}}],[\"实际上main就是一个函数\",{\"1\":{\"5\":1}}],[\"实现接口时\",{\"1\":{\"28\":1}}],[\"实现接口\",{\"1\":{\"28\":1}}],[\"定义目标函数\",{\"1\":{\"205\":1}}],[\"定义了agent与环境的交互行为\",{\"1\":{\"92\":1}}],[\"定义为ξ=\",{\"1\":{\"86\":1}}],[\"定义接口\",{\"1\":{\"28\":1}}],[\"定义\",{\"0\":{\"44\":1},\"1\":{\"28\":1,\"121\":1}}],[\"他们都具有学习这个能力\",{\"1\":{\"28\":1}}],[\"他们可能会做不同的事情\",{\"1\":{\"16\":1}}],[\"他只代表某个确切的功能\",{\"1\":{\"28\":1}}],[\"接收端\",{\"1\":{\"83\":1}}],[\"接口内部必须有且仅有一个抽象方法\",{\"1\":{\"67\":1}}],[\"接口也可以通过这种匿名内部类的形式\",{\"1\":{\"65\":1}}],[\"接口同样支持向下转型\",{\"1\":{\"28\":1}}],[\"接口跟抽象类一样\",{\"1\":{\"28\":1}}],[\"接口定义\",{\"1\":{\"28\":1}}],[\"接口的默认方法是保底的\",{\"1\":{\"28\":1}}],[\"接口的继承相当于是对接口功能的融合罢了\",{\"1\":{\"28\":1}}],[\"接口的使用和继承的概念有一定的出入\",{\"1\":{\"28\":1}}],[\"接口支持多继承\",{\"1\":{\"28\":1}}],[\"接口是可以继承\",{\"1\":{\"28\":1}}],[\"接口中可以存在让抽象方法的默认实现\",{\"1\":{\"28\":1}}],[\"接口中只能定义访问权限为public抽象方法\",{\"1\":{\"28\":1}}],[\"接口中如果定义了与\",{\"1\":{\"28\":1}}],[\"接口中定义的静态方法也只能是public的\",{\"1\":{\"28\":1}}],[\"接口中定义的静态变量只能是public\",{\"1\":{\"28\":1}}],[\"接口中不允许存在成员变量和成员方法\",{\"1\":{\"28\":1}}],[\"接口中的方法可以存在默认实现\",{\"1\":{\"28\":1}}],[\"接口不同于类\",{\"1\":{\"28\":1}}],[\"接口可以实现很多个\",{\"1\":{\"28\":1}}],[\"接口里只能定义对应的抽象方法\",{\"1\":{\"28\":1}}],[\"接口包含了一些列方法的定义\",{\"1\":{\"28\":1}}],[\"接口一般只代表某些功能的抽象\",{\"1\":{\"28\":1}}],[\"接口甚至比抽象类还抽象\",{\"1\":{\"28\":1}}],[\"接口\",{\"0\":{\"28\":1},\"1\":{\"28\":1,\"65\":1}}],[\"接着我们对三个属性挨个进行比较\",{\"1\":{\"25\":1}}],[\"中取出一定数量的样本\",{\"1\":{\"215\":1}}],[\"中采用一个\",{\"1\":{\"215\":1}}],[\"中直接根据\",{\"1\":{\"188\":1}}],[\"中不断切换\",{\"1\":{\"155\":1}}],[\"中第一次出现的\",{\"1\":{\"154\":1}}],[\"中求解\",{\"1\":{\"141\":1}}],[\"中涉及的\",{\"1\":{\"135\":1}}],[\"中方法\",{\"1\":{\"65\":1}}],[\"中除了\",{\"1\":{\"55\":1}}],[\"中字符的所有字符\",{\"1\":{\"55\":1}}],[\"中所有的\",{\"1\":{\"55\":1}}],[\"中的前三个\",{\"1\":{\"55\":1}}],[\"中的所有字符\",{\"1\":{\"55\":1}}],[\"中的所有\",{\"1\":{\"55\":1}}],[\"中的两个\",{\"1\":{\"55\":1}}],[\"中的\",{\"1\":{\"55\":3,\"161\":1,\"215\":2}}],[\"中\",{\"0\":{\"172\":1},\"1\":{\"38\":1,\"51\":1,\"65\":1,\"66\":1,\"102\":1,\"144\":1,\"150\":1,\"161\":1,\"165\":1,\"210\":1,\"215\":1}}],[\"中定义的\",{\"1\":{\"27\":1}}],[\"中使用一个类之前\",{\"1\":{\"14\":1}}],[\"否则\",{\"1\":{\"86\":1}}],[\"否则返回false\",{\"1\":{\"55\":1}}],[\"否则会无法通过编译\",{\"1\":{\"27\":1}}],[\"否则要创建这个类的对象\",{\"1\":{\"9\":1}}],[\"具体推导过程\",{\"1\":{\"228\":1}}],[\"具体解决如下\",{\"1\":{\"170\":1}}],[\"具体求解方法\",{\"1\":{\"152\":1}}],[\"具体算法\",{\"0\":{\"152\":1}}],[\"具体步骤\",{\"0\":{\"136\":1}}],[\"具体分两步\",{\"1\":{\"129\":1}}],[\"具体代码\",{\"1\":{\"86\":1}}],[\"具体表述如下\",{\"1\":{\"84\":1}}],[\"具体如下\",{\"1\":{\"83\":1,\"215\":1}}],[\"具体实现克隆\",{\"1\":{\"29\":1}}],[\"具体的实现由\",{\"1\":{\"27\":1}}],[\"具体而言\",{\"1\":{\"5\":1}}],[\"正确性和收敛性分析\",{\"0\":{\"177\":1}}],[\"正常情况下\",{\"1\":{\"65\":1}}],[\"正常实例化方法是无法创造抽象类的实例\",{\"1\":{\"27\":1}}],[\"正好可以匹配\",{\"1\":{\"55\":1}}],[\"正则表达式并不是只有java才支持\",{\"1\":{\"55\":1}}],[\"正则表达式\",{\"0\":{\"41\":1,\"55\":1},\"1\":{\"55\":2}}],[\"正是这三大特性\",{\"1\":{\"21\":1}}],[\"抽象成接口来进行使用\",{\"1\":{\"28\":1}}],[\"抽象方法的访问权限不能为\",{\"1\":{\"27\":1}}],[\"抽象方法是指\",{\"1\":{\"27\":1}}],[\"抽象方法\",{\"1\":{\"27\":2}}],[\"抽象类中可以具有抽象方法\",{\"1\":{\"27\":1}}],[\"抽象类的子类也可以是一个抽象类\",{\"1\":{\"27\":1}}],[\"抽象类一般只用作继承使用\",{\"1\":{\"27\":1}}],[\"抽象类具有\",{\"1\":{\"27\":1}}],[\"抽象类\",{\"0\":{\"27\":1},\"1\":{\"27\":1}}],[\"加权均值\",{\"1\":{\"121\":1}}],[\"加上\",{\"1\":{\"121\":1}}],[\"加\",{\"1\":{\"26\":1}}],[\"上行\",{\"1\":{\"86\":1}}],[\"上\",{\"1\":{\"26\":1}}],[\"调用\",{\"1\":{\"68\":1}}],[\"调用clone方法\",{\"1\":{\"29\":1}}],[\"调用父类的实现\",{\"1\":{\"25\":1}}],[\"调用类的静态方法\",{\"1\":{\"14\":1}}],[\"必须是递增的\",{\"1\":{\"171\":1}}],[\"必须留一个抽象方法出来\",{\"1\":{\"67\":1}}],[\"必须要实现抽象类中所有抽象方法\",{\"1\":{\"27\":1}}],[\"必须使用equals方法\",{\"1\":{\"25\":1}}],[\"必须调用我们自己定义的构造方法\",{\"1\":{\"9\":1}}],[\"不太一样\",{\"1\":{\"217\":1}}],[\"不用进行求导\",{\"1\":{\"215\":1}}],[\"不记录\",{\"1\":{\"154\":1}}],[\"不动点x∗是唯一的\",{\"1\":{\"131\":1}}],[\"不常用\",{\"1\":{\"120\":1}}],[\"不会影响所求的梯度\",{\"1\":{\"99\":1}}],[\"不会调用外部类的\",{\"1\":{\"64\":1}}],[\"不见当年倒茶人\",{\"1\":{\"68\":1}}],[\"不是静态方法\",{\"1\":{\"68\":1}}],[\"不是基本类型\",{\"1\":{\"36\":1}}],[\"不支持抽象类\",{\"1\":{\"67\":1}}],[\"不包括换行\",{\"1\":{\"55\":1}}],[\"不要使用\",{\"1\":{\"53\":1}}],[\"不允许\",{\"1\":{\"48\":1}}],[\"不允许进行修改\",{\"1\":{\"45\":1}}],[\"不需要声明访问权限\",{\"1\":{\"63\":1}}],[\"不需要依附任何对象\",{\"1\":{\"62\":1}}],[\"不需要\",{\"1\":{\"36\":1,\"62\":1}}],[\"不过没必要\",{\"1\":{\"53\":1}}],[\"不过可以省略\",{\"1\":{\"28\":1}}],[\"不过如果子类也是抽象类\",{\"1\":{\"27\":1}}],[\"不能用同一个q\",{\"1\":{\"86\":1}}],[\"不能直接通过\",{\"1\":{\"65\":1}}],[\"不能直接创建对象\",{\"1\":{\"28\":1}}],[\"不能匹配\",{\"1\":{\"55\":2}}],[\"不能随便进行修改\",{\"1\":{\"45\":1}}],[\"不能\",{\"1\":{\"38\":1}}],[\"不能使用默认\",{\"1\":{\"28\":1}}],[\"不能使用==\",{\"1\":{\"25\":1}}],[\"不能是\",{\"1\":{\"27\":1}}],[\"不同的无人机agent的q\",{\"1\":{\"86\":1}}],[\"不同的访问权限\",{\"1\":{\"17\":1}}],[\"不同包下的类\",{\"1\":{\"17\":1}}],[\"不同包下的子类\",{\"1\":{\"17\":1}}],[\"不同类的重名问题\",{\"1\":{\"16\":1}}],[\"不同对象的属性是分开独立存放的\",{\"1\":{\"4\":1}}],[\"那作用范围也就只能在方法中了\",{\"1\":{\"63\":1}}],[\"那就是参数了\",{\"1\":{\"61\":1}}],[\"那肯定不相等\",{\"1\":{\"25\":1}}],[\"那么此时的\",{\"1\":{\"208\":1}}],[\"那么此时就出现了歧义\",{\"1\":{\"16\":1}}],[\"那么只要求解\",{\"1\":{\"172\":1}}],[\"那么这称为\",{\"1\":{\"157\":1}}],[\"那么这些类如果都放在一起的话\",{\"1\":{\"16\":1}}],[\"那么对应的损失函数求解为\",{\"1\":{\"215\":1}}],[\"那么对于随机变量x的估计可以为\",{\"1\":{\"148\":1}}],[\"那么对象构造好之后\",{\"1\":{\"9\":1}}],[\"那么对象的属性都会存在初始值\",{\"1\":{\"4\":1}}],[\"那么\",{\"1\":{\"141\":1,\"167\":1,\"171\":1}}],[\"那么使用时候\",{\"1\":{\"68\":1}}],[\"那么需要在前面添加外部类型名称\",{\"1\":{\"61\":1}}],[\"那么需要在前面添加外部类型名称test\",{\"1\":{\"61\":1}}],[\"那么如果内部类中也定义了同名的变量\",{\"1\":{\"61\":1}}],[\"那么就是b的\",{\"1\":{\"61\":1}}],[\"那么就是a的\",{\"1\":{\"61\":1}}],[\"那么就是不同的对象了\",{\"1\":{\"53\":1}}],[\"那么就像我们把成员变量访问权限变成私有一样\",{\"1\":{\"61\":1}}],[\"那么就需要创造一个对象\",{\"1\":{\"61\":1}}],[\"那么中间就需要产生4个字符串对象出来\",{\"1\":{\"54\":1}}],[\"那么始终都是同一个对象\",{\"1\":{\"53\":1}}],[\"那么始终都会得到同一个对象\",{\"1\":{\"36\":1}}],[\"那么可以省去小括号\",{\"1\":{\"67\":1}}],[\"那么可以使用this关键字\",{\"1\":{\"7\":1}}],[\"那么可变长参数只能放在最后\",{\"1\":{\"50\":1}}],[\"那么会直接返回已经提前创建好的对象\",{\"1\":{\"36\":1}}],[\"那么自动装箱的呢\",{\"1\":{\"36\":1}}],[\"那么是不相等的\",{\"1\":{\"36\":1}}],[\"那么实现类中不强制要求进行实现\",{\"1\":{\"28\":1}}],[\"那么实际上只是传递了对象的引用\",{\"1\":{\"4\":1}}],[\"那么还有什么意义呢\",{\"1\":{\"27\":1}}],[\"那么无法重写\",{\"1\":{\"25\":1}}],[\"那么同样可以使用\",{\"1\":{\"25\":1}}],[\"那么我们可以对\",{\"1\":{\"151\":1}}],[\"那么我们可以将匿名内部类简写为lambda表达式\",{\"1\":{\"67\":1}}],[\"那么我们可以通过静态导入的方式将其中的静态方法或是静态变量直接导入使用\",{\"1\":{\"17\":1}}],[\"那么我们能否创建数组类型的数组呢\",{\"1\":{\"49\":1}}],[\"那么我们也可以给枚举类型添加独有的成员方法\",{\"1\":{\"30\":1}}],[\"那么我们就可以将学习这个能力\",{\"1\":{\"28\":1}}],[\"那么我们就可以将其重写了\",{\"1\":{\"25\":1}}],[\"那么其他的对象读取的就是被改变的值\",{\"1\":{\"13\":1}}],[\"那么通过这个类创建的所有对象\",{\"1\":{\"13\":1}}],[\"那么能否实现在对象创建时就为其指定名字\",{\"1\":{\"9\":1}}],[\"那么默认情况下可以不使用this关键字来明确表示当前对象\",{\"1\":{\"7\":1}}],[\"那么默认是null\",{\"1\":{\"4\":1}}],[\"那么默认是统一为0\",{\"1\":{\"4\":1}}],[\"那么创建对象之后能否直接访问呢\",{\"1\":{\"4\":1}}],[\"finding\",{\"1\":{\"165\":1,\"177\":1,\"183\":1}}],[\"final类型\",{\"1\":{\"45\":1}}],[\"final的status类型成员变量\",{\"1\":{\"30\":1}}],[\"final的\",{\"1\":{\"28\":1}}],[\"finalize\",{\"1\":{\"24\":1}}],[\"final\",{\"1\":{\"24\":6,\"26\":2,\"30\":5,\"48\":1,\"61\":2,\"62\":1,\"63\":1,\"66\":3}}],[\"first\",{\"1\":{\"154\":1}}],[\"fix\",{\"1\":{\"131\":1}}],[\"f\",{\"1\":{\"131\":4,\"174\":3,\"177\":1}}],[\"fundamental\",{\"1\":{\"125\":1}}],[\"function\",{\"0\":{\"203\":1,\"206\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1},\"1\":{\"50\":1,\"113\":1,\"131\":1,\"203\":1,\"207\":2,\"208\":1,\"209\":1,\"214\":1,\"215\":1,\"220\":1,\"221\":1}}],[\"future\",{\"0\":{\"117\":1},\"1\":{\"118\":1}}],[\"fs\",{\"1\":{\"83\":1}}],[\"fs​+l\",{\"1\":{\"83\":1}}],[\"fast\",{\"1\":{\"131\":1}}],[\"fair\",{\"1\":{\"83\":1}}],[\"false\",{\"1\":{\"25\":2}}],[\"fc​是载波频率\",{\"1\":{\"82\":1}}],[\"free\",{\"0\":{\"148\":1},\"1\":{\"118\":1,\"149\":1,\"150\":1}}],[\"framework\",{\"1\":{\"77\":1}}],[\"from\",{\"1\":{\"30\":1,\"114\":1,\"121\":2}}],[\"found\",{\"1\":{\"233\":1}}],[\"foem\",{\"1\":{\"128\":1}}],[\"following\",{\"1\":{\"92\":1}}],[\"fooooood\",{\"1\":{\"55\":1}}],[\"foooood\",{\"1\":{\"55\":1}}],[\"food\",{\"1\":{\"55\":1}}],[\"forallw\",{\"1\":{\"171\":1}}],[\"foralls∈s\",{\"1\":{\"152\":1}}],[\"fortheother∣a\",{\"1\":{\"160\":1}}],[\"formulation\",{\"0\":{\"178\":1}}],[\"form\",{\"0\":{\"119\":1},\"1\":{\"119\":1,\"120\":1,\"122\":2,\"128\":1,\"131\":1,\"136\":1,\"139\":2}}],[\"foreach\",{\"1\":{\"46\":1}}],[\"for\",{\"1\":{\"46\":3,\"50\":1,\"51\":1,\"53\":1,\"77\":2,\"93\":1,\"126\":2,\"131\":1}}],[\"float\",{\"1\":{\"36\":2}}],[\"注重近期的reward\",{\"1\":{\"92\":1}}],[\"注解\",{\"1\":{\"25\":1}}],[\"注意只是相当于外部来说\",{\"1\":{\"62\":1}}],[\"注意\",{\"1\":{\"9\":1,\"16\":1,\"50\":1,\"52\":1,\"61\":1}}],[\"时的平均reward\",{\"1\":{\"226\":1}}],[\"时wk​→w∗\",{\"1\":{\"177\":1}}],[\"时\",{\"1\":{\"24\":1}}],[\"垃圾\",{\"1\":{\"24\":1}}],[\"lnπ\",{\"1\":{\"228\":1}}],[\"limiting\",{\"1\":{\"208\":1}}],[\"link\",{\"1\":{\"81\":1}}],[\"l2​=log2​\",{\"1\":{\"83\":1}}],[\"l2​\",{\"1\":{\"83\":1}}],[\"l1​=log2​\",{\"1\":{\"83\":1}}],[\"l1​\",{\"1\":{\"83\":1}}],[\"l=min\",{\"1\":{\"83\":1}}],[\"learning是优化长期目标\",{\"1\":{\"86\":1}}],[\"learning的部署算法不同的是\",{\"1\":{\"87\":1}}],[\"learning的移动算法\",{\"1\":{\"87\":1}}],[\"learning的优化目标是最大化长期收益\",{\"1\":{\"86\":1}}],[\"learning的方案来解决无人机的np\",{\"1\":{\"78\":1}}],[\"learning算法\",{\"1\":{\"86\":1}}],[\"learning\",{\"0\":{\"76\":1,\"182\":1,\"184\":1,\"188\":1,\"192\":1,\"193\":1,\"197\":1,\"210\":1,\"211\":1,\"213\":1,\"214\":1},\"1\":{\"78\":2,\"192\":1,\"193\":1,\"196\":1,\"197\":1,\"214\":1}}],[\"length方法可以求字符串长度\",{\"1\":{\"53\":1}}],[\"length属性是int类型的值\",{\"1\":{\"45\":1}}],[\"length\",{\"1\":{\"45\":2,\"46\":1,\"53\":2}}],[\"large\",{\"1\":{\"148\":1}}],[\"law\",{\"1\":{\"148\":1}}],[\"lambda表达式的具体规范\",{\"1\":{\"67\":1}}],[\"lambda表达式\",{\"0\":{\"67\":1}}],[\"lambda\",{\"1\":{\"66\":1,\"67\":1}}],[\"lang包下的\",{\"1\":{\"16\":1}}],[\"lang\",{\"1\":{\"16\":1,\"29\":1,\"30\":2}}],[\"lang这个包下的所有类\",{\"1\":{\"16\":1}}],[\"lbwnb\",{\"1\":{\"51\":1,\"68\":2}}],[\"loss\",{\"1\":{\"214\":1}}],[\"low\",{\"1\":{\"36\":2}}],[\"long\",{\"1\":{\"24\":2,\"36\":3,\"38\":1,\"208\":1}}],[\"wt+1​=wt​+αt​n1​i=1∑n​\",{\"1\":{\"215\":1}}],[\"wt​=w\",{\"1\":{\"215\":1}}],[\"wt​\",{\"1\":{\"209\":2,\"210\":2,\"211\":4,\"212\":3,\"213\":3,\"215\":10}}],[\"w−e\",{\"1\":{\"172\":1,\"183\":1}}],[\"w∗\",{\"1\":{\"171\":2}}],[\"w∈r\",{\"1\":{\"169\":1}}],[\"wk−1​\",{\"1\":{\"171\":1}}],[\"wk​−\",{\"1\":{\"183\":1}}],[\"wk​−xk​\",{\"1\":{\"167\":2,\"172\":1}}],[\"wk​\",{\"1\":{\"170\":6,\"171\":1,\"172\":1,\"174\":6,\"177\":12,\"183\":1,\"209\":1}}],[\"wk​+xk​\",{\"1\":{\"167\":1}}],[\"wk​=k−11​i=1∑k−1​xi​\",{\"1\":{\"167\":1}}],[\"wk+1​=wk​+αk​\",{\"1\":{\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1}}],[\"wk+1​=wk​−αk​g~​\",{\"1\":{\"183\":1}}],[\"wk+1​=wk​−αk​g​\",{\"1\":{\"172\":1,\"177\":1}}],[\"wk+1​=wk​−αk​e\",{\"1\":{\"177\":1}}],[\"wk+1​=wk​−αk​▽w​j\",{\"1\":{\"209\":1}}],[\"wk+1​=wk​−αk​▽w​f\",{\"1\":{\"174\":1}}],[\"wk+1​=wk​−αk​▽w​e\",{\"1\":{\"174\":1}}],[\"wk+1​=wk​−αk​n1​i=1∑n​▽w​f\",{\"1\":{\"174\":1}}],[\"wk+1​=wk​−ak​g​\",{\"1\":{\"170\":1}}],[\"wk+1​=wk​−k1​\",{\"1\":{\"167\":1}}],[\"wk+1​=k1​i=1∑k​xi​\",{\"1\":{\"167\":1}}],[\"wk+1​​=k1​∑i=1k​xi​​=k1​\",{\"1\":{\"167\":1}}],[\"wk+1​可以由wk​推导出来\",{\"1\":{\"167\":1}}],[\"why\",{\"0\":{\"120\":1}}],[\"where\",{\"1\":{\"119\":1,\"131\":1}}],[\"when\",{\"1\":{\"92\":1}}],[\"wargmin​j\",{\"1\":{\"174\":1}}],[\"walk\",{\"1\":{\"87\":2}}],[\"wait\",{\"1\":{\"24\":3}}],[\"with\",{\"0\":{\"210\":1,\"211\":1,\"212\":1,\"213\":1},\"1\":{\"83\":1,\"92\":2,\"171\":1}}],[\"wireless\",{\"1\":{\"77\":1}}],[\"w\",{\"1\":{\"55\":1,\"169\":3,\"170\":2,\"171\":3,\"172\":5,\"174\":5,\"177\":15,\"183\":4,\"204\":1,\"205\":1,\"206\":2,\"207\":3,\"208\":3,\"209\":6,\"214\":3,\"215\":21}}],[\"world\",{\"1\":{\"53\":6}}],[\"worker\",{\"1\":{\"27\":2}}],[\"www\",{\"1\":{\"16\":1,\"55\":1}}],[\"使得v^\",{\"1\":{\"205\":1}}],[\"使得java能够更好的体现面向对象的思想\",{\"1\":{\"35\":1}}],[\"使得持有当前对象锁的线程进入等待状态\",{\"1\":{\"24\":1}}],[\"使用new表示\",{\"1\":{\"68\":1}}],[\"使用双冒号来进行方法引用\",{\"1\":{\"68\":1}}],[\"使用外部静态变量\",{\"1\":{\"64\":1}}],[\"使用频率很低\",{\"1\":{\"63\":1}}],[\"使用split方法进行字符串分割\",{\"1\":{\"53\":1}}],[\"使用就像对象的参数一样\",{\"1\":{\"30\":1}}],[\"使用枚举类也非常方便\",{\"1\":{\"30\":1}}],[\"使用default关键字为接口中的方法添加默认实现\",{\"1\":{\"28\":1}}],[\"使用implements关键字来实现接口\",{\"1\":{\"28\":1}}],[\"使用import关键字导入其他包中的类\",{\"1\":{\"16\":1}}],[\"使用interface表示这是一个接口\",{\"1\":{\"28\":1}}],[\"唤醒所有等待当前对象锁的线程\",{\"1\":{\"24\":1}}],[\"唤醒一个等待当前对象锁的线程\",{\"1\":{\"24\":1}}],[\"十六进制哈希值\",{\"1\":{\"24\":1}}],[\"完整类名\",{\"1\":{\"24\":1}}],[\"完成自我介绍需要执行的所有代码就在这个花括号中编写\",{\"1\":{\"5\":1}}],[\"将基于表格表示的策略\",{\"1\":{\"221\":1}}],[\"将该网络的\",{\"1\":{\"215\":1}}],[\"将\",{\"1\":{\"131\":1,\"158\":1,\"215\":2}}],[\"将所有状态的\",{\"1\":{\"119\":1}}],[\"将处于\",{\"1\":{\"102\":1}}],[\"将对于\",{\"1\":{\"96\":1}}],[\"将对应的轨迹所获得的所有reward的总和\",{\"1\":{\"92\":1}}],[\"将无人机部署在每个中心内\",{\"1\":{\"86\":1}}],[\"将上述优化问题简化\",{\"1\":{\"86\":1}}],[\"将其平均分配给其∣kn​∣个关联用户\",{\"1\":{\"82\":1}}],[\"将其中的抽象方法实现\",{\"1\":{\"65\":1}}],[\"将匹配\",{\"1\":{\"55\":1}}],[\"将匹配的子串替换或者从某个串中取出符合某个条件的子串等\",{\"1\":{\"55\":1}}],[\"将这些值提前做成包装类放在数组中存放\",{\"1\":{\"36\":1}}],[\"将int类型值作为包装类型使用\",{\"1\":{\"36\":1}}],[\"将当前对象转换为string的形式\",{\"1\":{\"24\":1}}],[\"将main类放到com\",{\"1\":{\"16\":1}}],[\"判断当前对象和给定对象是否相等\",{\"1\":{\"24\":1}}],[\">=\",{\"1\":{\"36\":1}}],[\">\",{\"1\":{\"24\":1,\"36\":8,\"67\":7,\"68\":2}}],[\"这另一个策略会更新到最优的策略\",{\"1\":{\"196\":1}}],[\"这类算法统称为\",{\"1\":{\"155\":1}}],[\"这一条\",{\"1\":{\"154\":1}}],[\"这两个算法是一致的\",{\"1\":{\"143\":1}}],[\"这步是更新策略π\",{\"1\":{\"136\":1}}],[\"这种迭代算法称为\",{\"1\":{\"135\":1}}],[\"这种情况下的\",{\"1\":{\"225\":1}}],[\"这种情况我们将\",{\"1\":{\"225\":1}}],[\"这种情况是可以简化的\",{\"1\":{\"67\":1}}],[\"这种情况实际上会被优化为下面的写法\",{\"1\":{\"54\":1}}],[\"这种局部内部类的形式\",{\"1\":{\"63\":1}}],[\"这种即可\",{\"1\":{\"28\":1}}],[\"这\",{\"1\":{\"54\":2}}],[\"这样才能去估计相应的qπ​\",{\"1\":{\"157\":1}}],[\"这样\",{\"1\":{\"154\":1}}],[\"这样就可以与\",{\"1\":{\"184\":1}}],[\"这样就是一个实现\",{\"1\":{\"65\":1}}],[\"这样就不同\",{\"1\":{\"53\":1}}],[\"这样就不会得到同一个对象了\",{\"1\":{\"36\":1}}],[\"这样参数名称所表示的就是一个数组\",{\"1\":{\"50\":1}}],[\"这样的任务称为episodic\",{\"1\":{\"92\":1}}],[\"这样的\",{\"1\":{\"47\":1}}],[\"这样是不能赋值的\",{\"1\":{\"47\":1}}],[\"这些方法并没有被重写\",{\"1\":{\"45\":1}}],[\"这些可以省\",{\"1\":{\"28\":1}}],[\"这是我们所求的量\",{\"1\":{\"209\":1}}],[\"这是来求解\",{\"1\":{\"152\":1}}],[\"这是为了提升效率\",{\"1\":{\"36\":1}}],[\"这是因为\",{\"1\":{\"36\":1}}],[\"这是浅拷贝\",{\"1\":{\"29\":1}}],[\"这是强制要求的\",{\"1\":{\"27\":1}}],[\"这同样不是se中需要学习的内容\",{\"1\":{\"24\":1}}],[\"这个仍然与之前一致\",{\"1\":{\"152\":1}}],[\"这个\",{\"1\":{\"151\":1}}],[\"这个便成为\",{\"1\":{\"96\":1}}],[\"这个抽象类直接就定义好了\",{\"1\":{\"65\":1}}],[\"这个长度是字符的数量\",{\"1\":{\"53\":1}}],[\"这个接口中什么都没定义\",{\"1\":{\"29\":1}}],[\"这个默认方法没有任何作用\",{\"1\":{\"28\":1}}],[\"这个注解默认情况下可以省略\",{\"1\":{\"25\":1}}],[\"这个方法我们会在jvm篇视频教程中详细介绍\",{\"1\":{\"24\":1}}],[\"这个我们会在最后一章的反射中进行讲解\",{\"1\":{\"24\":1}}],[\"这个类在初始化时会对类中其他本地方法进行注册\",{\"1\":{\"24\":1}}],[\"这里可以用不同的方法来近似\",{\"1\":{\"229\":1}}],[\"这里在视频没有详细介绍\",{\"1\":{\"228\":1}}],[\"这里包含了一个\",{\"1\":{\"209\":1}}],[\"这里需要通过迭代算法来精确求出\",{\"1\":{\"143\":1}}],[\"这里需要特别说一下\",{\"1\":{\"47\":1}}],[\"这里需要特别注意\",{\"1\":{\"9\":1}}],[\"这里\",{\"1\":{\"139\":1}}],[\"这里有多种mobility\",{\"1\":{\"87\":1}}],[\"这里采用离散化空间坐标\",{\"1\":{\"86\":1}}],[\"这里仅仅是对正则表达式的简单使用\",{\"1\":{\"55\":1}}],[\"这里字符串是oooo\",{\"1\":{\"55\":1}}],[\"这里进行4次加法运算\",{\"1\":{\"54\":1}}],[\"这里我们需要特别注意一下\",{\"1\":{\"61\":1}}],[\"这里我们可以自由传入任意数量的字符串\",{\"1\":{\"50\":1}}],[\"这里我们定义一个info静态变量\",{\"1\":{\"13\":1}}],[\"这里ceiling表示向上取整\",{\"1\":{\"38\":1}}],[\"这里会有一个integercache\",{\"1\":{\"36\":1}}],[\"这里本质上就是被自动包装成了一个integer类型的对象\",{\"1\":{\"36\":1}}],[\"这里使用javap命令对class文件进行反编译得到\",{\"1\":{\"30\":1}}],[\"这里向上抛出一下异常\",{\"1\":{\"29\":1}}],[\"这里是每个无人机都有一张自己的q\",{\"1\":{\"86\":1}}],[\"这里是\",{\"1\":{\"9\":1}}],[\"这里没有使用this\",{\"1\":{\"7\":1}}],[\"这里实际上是将方法参数的局部变量name赋值为本身\",{\"1\":{\"7\":1}}],[\"这里编写代码跟我们之前在main中是一样的\",{\"1\":{\"5\":1}}],[\"这里的策略是随机性\",{\"1\":{\"228\":1}}],[\"这里的最优策略πk+1​是一个\",{\"1\":{\"136\":1}}],[\"这里的name是其所依附对象的\",{\"1\":{\"61\":1}}],[\"这里的话只能使用接口中的方法\",{\"1\":{\"28\":1}}],[\"这里的p1存放的是对象的引用\",{\"1\":{\"4\":1}}],[\"这里的a存放的是具体的某个值\",{\"1\":{\"4\":1}}],[\"这里定义的人类具有三个属性\",{\"1\":{\"4\":1}}],[\"hybrid\",{\"1\":{\"87\":1}}],[\"huav​\",{\"1\":{\"86\":2,\"87\":2}}],[\"hmax​−hmin​+1\",{\"1\":{\"86\":1}}],[\"hmax​mostotal​=∑n=1n​∑kn​=1kn​​moskn​​\",{\"1\":{\"86\":1}}],[\"hmax​mostotal​=∑n=1n​∑kn​=1kn​​∑t=0ts​​moskn​​\",{\"1\":{\"84\":1}}],[\"hmax​\",{\"1\":{\"81\":1,\"86\":1}}],[\"hmin​≤hn​\",{\"1\":{\"84\":1,\"86\":1}}],[\"hmin​\",{\"1\":{\"81\":1,\"86\":1}}],[\"has\",{\"1\":{\"131\":1}}],[\"hashcode\",{\"1\":{\"24\":3}}],[\"hard问题\",{\"1\":{\"86\":1}}],[\"hard\",{\"1\":{\"78\":1}}],[\"html\",{\"1\":{\"55\":1}}],[\"https\",{\"1\":{\"55\":1}}],[\"h\",{\"1\":{\"38\":2}}],[\"high\",{\"1\":{\"36\":1}}],[\"hello\",{\"1\":{\"5\":2,\"53\":6,\"63\":1}}],[\"e+γvπ​\",{\"1\":{\"186\":1}}],[\"eplison\",{\"0\":{\"158\":1}}],[\"episodestartingfrom\",{\"1\":{\"154\":4}}],[\"episodes\",{\"1\":{\"151\":1,\"155\":2,\"157\":2}}],[\"episode\",{\"0\":{\"154\":1},\"1\":{\"92\":2,\"151\":2,\"154\":6,\"155\":1,\"157\":1,\"159\":1,\"210\":1}}],[\"efficient\",{\"1\":{\"154\":1}}],[\"every\",{\"1\":{\"154\":2}}],[\"evaluation\",{\"1\":{\"96\":2,\"118\":1,\"120\":1,\"139\":1,\"143\":1,\"152\":1,\"155\":1}}],[\"elementwise\",{\"1\":{\"122\":1,\"128\":1,\"136\":1,\"139\":2}}],[\"euqation\",{\"1\":{\"120\":1}}],[\"euqals\",{\"1\":{\"24\":1}}],[\"equation\",{\"0\":{\"115\":1,\"118\":1,\"119\":1,\"127\":1},\"1\":{\"110\":1,\"118\":1,\"119\":2,\"122\":2,\"125\":1,\"131\":1,\"139\":1,\"141\":2,\"143\":1,\"144\":1,\"150\":1,\"186\":3,\"188\":1,\"192\":1}}],[\"equals\",{\"1\":{\"24\":1,\"25\":4,\"53\":1}}],[\"e\",{\"1\":{\"55\":2,\"116\":2,\"117\":4,\"121\":2,\"148\":1,\"167\":1,\"171\":2,\"172\":3,\"174\":1,\"177\":2,\"183\":1,\"186\":1}}],[\"estimation\",{\"0\":{\"172\":1,\"176\":1,\"205\":1},\"1\":{\"148\":2,\"150\":1,\"155\":1,\"165\":1}}],[\"es∼η\",{\"1\":{\"99\":1}}],[\"es\",{\"1\":{\"55\":1}}],[\"environment\",{\"1\":{\"92\":2}}],[\"enum<com\",{\"1\":{\"30\":1}}],[\"enum表示这是一个枚举类\",{\"1\":{\"30\":1}}],[\"enum\",{\"1\":{\"30\":2}}],[\"entity\",{\"1\":{\"16\":5,\"17\":1,\"64\":3,\"65\":2}}],[\"existence\",{\"1\":{\"131\":1}}],[\"excellent\",{\"1\":{\"83\":1}}],[\"exploration\",{\"1\":{\"160\":1}}],[\"exploring\",{\"0\":{\"153\":1,\"156\":1,\"157\":1},\"1\":{\"153\":1,\"157\":2,\"158\":1,\"161\":1}}],[\"exploitation\",{\"1\":{\"160\":1}}],[\"exponentially\",{\"1\":{\"131\":1}}],[\"expected\",{\"0\":{\"191\":1},\"1\":{\"113\":1}}],[\"expection\",{\"1\":{\"113\":1,\"186\":1,\"206\":1,\"209\":1}}],[\"experience\",{\"0\":{\"83\":1},\"1\":{\"77\":1,\"153\":1,\"185\":1,\"189\":1}}],[\"expression\",{\"1\":{\"55\":1}}],[\"extends\",{\"1\":{\"27\":1,\"28\":2,\"29\":1,\"30\":1}}],[\"exam\",{\"1\":{\"25\":2,\"27\":2}}],[\"other\",{\"1\":{\"126\":1}}],[\"optimization\",{\"0\":{\"209\":1},\"1\":{\"165\":1}}],[\"optimality\",{\"0\":{\"127\":1},\"1\":{\"125\":1,\"192\":1}}],[\"optimal\",{\"0\":{\"126\":1,\"192\":1},\"1\":{\"125\":2,\"126\":1,\"193\":1}}],[\"opinion\",{\"1\":{\"78\":1}}],[\"obejctive\",{\"0\":{\"206\":1},\"1\":{\"209\":1}}],[\"obtained\",{\"1\":{\"114\":1}}],[\"obj\",{\"1\":{\"24\":2,\"25\":4,\"44\":1}}],[\"objective\",{\"1\":{\"207\":2,\"208\":1,\"214\":1,\"215\":1,\"221\":1}}],[\"object类中的\",{\"0\":{\"29\":1}}],[\"object\",{\"0\":{\"24\":1},\"1\":{\"24\":4,\"25\":3,\"28\":2,\"29\":1,\"44\":1,\"45\":1,\"47\":5}}],[\"originalepisode\",{\"1\":{\"154\":1}}],[\"or\",{\"1\":{\"92\":1,\"113\":1,\"165\":1,\"185\":1,\"208\":1}}],[\"off\",{\"0\":{\"102\":1,\"104\":1,\"194\":1,\"196\":1,\"198\":1},\"1\":{\"196\":1,\"197\":1}}],[\"of\",{\"0\":{\"83\":1,\"116\":1,\"117\":1,\"184\":1,\"188\":1,\"192\":1,\"205\":1},\"1\":{\"77\":3,\"83\":1,\"92\":4,\"93\":3,\"113\":1,\"114\":1,\"118\":2,\"131\":1,\"148\":1,\"154\":1,\"155\":1,\"165\":1,\"172\":1,\"192\":1,\"208\":1,\"215\":1}}],[\"on\",{\"0\":{\"194\":1,\"195\":1,\"199\":1},\"1\":{\"76\":1,\"196\":1,\"197\":1,\"213\":1,\"217\":1},\"2\":{\"89\":1}}],[\"o\",{\"1\":{\"55\":15}}],[\"o+\",{\"1\":{\"55\":2}}],[\"oooo\",{\"1\":{\"55\":1}}],[\"override\",{\"1\":{\"25\":3,\"27\":1,\"28\":1,\"29\":2,\"65\":2,\"66\":1}}],[\"out\",{\"1\":{\"4\":1,\"5\":1,\"9\":3,\"13\":2,\"17\":1,\"25\":2,\"27\":1,\"28\":2,\"29\":4,\"30\":1,\"36\":3,\"37\":1,\"38\":2,\"45\":1,\"46\":2,\"49\":1,\"50\":1,\"51\":1,\"53\":6,\"54\":3,\"55\":2,\"61\":5,\"62\":1,\"63\":1,\"64\":7,\"65\":3,\"66\":1,\"67\":3,\"68\":2}}],[\"父类是\",{\"1\":{\"23\":1}}],[\"多维数组\",{\"0\":{\"49\":1}}],[\"多个不同的对象对同一消息作出响应\",{\"1\":{\"21\":1}}],[\"多态\",{\"1\":{\"21\":1}}],[\"叫子类\",{\"1\":{\"21\":1}}],[\"继承自number类\",{\"1\":{\"36\":1}}],[\"继承\",{\"0\":{\"23\":1},\"1\":{\"21\":1}}],[\"继承和多态是面向对象编程的三大特性\",{\"1\":{\"21\":1}}],[\"继承和多态\",{\"0\":{\"21\":1}}],[\"隐藏实现细节\",{\"1\":{\"21\":1}}],[\"隐式加载\",{\"1\":{\"14\":1}}],[\"封装成一个类\",{\"1\":{\"36\":1}}],[\"封装\",{\"0\":{\"21\":1,\"22\":1},\"1\":{\"21\":2}}],[\"❌\",{\"1\":{\"17\":6}}],[\"✅\",{\"1\":{\"17\":10}}],[\"同理\",{\"1\":{\"92\":1}}],[\"同时也使得基本类型能够支持对象操作\",{\"1\":{\"35\":1}}],[\"同时具体实现还需要由主体来实现\",{\"1\":{\"28\":1}}],[\"同样地\",{\"1\":{\"121\":1}}],[\"同样可以用条件概率的形式进行描述\",{\"1\":{\"92\":1}}],[\"同样\",{\"1\":{\"82\":1,\"172\":1}}],[\"同样需要使用\",{\"1\":{\"44\":1}}],[\"同样需要将接口中所有的抽象方法全部实现\",{\"1\":{\"28\":1}}],[\"同样的\",{\"1\":{\"13\":2,\"36\":1,\"65\":1}}],[\"同名的方法\",{\"1\":{\"28\":1}}],[\"同上\",{\"1\":{\"24\":4,\"44\":1}}],[\"同一无人机通过fdma同时为同一集群中的多个用户提供服务\",{\"1\":{\"81\":1}}],[\"同一消息根据不同的对象而采用各种不同的方法\",{\"1\":{\"21\":1}}],[\"同一个包下的类\",{\"1\":{\"17\":1}}],[\"公共\",{\"1\":{\"17\":1}}],[\"子类也可以定义\",{\"1\":{\"27\":1}}],[\"子类对应必须是\",{\"1\":{\"27\":1}}],[\"子类必须要实现抽象类所有的抽象方法\",{\"1\":{\"27\":1}}],[\"子类\",{\"1\":{\"27\":1}}],[\"子类是\",{\"1\":{\"23\":1}}],[\"子类实现了父类所有非私有化的属性和方法\",{\"1\":{\"21\":1}}],[\"子类我们会在下一章介绍\",{\"1\":{\"17\":1}}],[\"子类初始化时\",{\"1\":{\"14\":1}}],[\"受保护\",{\"1\":{\"17\":1}}],[\"私有\",{\"1\":{\"17\":1}}],[\"编译器是很聪明的\",{\"1\":{\"54\":1}}],[\"编译器不知道到底我们想用的是哪一个string类\",{\"1\":{\"16\":1}}],[\"编译出来就自带\",{\"1\":{\"9\":1}}],[\"表示是\",{\"1\":{\"208\":1}}],[\"表示是一种长时间的交互行为\",{\"1\":{\"208\":1}}],[\"表示是最佳部署位置\",{\"1\":{\"86\":1}}],[\"表示对于对应\",{\"1\":{\"157\":1}}],[\"表示对于每一个\",{\"1\":{\"157\":1}}],[\"表示状态转移矩阵\",{\"1\":{\"119\":1}}],[\"表示一个函数\",{\"1\":{\"183\":1}}],[\"表示一个给定的策略\",{\"1\":{\"118\":1}}],[\"表示一个区间\",{\"1\":{\"55\":1}}],[\"表示为\",{\"1\":{\"113\":1,\"225\":3}}],[\"表示为rkn​​\",{\"1\":{\"82\":1}}],[\"表示在单步情况下\",{\"1\":{\"226\":1}}],[\"表示在策略π下\",{\"1\":{\"226\":1}}],[\"表示在一个\",{\"1\":{\"210\":1}}],[\"表示在各状态执行各动作的概率\",{\"1\":{\"93\":1}}],[\"表示在状态s下采取动作a\",{\"1\":{\"93\":2}}],[\"表示\",{\"1\":{\"83\":1,\"208\":1,\"215\":1,\"228\":1}}],[\"表示round\",{\"1\":{\"83\":1}}],[\"表示无人机与用户之间的仰角\",{\"1\":{\"82\":1}}],[\"表示所有小写字母\",{\"1\":{\"55\":1}}],[\"表示abc这几个字符可以出现\",{\"1\":{\"55\":1}}],[\"表示当前数组长度\",{\"1\":{\"45\":1}}],[\"表示long的最大值\",{\"1\":{\"38\":1}}],[\"表示类支持接口代表的功能\",{\"1\":{\"28\":1}}],[\"表示类具有的属性\",{\"1\":{\"4\":1}}],[\"表示这个类具有克隆的功能\",{\"1\":{\"29\":1}}],[\"表示这个类是一个抽象类\",{\"1\":{\"27\":1}}],[\"表示这个类不能再被继承了\",{\"1\":{\"26\":1}}],[\"表示导入这个包中全部的类\",{\"1\":{\"16\":1}}],[\"导入我们需要使用的类\",{\"1\":{\"16\":1}}],[\"identically\",{\"1\":{\"148\":1}}],[\"idle\",{\"1\":{\"83\":1}}],[\"iid\",{\"1\":{\"148\":1}}],[\"i−γpπk​​\",{\"1\":{\"141\":1}}],[\"i−γpπ​\",{\"1\":{\"120\":2}}],[\"ij​=pπ​\",{\"1\":{\"119\":1}}],[\"immediate\",{\"0\":{\"116\":1},\"1\":{\"118\":1}}],[\"improvement\",{\"1\":{\"139\":1,\"143\":1,\"152\":1,\"155\":1,\"161\":1,\"184\":1,\"188\":1,\"193\":1}}],[\"implements\",{\"1\":{\"28\":2,\"29\":1}}],[\"importance\",{\"0\":{\"103\":1}}],[\"import\",{\"1\":{\"16\":3,\"17\":1,\"38\":2,\"64\":1}}],[\"i=1n​\",{\"1\":{\"92\":2}}],[\"iteration\",{\"0\":{\"135\":1,\"138\":1,\"142\":1,\"143\":2,\"144\":1,\"145\":1},\"1\":{\"135\":1,\"141\":2,\"142\":2,\"143\":10,\"144\":3,\"149\":1,\"150\":1,\"152\":1,\"155\":1}}],[\"iterative\",{\"1\":{\"120\":1,\"165\":1}}],[\"it\",{\"1\":{\"77\":1,\"154\":1}}],[\"is\",{\"1\":{\"77\":3,\"92\":1,\"93\":2,\"114\":1,\"126\":1,\"131\":2,\"154\":1,\"159\":2,\"227\":1}}],[\"ieee\",{\"1\":{\"76\":1},\"2\":{\"89\":1}}],[\"i都会更新成数组中下一个元素\",{\"1\":{\"46\":1}}],[\"i+\",{\"1\":{\"46\":1}}],[\"i++\",{\"1\":{\"46\":1}}],[\"i就是每一个数组中的元素\",{\"1\":{\"46\":1}}],[\"i\",{\"1\":{\"36\":9,\"37\":4,\"38\":6,\"46\":4,\"148\":2,\"151\":1}}],[\"if\",{\"1\":{\"25\":2,\"28\":1,\"36\":1,\"126\":1,\"131\":1,\"159\":1,\"171\":1}}],[\"independent\",{\"1\":{\"148\":1}}],[\"invoked\",{\"1\":{\"77\":1}}],[\"in\",{\"0\":{\"76\":1},\"1\":{\"77\":1,\"154\":1,\"171\":1,\"192\":1}}],[\"inner2\",{\"1\":{\"61\":2}}],[\"inner1\",{\"1\":{\"61\":2}}],[\"inner\",{\"1\":{\"61\":12,\"62\":5,\"63\":5,\"64\":3}}],[\"info\",{\"1\":{\"13\":3}}],[\"interacting\",{\"1\":{\"92\":1}}],[\"interface\",{\"1\":{\"28\":3,\"29\":1,\"65\":1,\"68\":2}}],[\"interruptedexception\",{\"1\":{\"24\":3}}],[\"integercache会默认缓存\",{\"1\":{\"36\":1}}],[\"integercache\",{\"1\":{\"36\":4}}],[\"integer\",{\"1\":{\"24\":1,\"36\":13,\"37\":7,\"47\":1,\"68\":2}}],[\"int\",{\"1\":{\"4\":2,\"5\":1,\"8\":3,\"9\":7,\"13\":1,\"17\":1,\"24\":2,\"27\":3,\"28\":1,\"29\":1,\"36\":3,\"44\":2,\"45\":2,\"46\":5,\"47\":4,\"48\":1,\"49\":1,\"50\":3,\"66\":1,\"68\":3}}],[\"instanceof\",{\"1\":{\"25\":1,\"28\":1}}],[\"instance\",{\"1\":{\"4\":1}}],[\"当策略是以函数的形式存在时\",{\"1\":{\"221\":1}}],[\"当策略是以表格的形式保存时\",{\"1\":{\"221\":1}}],[\"当a为静止时\",{\"1\":{\"86\":1}}],[\"当执行动作at​时\",{\"1\":{\"86\":1}}],[\"当你使用内部类静态方法\",{\"1\":{\"64\":1}}],[\"当我们字符串编辑完成之后\",{\"1\":{\"54\":1}}],[\"当我们使用同一个包中的类时\",{\"1\":{\"16\":1}}],[\"当前数组长度为\",{\"1\":{\"45\":1}}],[\"当前类\",{\"1\":{\"17\":1}}],[\"当对象被判定为已经不再使用的\",{\"1\":{\"24\":1}}],[\"当然前提是方法定义\",{\"1\":{\"68\":1}}],[\"当然\",{\"1\":{\"7\":1,\"9\":3,\"16\":2,\"27\":1,\"55\":1,\"65\":1}}],[\"比如这里就是通过空格分隔\",{\"1\":{\"53\":1}}],[\"比如字符串的裁剪\",{\"1\":{\"53\":1}}],[\"比如说\",{\"1\":{\"28\":1}}],[\"比如我们要估计某个随机变量x的\",{\"1\":{\"172\":1}}],[\"比如我们现在不希望使用object类中提供的equals方法\",{\"1\":{\"25\":1}}],[\"比如我们经常访问的\",{\"1\":{\"16\":1}}],[\"比如com\",{\"1\":{\"16\":1}}],[\"可知\",{\"1\":{\"82\":1}}],[\"可变长参数\",{\"0\":{\"50\":1}}],[\"可以根据对各个状态的重要程度进行选择\",{\"1\":{\"225\":1}}],[\"可以最大化一个确定的常数指标\",{\"1\":{\"221\":1}}],[\"可以最开始均初始化为\",{\"1\":{\"120\":1}}],[\"可以描述为\",{\"1\":{\"221\":1}}],[\"可以写成\",{\"1\":{\"207\":1}}],[\"可以提高存储效率\",{\"1\":{\"204\":1}}],[\"可以参考\",{\"1\":{\"177\":1}}],[\"可以是\",{\"1\":{\"228\":1}}],[\"可以是标量\",{\"1\":{\"174\":1}}],[\"可以是跑步\",{\"1\":{\"30\":1}}],[\"可以平衡\",{\"1\":{\"160\":1}}],[\"可以拆分为多个\",{\"1\":{\"154\":1}}],[\"可以互相转化\",{\"1\":{\"121\":1}}],[\"可以得到一个序列v0​\",{\"1\":{\"120\":1}}],[\"可以求解\",{\"1\":{\"118\":1}}],[\"可以用来衡量一个状态的价值\",{\"1\":{\"113\":1}}],[\"可以用来检查一个串是否含有某种子串\",{\"1\":{\"55\":1}}],[\"可以通过\",{\"1\":{\"139\":1}}],[\"可以通过contraction\",{\"1\":{\"131\":1}}],[\"可以通过设置将episodic\",{\"1\":{\"92\":1}}],[\"可以通过这个成员内部类又创建出更多对象\",{\"1\":{\"61\":1}}],[\"可以粗步衡量一个策略的好坏\",{\"1\":{\"92\":1}}],[\"可以有限\",{\"1\":{\"92\":1}}],[\"可以有多个方法\",{\"1\":{\"67\":1}}],[\"可以忽略\",{\"1\":{\"83\":1}}],[\"可以表示为\",{\"1\":{\"82\":1,\"208\":1}}],[\"可以减轻无人机对用户接收到的干扰\",{\"1\":{\"82\":1}}],[\"可以直接使用lambda表达式\",{\"1\":{\"68\":1}}],[\"可以直接省去花括号和return关键字\",{\"1\":{\"67\":1}}],[\"可以在方法中定义\",{\"1\":{\"63\":1}}],[\"可以到\",{\"1\":{\"55\":1}}],[\"可以使用方括号\",{\"1\":{\"55\":1}}],[\"可以匹配\",{\"1\":{\"55\":1}}],[\"可以保存一个2字节的unicode字符\",{\"1\":{\"52\":1}}],[\"可以看到\",{\"1\":{\"51\":1}}],[\"可以传入\",{\"1\":{\"50\":1}}],[\"可以理解为\",{\"1\":{\"28\":1}}],[\"可以将复制一个完全一样的对象出来\",{\"1\":{\"24\":1}}],[\"可以相同\",{\"1\":{\"8\":1}}],[\"可见性\",{\"1\":{\"17\":1}}],[\"可写可不写\",{\"1\":{\"16\":1}}],[\"可能会创建各种各样的类\",{\"1\":{\"16\":1}}],[\"包括返回值\",{\"1\":{\"67\":1}}],[\"包括对方法的调用和super关键字的使用\",{\"1\":{\"61\":1}}],[\"包括对象内部的所有成员变量\",{\"1\":{\"29\":1}}],[\"包括对象的各个属性\",{\"1\":{\"24\":1}}],[\"包括换行\",{\"1\":{\"55\":1}}],[\"包括引用类型和基本类型\",{\"1\":{\"43\":1}}],[\"包装类支持字符串直接转换\",{\"1\":{\"37\":1}}],[\"包装类的方法\",{\"0\":{\"37\":1}}],[\"包装类型支持自动装箱\",{\"1\":{\"36\":1}}],[\"包装类型的自动装箱和拆箱机制\",{\"1\":{\"36\":1}}],[\"包装类实际上就是将我们的基本数据类型\",{\"1\":{\"36\":1}}],[\"包其实就是用来区分类位置的东西\",{\"1\":{\"16\":1}}],[\"包的命名规则同样是英文和数字的组合\",{\"1\":{\"16\":1}}],[\"包的声明和导入\",{\"0\":{\"16\":1}}],[\"包的访问与控制\",{\"0\":{\"15\":1}}],[\"所生成的数据\",{\"1\":{\"185\":1}}],[\"所求出的\",{\"1\":{\"139\":1}}],[\"所获得的均值\",{\"1\":{\"121\":1}}],[\"所得到的\",{\"1\":{\"113\":1}}],[\"所以状态其实共有\",{\"1\":{\"86\":1}}],[\"所以在无人机辅助通信网络中我们需要考虑qoe模型\",{\"1\":{\"83\":1}}],[\"所以只初始化了内部类\",{\"1\":{\"64\":1}}],[\"所以相对外部来说\",{\"1\":{\"62\":1}}],[\"所以可能不满足真实需求\",{\"1\":{\"45\":1}}],[\"所以\",{\"1\":{\"44\":1,\"121\":1}}],[\"所以说只能使用\",{\"1\":{\"68\":1}}],[\"所以说里面也可以有成员变量\",{\"1\":{\"61\":1}}],[\"所以说int类型的数组时不能被object类型的数组变量接收的\",{\"1\":{\"47\":1}}],[\"所以说可以直接向上转型\",{\"1\":{\"44\":1}}],[\"所以说==判断为假\",{\"1\":{\"36\":1}}],[\"所以说两个不同的对象\",{\"1\":{\"36\":1}}],[\"所以说照着写就行了\",{\"1\":{\"29\":1}}],[\"所以说有些人说接口其实就是java中的多继承\",{\"1\":{\"28\":1}}],[\"所以说不能为私有\",{\"1\":{\"27\":1}}],[\"所以说判断结果为真\",{\"1\":{\"25\":1}}],[\"所以说我们需要明确指定\",{\"1\":{\"16\":1}}],[\"所以说我们在静态方法中\",{\"1\":{\"13\":1}}],[\"所以说没有包这个概念\",{\"1\":{\"16\":1}}],[\"所以说静态内容一定会在第一个对象初始化之前完成加载\",{\"1\":{\"14\":1}}],[\"所以说\",{\"1\":{\"13\":1,\"68\":1}}],[\"所以说直接就使用了\",{\"1\":{\"7\":1}}],[\"所以说使用void\",{\"1\":{\"5\":1}}],[\"所有可能动作的\",{\"1\":{\"121\":1}}],[\"所有状态的集合\",{\"1\":{\"92\":1}}],[\"所有包装类如下\",{\"0\":{\"36\":1}}],[\"所有其他类都是继承它的\",{\"1\":{\"24\":1}}],[\"所有被标记为静态的内容\",{\"1\":{\"14\":1}}],[\"所处的包和对应的目录是一一对应的\",{\"1\":{\"16\":1}}],[\"其求解梯度比较难求\",{\"1\":{\"215\":1}}],[\"其定义都是一个均值\",{\"1\":{\"165\":1}}],[\"其探索性就很强\",{\"1\":{\"160\":1}}],[\"其属于\",{\"1\":{\"160\":1}}],[\"其原始定义都是从期望出发的\",{\"1\":{\"148\":1}}],[\"其核心思想是\",{\"1\":{\"148\":1}}],[\"其对应的\",{\"1\":{\"136\":1}}],[\"其策略π表示的是最优策略\",{\"1\":{\"128\":1}}],[\"其目标也应该不一样\",{\"1\":{\"86\":1}}],[\"其状态为其3d坐标\",{\"1\":{\"86\":1}}],[\"其高度的下界是距离dkn​​\",{\"1\":{\"82\":1}}],[\"其每个用户带宽表示为\",{\"1\":{\"82\":1}}],[\"其可用带宽为bn​\",{\"1\":{\"82\":1}}],[\"其水平坐标表示为qn​\",{\"1\":{\"81\":1}}],[\"其垂直高度表示为hn​\",{\"1\":{\"81\":1}}],[\"其坐标表示为wkn​​=\",{\"1\":{\"81\":1}}],[\"其实枚举类型的本质就是一个普通的类\",{\"1\":{\"30\":1}}],[\"其中st​是随机变量s的一个样本\",{\"1\":{\"209\":1}}],[\"其中w∈rm是参数向量\",{\"1\":{\"204\":1}}],[\"其中hk​=wk​\",{\"1\":{\"171\":1}}],[\"其中ak∗​=argmaxa​qπk​​\",{\"1\":{\"152\":1}}],[\"其中ak∗​\",{\"1\":{\"136\":1}}],[\"其中a∗表示在该状态下计算出来的最大\",{\"1\":{\"130\":1}}],[\"其中vk​是给定的\",{\"1\":{\"136\":1}}],[\"其中f\",{\"1\":{\"131\":1}}],[\"其中cmax​表示用户的最大速度\",{\"1\":{\"87\":1}}],[\"其中σ2=bkn​​n0​\",{\"1\":{\"82\":1}}],[\"其中k0​=\",{\"1\":{\"82\":1}}],[\"其中kn​表示划分到集群n的用户\",{\"1\":{\"81\":1}}],[\"其中θkn​​\",{\"1\":{\"82\":1}}],[\"其中用户表示为k=k1​\",{\"1\":{\"81\":1}}],[\"其中\",{\"1\":{\"55\":1,\"82\":1,\"83\":2,\"119\":1,\"139\":1,\"160\":1,\"169\":1,\"170\":1,\"174\":1,\"183\":1,\"185\":1,\"186\":1,\"189\":1,\"208\":1,\"214\":1,\"221\":1,\"226\":1,\"228\":1}}],[\"其中存放的每一个数据称为数组的一个元素\",{\"1\":{\"43\":1}}],[\"其中能够表示数字的基本类型包装类\",{\"1\":{\"36\":1}}],[\"其中public和abstract关键字可以省略\",{\"1\":{\"28\":1}}],[\"其中的\",{\"1\":{\"16\":1}}],[\"其子类的对应的方法的访问权限需要高于抽象类中的方法\",{\"1\":{\"27\":1}}],[\"其他的算法\",{\"1\":{\"229\":1}}],[\"其他的情况会在讲到反射时介绍\",{\"1\":{\"14\":1}}],[\"其他很多语言比如javascript\",{\"1\":{\"55\":1}}],[\"其他定义方法\",{\"1\":{\"44\":1}}],[\"其他地方不能修改\",{\"1\":{\"26\":1}}],[\"其变量名存储的是对象的引用\",{\"1\":{\"4\":1}}],[\"创建出来的数组每个位置上都有默认值\",{\"1\":{\"44\":1}}],[\"创建对象越多\",{\"1\":{\"36\":1}}],[\"创建枚举需要添加参数\",{\"1\":{\"30\":1}}],[\"创建类的实例\",{\"1\":{\"14\":1}}],[\"创建一个变量指代我们刚刚创建好的对象\",{\"1\":{\"4\":1}}],[\"优化方法\",{\"0\":{\"215\":1}}],[\"优化算法\",{\"0\":{\"209\":1}}],[\"优化目标函数的算法\",{\"1\":{\"205\":1}}],[\"优化问题\",{\"1\":{\"176\":1}}],[\"优化问题建立\",{\"0\":{\"84\":1}}],[\"优化\",{\"1\":{\"14\":1}}],[\"优先使用作用域最接近的\",{\"1\":{\"7\":1}}],[\"文件其实就是我们编写的一个类\",{\"1\":{\"14\":1}}],[\"文件丢给\",{\"1\":{\"14\":1}}],[\"去执行的\",{\"1\":{\"14\":1}}],[\"j\",{\"1\":{\"139\":2,\"141\":1,\"151\":1,\"174\":1,\"177\":1,\"206\":1,\"207\":1,\"208\":1,\"209\":1,\"214\":1,\"215\":3,\"221\":1,\"224\":1,\"228\":1,\"229\":1}}],[\"j+1\",{\"1\":{\"139\":2,\"141\":1}}],[\"jvm\",{\"1\":{\"14\":2}}],[\"java提供的基本类型包装类\",{\"1\":{\"35\":1}}],[\"java中没有字符串这种基本类型\",{\"1\":{\"52\":1}}],[\"java中的基本类型\",{\"1\":{\"35\":1}}],[\"java中引入了访问权限控制\",{\"1\":{\"17\":1}}],[\"java并不是纯面向对象的语言\",{\"1\":{\"35\":1}}],[\"java8开始\",{\"1\":{\"28\":1}}],[\"java会默认导入java\",{\"1\":{\"16\":1}}],[\"java\",{\"0\":{\"3\":1,\"12\":1,\"20\":1,\"33\":1,\"41\":1,\"58\":1,\"71\":1,\"235\":1},\"1\":{\"14\":1,\"16\":1,\"29\":1,\"30\":3,\"38\":3,\"51\":1,\"65\":2},\"2\":{\"11\":1,\"19\":1,\"32\":1,\"40\":1,\"57\":1,\"70\":1,\"75\":1}}],[\"因此该指标可以描述为\",{\"1\":{\"223\":1}}],[\"因此左侧那个类似\",{\"1\":{\"215\":1}}],[\"因此对应的损失函数的梯度可以修改为\",{\"1\":{\"215\":1}}],[\"因此对应算法为\",{\"1\":{\"211\":1}}],[\"因此对于用户kn​的在时刻t的传输速率rkn​​\",{\"1\":{\"82\":1}}],[\"因此需要用近似算法来进行替代\",{\"1\":{\"209\":1}}],[\"因此可以考虑\",{\"1\":{\"209\":1}}],[\"因此采用这种\",{\"1\":{\"207\":1}}],[\"因此这种情况下的\",{\"1\":{\"207\":1}}],[\"因此不需要进行\",{\"1\":{\"193\":1}}],[\"因此是一个确定的贪心策略\",{\"1\":{\"161\":1}}],[\"因此是2n\",{\"1\":{\"87\":1}}],[\"因此随着用户位置的变化\",{\"1\":{\"87\":1}}],[\"因此mos不仅与欧氏距离有关\",{\"1\":{\"86\":1}}],[\"因此moskn​​delay\",{\"1\":{\"83\":1}}],[\"因此gak\",{\"1\":{\"86\":1}}],[\"因此我们需要保证对于所有的\",{\"1\":{\"228\":1}}],[\"因此我们用\",{\"1\":{\"210\":1}}],[\"因此我们可以通过\",{\"1\":{\"172\":1}}],[\"因此我们将优化问题简化为区域分割问题\",{\"1\":{\"86\":1}}],[\"因此我们不需要手动指定\",{\"1\":{\"16\":1}}],[\"因此\",{\"1\":{\"53\":1,\"65\":2,\"82\":2,\"83\":1,\"99\":1,\"115\":1,\"121\":1,\"131\":1,\"144\":1,\"167\":1,\"174\":1,\"177\":2,\"183\":1,\"186\":1,\"197\":1}}],[\"因此只能使用类来进行定义\",{\"1\":{\"52\":1}}],[\"因此会读取命令行中的指令参数进行存储到\",{\"1\":{\"51\":1}}],[\"因为我们需要计算的是\",{\"1\":{\"228\":1}}],[\"因为不需要\",{\"1\":{\"198\":1}}],[\"因为最终策略更新的核心仍然是\",{\"1\":{\"152\":1}}],[\"因为\",{\"1\":{\"150\":1,\"177\":1,\"197\":1}}],[\"因为无论是\",{\"1\":{\"148\":1}}],[\"因为其满足该理论\",{\"1\":{\"131\":1}}],[\"因为其他类就算继承这个接口\",{\"1\":{\"28\":1}}],[\"因为在\",{\"1\":{\"165\":1}}],[\"因为在不考虑用户自由穿梭集群的情况\",{\"1\":{\"87\":1}}],[\"因为在编译的时候\",{\"1\":{\"64\":1}}],[\"因为q\",{\"1\":{\"86\":1}}],[\"因为目标函数对于无人机的3d坐标是非凸的\",{\"1\":{\"84\":1}}],[\"因为现在只需要一个string类型的返回值\",{\"1\":{\"68\":1}}],[\"因为integer类中默认提供了求两个int值之和的静态方法\",{\"1\":{\"68\":1}}],[\"因为匿名对象没有类名\",{\"1\":{\"65\":1}}],[\"因为它本质上就相当于是对应类型的子类\",{\"1\":{\"65\":1}}],[\"因为它作用范围就只是方法内\",{\"1\":{\"63\":1}}],[\"因为并没有使用到外部类的任何静态变量\",{\"1\":{\"64\":1}}],[\"因为成员内部类本身就是某个对象所有的\",{\"1\":{\"61\":1}}],[\"因为父类都是\",{\"1\":{\"47\":1}}],[\"因为同样是类\",{\"1\":{\"44\":1}}],[\"因为超出了缓存的范围\",{\"1\":{\"36\":1}}],[\"因为小的数使用频率非常高\",{\"1\":{\"36\":1}}],[\"因为包装类是一个类\",{\"1\":{\"36\":1}}],[\"因为底层是c++实现\",{\"1\":{\"29\":1}}],[\"因为抽象方法一定要由子类实现\",{\"1\":{\"27\":1}}],[\"因为都直接在一个缺省的包中\",{\"1\":{\"16\":1}}],[\"因为this关键字代表的是当前的对象本身\",{\"1\":{\"13\":1}}],[\"因为静态方法属于类的\",{\"1\":{\"13\":1}}],[\"因而也被称为实例\",{\"1\":{\"4\":1}}],[\"t=0∑∞​γtrt+1​\",{\"1\":{\"224\":1}}],[\"t=0\",{\"1\":{\"185\":1,\"189\":1}}],[\"td\",{\"0\":{\"184\":1,\"187\":1,\"188\":1,\"192\":1,\"200\":1,\"211\":1},\"1\":{\"186\":1,\"211\":1,\"215\":1,\"229\":1}}],[\"t∈rn\",{\"1\":{\"119\":2}}],[\"t∈r2×1\",{\"1\":{\"81\":2}}],[\"tabular\",{\"1\":{\"203\":1}}],[\"table过大\",{\"1\":{\"87\":1}}],[\"table来找出对应q\",{\"1\":{\"86\":1}}],[\"table\",{\"1\":{\"86\":2}}],[\"table管理\",{\"1\":{\"86\":1}}],[\"target\",{\"1\":{\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":2,\"215\":4}}],[\"take\",{\"1\":{\"159\":1}}],[\"taking\",{\"1\":{\"121\":1}}],[\"tasks转换成continuing\",{\"1\":{\"92\":1}}],[\"tasks\",{\"1\":{\"92\":3}}],[\"taobao\",{\"1\":{\"55\":2}}],[\"time\",{\"1\":{\"83\":1,\"154\":1}}],[\"timeout\",{\"1\":{\"24\":2}}],[\"truncated\",{\"0\":{\"142\":1,\"144\":1,\"145\":1},\"1\":{\"144\":1}}],[\"trial\",{\"1\":{\"92\":2}}],[\"trip\",{\"1\":{\"83\":1}}],[\"trajectory以及对应的\",{\"1\":{\"227\":1}}],[\"trajectory是在策略给定下\",{\"1\":{\"92\":1}}],[\"trajectory\",{\"1\":{\"92\":4,\"114\":1,\"115\":1,\"227\":1}}],[\"transition\",{\"1\":{\"92\":2,\"93\":1}}],[\"transmission\",{\"1\":{\"81\":1}}],[\"transactions\",{\"1\":{\"76\":1},\"2\":{\"89\":1}}],[\"t\",{\"1\":{\"81\":11,\"82\":20,\"83\":12,\"84\":8,\"86\":6}}],[\"terms\",{\"1\":{\"192\":1}}],[\"terminal\",{\"1\":{\"92\":1}}],[\"temporal\",{\"0\":{\"182\":1}}],[\"technology\",{\"1\":{\"76\":1},\"2\":{\"89\":1}}],[\"ten\",{\"1\":{\"38\":1}}],[\"teacher\",{\"1\":{\"28\":7}}],[\"test这个包中\",{\"1\":{\"16\":1}}],[\"test\",{\"1\":{\"13\":1,\"16\":7,\"17\":4,\"28\":2,\"30\":7,\"47\":1,\"50\":3,\"51\":1,\"61\":25,\"62\":6,\"63\":4,\"64\":11,\"65\":7,\"66\":1,\"67\":2}}],[\"tool\",{\"1\":{\"125\":1}}],[\"to\",{\"0\":{\"120\":1},\"1\":{\"77\":1,\"92\":1,\"93\":1,\"155\":1,\"159\":1,\"165\":1,\"171\":1}}],[\"tochararray\",{\"1\":{\"53\":1}}],[\"tohexstring\",{\"1\":{\"24\":1,\"37\":1}}],[\"tostring\",{\"1\":{\"24\":2,\"54\":2,\"61\":4}}],[\"that\",{\"1\":{\"114\":1,\"131\":1,\"154\":1}}],[\"then\",{\"1\":{\"131\":2}}],[\"theorem来求解贝尔曼最优公式\",{\"1\":{\"131\":1}}],[\"theorem\",{\"1\":{\"126\":2,\"131\":1,\"135\":1,\"171\":1}}],[\"the\",{\"0\":{\"97\":1,\"116\":1,\"117\":1},\"1\":{\"77\":1,\"82\":1,\"83\":1,\"92\":6,\"93\":4,\"110\":1,\"113\":1,\"114\":2,\"120\":1,\"121\":4,\"122\":2,\"125\":1,\"126\":2,\"131\":2,\"154\":1,\"155\":2,\"159\":1,\"171\":2,\"208\":1,\"215\":2}}],[\"throughput\",{\"1\":{\"77\":1}}],[\"throwable\",{\"1\":{\"24\":1}}],[\"throws\",{\"1\":{\"24\":5,\"29\":2}}],[\"this\",{\"0\":{\"7\":1},\"1\":{\"7\":1,\"9\":12,\"23\":1,\"24\":1,\"25\":3,\"27\":4,\"30\":2,\"61\":6,\"62\":1,\"63\":1,\"227\":1}}],[\"让\",{\"1\":{\"177\":1}}],[\"让类来使用这个接口\",{\"1\":{\"28\":1}}],[\"让我们的java程序更加生动形象\",{\"1\":{\"21\":1}}],[\"让我看看\",{\"1\":{\"13\":1}}],[\"让当前对象的name变量值等于参数传入的值\",{\"1\":{\"7\":1}}],[\"操作的都是同一个目标\",{\"1\":{\"13\":1}}],[\"关键元素\",{\"1\":{\"93\":1}}],[\"关键字\",{\"1\":{\"25\":1,\"44\":1}}],[\"关键字来声明一个变量或一个方法为静态的\",{\"1\":{\"13\":1}}],[\"关于对象类型的变量\",{\"1\":{\"4\":1}}],[\"certain\",{\"1\":{\"221\":1}}],[\"ceiling\",{\"1\":{\"38\":1}}],[\"critic\",{\"0\":{\"96\":1,\"97\":1,\"98\":1,\"102\":1,\"106\":1},\"1\":{\"96\":2}}],[\"choose\",{\"1\":{\"93\":1}}],[\"chapter\",{\"1\":{\"135\":1,\"141\":1}}],[\"chain\",{\"1\":{\"92\":1}}],[\"channel\",{\"1\":{\"82\":1}}],[\"chars\",{\"1\":{\"53\":2}}],[\"character\",{\"1\":{\"36\":1}}],[\"char\",{\"1\":{\"36\":1,\"52\":1,\"53\":1}}],[\"carlo\",{\"0\":{\"148\":1,\"210\":1},\"1\":{\"148\":1,\"229\":1}}],[\"can\",{\"1\":{\"114\":1,\"121\":2}}],[\"called\",{\"1\":{\"92\":1,\"154\":1,\"159\":1}}],[\"cache\",{\"1\":{\"36\":1}}],[\"cmax​\",{\"1\":{\"87\":1}}],[\"c\",{\"1\":{\"84\":1,\"86\":1}}],[\"cycles\",{\"1\":{\"83\":1}}],[\"c1​和c2​是通过分析web浏览应用程序的实验结果确定的常数\",{\"1\":{\"83\":1}}],[\"c是光速\",{\"1\":{\"82\":1}}],[\"c4πfc​​\",{\"1\":{\"82\":1}}],[\"closed\",{\"1\":{\"120\":1}}],[\"cloneable\",{\"1\":{\"29\":2}}],[\"clonenotsupportedexception\",{\"1\":{\"24\":1,\"29\":2}}],[\"clone\",{\"1\":{\"24\":2,\"29\":6}}],[\"class<\",{\"1\":{\"24\":1}}],[\"class\",{\"1\":{\"4\":1,\"5\":1,\"9\":4,\"13\":1,\"14\":2,\"16\":3,\"17\":2,\"24\":1,\"25\":1,\"27\":2,\"28\":2,\"29\":1,\"30\":2,\"50\":1,\"61\":7,\"62\":2,\"63\":3,\"64\":7,\"65\":4,\"165\":1}}],[\"coefficient\",{\"1\":{\"170\":1}}],[\"core\",{\"1\":{\"125\":1}}],[\"converges\",{\"1\":{\"171\":1}}],[\"convergence\",{\"1\":{\"131\":1}}],[\"convex问题\",{\"1\":{\"84\":1}}],[\"consider\",{\"1\":{\"131\":1}}],[\"considered\",{\"1\":{\"77\":1}}],[\"contractive\",{\"1\":{\"131\":1}}],[\"contraction\",{\"1\":{\"126\":2,\"131\":2,\"135\":1}}],[\"continuing\",{\"1\":{\"92\":1}}],[\"concepts\",{\"1\":{\"125\":1}}],[\"comments\",{\"1\":{\"143\":1}}],[\"compiled\",{\"1\":{\"30\":1}}],[\"com\",{\"1\":{\"16\":8,\"17\":1,\"30\":6,\"51\":1,\"55\":1,\"64\":4,\"65\":2}}],[\"code\",{\"0\":{\"236\":1},\"2\":{\"10\":1,\"18\":1,\"31\":1,\"39\":1,\"56\":1,\"69\":1,\"74\":1}}],[\"0<c1​≤▽w​g\",{\"1\":{\"171\":1}}],[\"0≤t≤ts​\",{\"1\":{\"81\":2,\"84\":2}}],[\"0xa6\",{\"1\":{\"37\":1}}],[\"0\",{\"1\":{\"9\":1,\"46\":1,\"48\":1,\"50\":1,\"53\":1,\"55\":5,\"86\":17,\"87\":1,\"92\":1,\"120\":1,\"160\":1,\"177\":1}}],[\"但在发表\",{\"1\":{\"217\":1}}],[\"但对于\",{\"1\":{\"215\":1}}],[\"但对象中的属性都是同一个地址\",{\"1\":{\"29\":1}}],[\"但这里还有一个难点\",{\"1\":{\"209\":1}}],[\"但实际情况可能并不是所有状态的概率都是一致的\",{\"1\":{\"207\":1}}],[\"但实际上我们往往是选择一个非常小的常数\",{\"1\":{\"171\":1}}],[\"但实际上还是在调用本身的方法\",{\"1\":{\"25\":1}}],[\"但能否保证其精确度\",{\"1\":{\"174\":1}}],[\"但能匹配\",{\"1\":{\"55\":1}}],[\"但需要大量的\",{\"1\":{\"174\":1}}],[\"但由于\",{\"1\":{\"174\":1}}],[\"但目前无法保证\",{\"1\":{\"157\":1}}],[\"但存在一定的浪费\",{\"1\":{\"154\":1}}],[\"但现实场景中不太经常使用\",{\"1\":{\"149\":1}}],[\"但若考虑集群情况\",{\"1\":{\"87\":1}}],[\"但每个无人机所管理的用户不同\",{\"1\":{\"86\":1}}],[\"但即使仅考虑用户聚类\",{\"1\":{\"86\":1}}],[\"但它继承自一个现有的类或实现了一个接口\",{\"1\":{\"65\":1}}],[\"但类名必须与接口一致\",{\"1\":{\"65\":1}}],[\"但我们可以在方法中使用匿名内部类\",{\"1\":{\"65\":1}}],[\"但还存在\",{\"1\":{\"229\":1}}],[\"但还是先初始化内部类\",{\"1\":{\"64\":1}}],[\"但还未复制\",{\"1\":{\"9\":1}}],[\"但不能匹配\",{\"1\":{\"55\":1}}],[\"但不推荐\",{\"1\":{\"44\":1}}],[\"但不支持\",{\"1\":{\"38\":1}}],[\"但是必须保证其他方法有默认实现\",{\"1\":{\"67\":1}}],[\"但是可以直接new了\",{\"1\":{\"62\":1}}],[\"但是可以存在静态变量和静态方法\",{\"1\":{\"28\":1}}],[\"但是能匹配\",{\"1\":{\"55\":1}}],[\"但是拼接字符串实际上底层需要进行很多操作\",{\"1\":{\"54\":1}}],[\"但是如果我们使用构造方法主动创建两个新的对象\",{\"1\":{\"53\":1}}],[\"但是如果我们在运行时\",{\"1\":{\"51\":1}}],[\"但是如果是引用类型的话\",{\"1\":{\"47\":1}}],[\"但是如果超出这个缓存范围的话\",{\"1\":{\"36\":1}}],[\"但是\",{\"1\":{\"45\":1}}],[\"但是只能在定义时赋值\",{\"1\":{\"44\":1}}],[\"但是编程不可见\",{\"1\":{\"44\":1}}],[\"但是biginteger没有这些限制\",{\"1\":{\"38\":1}}],[\"但是各位小伙伴只需要知道\",{\"1\":{\"36\":1}}],[\"但是并不是同一个对象\",{\"1\":{\"36\":1}}],[\"但是java中的基本数据类型却不是面向对象的\",{\"1\":{\"35\":1}}],[\"但是它继承自enum类\",{\"1\":{\"30\":1}}],[\"但是推荐打上\",{\"1\":{\"30\":1}}],[\"但是像这样的拷贝操作其实也分为浅拷贝和深拷贝\",{\"1\":{\"29\":1}}],[\"但是我个人认为这种说法是错的\",{\"1\":{\"28\":1}}],[\"但是我们也可以将接口实现类的对象以接口的形式去使用\",{\"1\":{\"28\":1}}],[\"但是我们发现\",{\"1\":{\"9\":1}}],[\"但是我们前面说了\",{\"1\":{\"4\":1}}],[\"但是同样需要有访问权限的情况下才可以\",{\"1\":{\"17\":1}}],[\"但是现在\",{\"1\":{\"16\":1}}],[\"但是静态方法是可以访问到静态变量的\",{\"1\":{\"13\":1}}],[\"但是仅返回类型不同\",{\"1\":{\"8\":1}}],[\"但是需要的形式参数不一样\",{\"1\":{\"8\":1}}],[\"但是当前作用域下只有对象属性的name变量\",{\"1\":{\"7\":1}}],[\"但是规则跟变量的命名差不多\",{\"1\":{\"5\":1}}],[\"此时可以分析每一个状态在这个策略下的概率\",{\"1\":{\"208\":1}}],[\"此时也可以是\",{\"1\":{\"197\":1}}],[\"此时就是随机策略\",{\"1\":{\"160\":1}}],[\"此时\",{\"1\":{\"119\":1,\"150\":1,\"198\":1}}],[\"此时对应的discountedrate=0+γ0+γ20+γ31+γ41+⋯=γ31−γ1​\",{\"1\":{\"92\":1}}],[\"此时该trajectory的return=0+0+0+1+1+⋯=∞\",{\"1\":{\"92\":1}}],[\"此时的mos模型定义如下\",{\"1\":{\"83\":1}}],[\"此时这里创建出来的student对象\",{\"1\":{\"65\":1}}],[\"此时我们怎么去明确要使用的是哪一个\",{\"1\":{\"61\":1}}],[\"此时由于三个属性完全一致\",{\"1\":{\"25\":1}}],[\"此时this\",{\"1\":{\"9\":1}}],[\"此时变量没有引用任何对象\",{\"1\":{\"4\":1}}],[\"而对应的真实梯度可以用一个估计的梯度来替代\",{\"1\":{\"229\":1}}],[\"而对应的策略梯度上升算法就是对应\",{\"1\":{\"96\":1}}],[\"而最小值问题\",{\"1\":{\"177\":1}}],[\"而言\",{\"1\":{\"143\":1}}],[\"而\",{\"1\":{\"114\":1,\"143\":1,\"144\":1}}],[\"而字符串则是一系列字符的序列\",{\"1\":{\"52\":1}}],[\"而且是\",{\"1\":{\"45\":1}}],[\"而且如果是子类\",{\"1\":{\"27\":1}}],[\"而bigdecimal可以实现小数的精确计算\",{\"1\":{\"38\":1}}],[\"而实际上指向的还是原来的那个对象\",{\"1\":{\"29\":1}}],[\"而如果抽象类定义的是\",{\"1\":{\"27\":1}}],[\"而具体的实现\",{\"1\":{\"27\":1}}],[\"而方法的重写是覆盖原有的方法实现\",{\"1\":{\"25\":1}}],[\"而当我们需要使用其他包中的类时\",{\"1\":{\"16\":1}}],[\"而每一个\",{\"1\":{\"14\":1}}],[\"而是等\",{\"1\":{\"215\":1}}],[\"而是对应的样本\",{\"1\":{\"172\":1}}],[\"而是在需要时才会去加载\",{\"1\":{\"14\":1}}],[\"而是在这之前就已经完成了\",{\"1\":{\"9\":1}}],[\"而是通过这个类去使用\",{\"1\":{\"13\":1}}],[\"而非\",{\"1\":{\"9\":1}}],[\"而不是仅考虑吞吐量\",{\"1\":{\"77\":1}}],[\"而不是类所有的\",{\"1\":{\"61\":1}}],[\"而不是在对象创建的时候分配\",{\"1\":{\"14\":1}}],[\"而不是具体的某个对象\",{\"1\":{\"13\":1}}],[\"而不是对象本身的复制\",{\"1\":{\"4\":1}}],[\"而不是本体\",{\"1\":{\"4\":1}}],[\"要求g\",{\"1\":{\"171\":1}}],[\"要证明加入baseline成立\",{\"1\":{\"99\":1}}],[\"要创建一个抽象类的实例对象\",{\"1\":{\"65\":1}}],[\"要使用抽象类\",{\"1\":{\"27\":1}}],[\"要是都不是这个类型还比什么\",{\"1\":{\"25\":1}}],[\"要给成员变量设定初始值\",{\"1\":{\"9\":1}}],[\"要在对象创建时进行处理\",{\"1\":{\"9\":1}}],[\"会一直进行更新\",{\"1\":{\"215\":1}}],[\"会与环境一直交互下去\",{\"1\":{\"92\":1}}],[\"会将其划分为n个簇\",{\"1\":{\"81\":1}}],[\"会隐式修改为\",{\"1\":{\"66\":1}}],[\"会报错\",{\"1\":{\"47\":1}}],[\"会直接复制值给拷贝对象\",{\"1\":{\"29\":1}}],[\"会限制其子类不允许其重写所对应的成员变量\",{\"1\":{\"26\":1}}],[\"会由jvm来调用一次此方法进行资源释放之类的操作\",{\"1\":{\"24\":1}}],[\"会在类刚加载的时候就分配\",{\"1\":{\"14\":1}}],[\"会覆盖掉默认的那一个无参构造方法\",{\"1\":{\"9\":1}}],[\"会出现异常\",{\"1\":{\"4\":1}}],[\"在状态s采用动作a\",{\"1\":{\"226\":1}}],[\"在该策略下的所有\",{\"1\":{\"221\":1}}],[\"在该文中考虑的是网页浏览应用传输情况\",{\"1\":{\"83\":1}}],[\"在该文中\",{\"1\":{\"83\":1}}],[\"在原文是\",{\"1\":{\"217\":1}}],[\"在训练求解梯度时\",{\"1\":{\"215\":1}}],[\"在每一次迭代时\",{\"1\":{\"215\":1}}],[\"在初始化的时候是设为相同的\",{\"1\":{\"215\":1}}],[\"在初始时间假设用户处于静止下不断调整\",{\"1\":{\"78\":1}}],[\"在将\",{\"1\":{\"215\":1}}],[\"在计算\",{\"1\":{\"215\":1}}],[\"在通过经验来更新这个策略\",{\"1\":{\"195\":1}}],[\"在之前关于使用\",{\"1\":{\"178\":1}}],[\"在这里\",{\"1\":{\"160\":1}}],[\"在收集到了足够多的\",{\"1\":{\"155\":1}}],[\"在求解\",{\"1\":{\"144\":1}}],[\"在策略更新上\",{\"1\":{\"143\":1}}],[\"在策略梯度算法中引入一个\",{\"1\":{\"99\":1}}],[\"在当前状态s下\",{\"1\":{\"121\":1}}],[\"在当前状态s下采取动作\",{\"1\":{\"121\":1}}],[\"在当前包以外的其他包中无法访问\",{\"1\":{\"17\":1}}],[\"在实际情况中\",{\"1\":{\"100\":1}}],[\"在实际应用中\",{\"1\":{\"82\":1}}],[\"在policy是确定的情况下\",{\"1\":{\"93\":1}}],[\"在执行一个动作后获得的一个常数\",{\"1\":{\"92\":1}}],[\"在此情况下\",{\"1\":{\"87\":1}}],[\"在本文中\",{\"1\":{\"87\":1}}],[\"在本文中不考虑用户移动到其他集群的情况\",{\"1\":{\"87\":1}}],[\"在设计无人机的移动之前\",{\"1\":{\"87\":1}}],[\"在时刻t关联到无人机n的地面用户kn​的接受到的信噪比表示为\",{\"1\":{\"82\":1}}],[\"在时间t\",{\"1\":{\"82\":1}}],[\"在任意时刻t\",{\"1\":{\"81\":1}}],[\"在new的时候\",{\"1\":{\"65\":1}}],[\"在内部类中使用this关键字\",{\"1\":{\"61\":1}}],[\"在成员内部类中\",{\"1\":{\"61\":1}}],[\"在成员变量初始化之后执行\",{\"1\":{\"9\":1}}],[\"在c中就是一个字符数组\",{\"1\":{\"52\":1}}],[\"在运行时动态创建\",{\"1\":{\"44\":1}}],[\"在\",{\"1\":{\"26\":1,\"65\":1,\"98\":1,\"141\":2,\"159\":1,\"211\":1,\"215\":1}}],[\"在修改后\",{\"1\":{\"25\":1}}],[\"在回收之前\",{\"1\":{\"24\":1}}],[\"在不同包下的类\",{\"1\":{\"16\":1}}],[\"在放入包中\",{\"1\":{\"16\":1}}],[\"在静态方法中\",{\"1\":{\"13\":1}}],[\"在赋值之前看看是否有初始值\",{\"1\":{\"9\":1}}],[\"在我们自己定义一个构造方法之后\",{\"1\":{\"9\":1}}],[\"在创建了对象之后\",{\"1\":{\"4\":1}}],[\"男\",{\"1\":{\"9\":2,\"25\":4,\"28\":1,\"29\":1,\"30\":1}}],[\"只给出了梯度的公式\",{\"1\":{\"228\":1}}],[\"只不过我们此时需要遍历所有的\",{\"1\":{\"152\":1}}],[\"只不过根据区域划分\",{\"1\":{\"86\":1}}],[\"只不过还能更简单\",{\"1\":{\"68\":1}}],[\"只不过意义不大\",{\"1\":{\"65\":1}}],[\"只不过它比较特殊\",{\"1\":{\"53\":1}}],[\"只是进行了一步求解\",{\"1\":{\"144\":1}}],[\"只是求解各状态的\",{\"1\":{\"129\":1}}],[\"只是初始化了内部类的\",{\"1\":{\"64\":1}}],[\"只是数组的地址不准修改\",{\"1\":{\"48\":1}}],[\"只是语法上为了简单\",{\"1\":{\"36\":1}}],[\"只是不用我们去写\",{\"1\":{\"9\":1}}],[\"只会复制对象的地址\",{\"1\":{\"29\":1}}],[\"只要是实现这个接口的类\",{\"1\":{\"28\":1}}],[\"只要一个类的父类或者自身有对应方法\",{\"1\":{\"28\":1}}],[\"只保留方法的定义\",{\"1\":{\"27\":1}}],[\"只能使用\",{\"1\":{\"66\":1}}],[\"只能对其进行继承\",{\"1\":{\"65\":1}}],[\"只能表示内部类对象\",{\"1\":{\"61\":1}}],[\"只能重新创建\",{\"1\":{\"45\":1,\"52\":1}}],[\"只能\",{\"1\":{\"38\":1}}],[\"只能内部使用\",{\"1\":{\"30\":1}}],[\"只能作为一个附属功能加在主体上\",{\"1\":{\"28\":1}}],[\"只能在构造函数进行赋值\",{\"1\":{\"26\":1}}],[\"只能得到\",{\"1\":{\"25\":1,\"27\":1}}],[\"只能被类本身和同包中的其他类访问\",{\"1\":{\"17\":1}}],[\"只有当样本全部收集完才能估计\",{\"1\":{\"167\":1}}],[\"只有当所有\",{\"1\":{\"155\":1}}],[\"只有当所有东西都是确定性的\",{\"1\":{\"114\":1}}],[\"只有在你使用到外部类的静态变量或方法后\",{\"1\":{\"64\":1}}],[\"只有在类不在同一个包下时才需要进行导入\",{\"1\":{\"16\":1}}],[\"只有抽象类中的抽象方法\",{\"1\":{\"28\":1}}],[\"只有是当前类型的对象\",{\"1\":{\"25\":1}}],[\"只需要保证\",{\"1\":{\"99\":1}}],[\"只需要用\",{\"1\":{\"28\":1}}],[\"只需要在类名前面添加包名就行了\",{\"1\":{\"16\":1}}],[\"只需要\",{\"1\":{\"5\":1}}],[\"默认private\",{\"1\":{\"30\":1}}],[\"默认实现是直接用等号判断\",{\"1\":{\"24\":1}}],[\"默认的情况下\",{\"1\":{\"17\":1}}],[\"默认\",{\"1\":{\"17\":2}}],[\"默认情况下直接运行什么都没有\",{\"1\":{\"51\":1}}],[\"默认情况下格式为\",{\"1\":{\"24\":1}}],[\"默认情况下\",{\"1\":{\"17\":1}}],[\"默认情况下包名是可以省略的\",{\"1\":{\"16\":1}}],[\"默认情况下每个类都会自带一个没有任何参数的无参构造方法\",{\"1\":{\"9\":1}}],[\"默认值为false\",{\"1\":{\"4\":1}}],[\"并非只是一个单一的轨迹\",{\"1\":{\"92\":1}}],[\"并非是所对应的对象本身\",{\"1\":{\"4\":1}}],[\"并获得奖励rt​的这一过程可以用条件转移概率p\",{\"1\":{\"86\":1}}],[\"并与传统的基于遗传的学习算法进行对比\",{\"1\":{\"78\":1}}],[\"并直接创建实例对象\",{\"1\":{\"65\":1}}],[\"并没有初始化内部类\",{\"1\":{\"64\":1}}],[\"并返回一个新的子串对象\",{\"1\":{\"53\":1}}],[\"并根据实际需求扩展出新的行为\",{\"1\":{\"21\":1}}],[\"并提供对外访问的接口\",{\"1\":{\"21\":1}}],[\"并且如果按照这样一个策略\",{\"1\":{\"159\":1}}],[\"并且与状态\",{\"1\":{\"121\":1}}],[\"并且接口没有继承数量限制\",{\"1\":{\"28\":1}}],[\"并且默认在类中实现的权限是\",{\"1\":{\"28\":1}}],[\"并且也导入了我们自己定义的string类\",{\"1\":{\"16\":1}}],[\"并且方法名称与类名相同\",{\"1\":{\"9\":2}}],[\"并不是一直进行更新\",{\"1\":{\"215\":1}}],[\"并不是说只有抽象类和接口才可以像这样创建匿名内部类\",{\"1\":{\"65\":1}}],[\"并不是基本数据类型\",{\"1\":{\"44\":1}}],[\"并不是在构造方法之后\",{\"1\":{\"9\":1}}],[\"并不编写方法的主体\",{\"1\":{\"27\":1}}],[\"并不会在一开始就去加载它\",{\"1\":{\"14\":1}}],[\"来定义最优的策略\",{\"1\":{\"221\":1}}],[\"来更新参数值\",{\"1\":{\"215\":1}}],[\"来近似\",{\"1\":{\"210\":1,\"211\":1}}],[\"来进行求解\",{\"1\":{\"150\":1}}],[\"来进行迭代\",{\"1\":{\"86\":1}}],[\"来提升当前策略\",{\"1\":{\"139\":1}}],[\"来保证这个梯度的方差最小即可\",{\"1\":{\"99\":1}}],[\"来表示\",{\"1\":{\"86\":1}}],[\"来最大化mos总和\",{\"1\":{\"86\":1}}],[\"来考虑无人机的机动性\",{\"1\":{\"77\":1}}],[\"来实现\",{\"1\":{\"27\":1}}],[\"来完成\",{\"1\":{\"9\":1}}],[\"来明确表示当前类的示例对象本身\",{\"1\":{\"7\":1}}],[\"构造函数也不能赋值\",{\"1\":{\"26\":1}}],[\"构造器\",{\"1\":{\"9\":1}}],[\"构造方法也可以被引用\",{\"1\":{\"68\":1}}],[\"构造方法会在new的时候自动执行\",{\"1\":{\"9\":1}}],[\"构造方法会在对象创建时执行\",{\"1\":{\"9\":1}}],[\"构造方法不需要指定返回值\",{\"1\":{\"9\":1}}],[\"构造方法不需要填写返回值\",{\"1\":{\"9\":1}}],[\"构造方法\",{\"0\":{\"9\":1},\"1\":{\"9\":1}}],[\"各种属性都是默认值\",{\"1\":{\"9\":1}}],[\"buffer\",{\"0\":{\"216\":1},\"1\":{\"215\":1}}],[\"builder\",{\"1\":{\"54\":8}}],[\"bgd\",{\"0\":{\"179\":1},\"1\":{\"174\":1}}],[\"broad\",{\"1\":{\"165\":1}}],[\"batch\",{\"1\":{\"174\":1,\"215\":2}}],[\"based\",{\"0\":{\"161\":1},\"1\":{\"150\":1,\"155\":1,\"220\":2,\"229\":1}}],[\"baseline\",{\"0\":{\"99\":1,\"100\":1},\"1\":{\"98\":1,\"99\":2,\"100\":1}}],[\"basic\",{\"0\":{\"149\":1},\"1\":{\"153\":1,\"154\":1,\"161\":1}}],[\"baidu\",{\"1\":{\"16\":3}}],[\"bit\",{\"1\":{\"83\":2}}],[\"bigdecimal\",{\"1\":{\"38\":6}}],[\"biginteger\",{\"1\":{\"38\":10}}],[\"bkn​​=bn​\",{\"1\":{\"82\":1}}],[\"b2​\",{\"1\":{\"82\":1}}],[\"b2​pnlos​=1−plos​\",{\"1\":{\"82\":1}}],[\"b1​\",{\"1\":{\"82\":2}}],[\"behavior\",{\"1\":{\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"208\":1}}],[\"bellman\",{\"0\":{\"115\":1,\"118\":1,\"119\":1,\"127\":1},\"1\":{\"110\":1,\"119\":2,\"120\":1,\"122\":2,\"125\":1,\"139\":1,\"141\":2,\"143\":1,\"144\":1,\"150\":1,\"186\":3,\"188\":1,\"192\":1}}],[\"be\",{\"1\":{\"77\":1,\"114\":1}}],[\"boe\",{\"0\":{\"127\":1},\"1\":{\"125\":1,\"131\":1}}],[\"bootstrapping\",{\"1\":{\"118\":1}}],[\"boolean\",{\"1\":{\"24\":1,\"25\":1,\"36\":2}}],[\"bob\",{\"1\":{\"55\":2}}],[\"bbb\",{\"1\":{\"54\":1}}],[\"byte类型的包装类也有类似的机制\",{\"1\":{\"36\":1}}],[\"byte\",{\"1\":{\"36\":2}}],[\"b\",{\"1\":{\"8\":4,\"36\":6,\"50\":1,\"61\":2,\"68\":5,\"99\":1}}],[\"也称为\",{\"1\":{\"113\":1}}],[\"也支持向下转型\",{\"1\":{\"47\":1}}],[\"也只能表示64bit的数据\",{\"1\":{\"38\":1}}],[\"也只有这一个静态的变量或方法\",{\"1\":{\"13\":1}}],[\"也会进行拷贝\",{\"1\":{\"29\":1}}],[\"也就是只包含方法的定义\",{\"1\":{\"28\":1}}],[\"也就是直接判断是否为同一个对象\",{\"1\":{\"24\":1}}],[\"也就是说依然是采用的object中的默认实现\",{\"1\":{\"45\":1}}],[\"也就是说数组的长度一旦确定\",{\"1\":{\"45\":1}}],[\"也就是说这个方法只有定义\",{\"1\":{\"27\":1}}],[\"也就是说\",{\"1\":{\"13\":1}}],[\"也是一样的\",{\"1\":{\"61\":1}}],[\"也是以对象的形式存在的\",{\"1\":{\"44\":1}}],[\"也是支持拆箱的\",{\"1\":{\"36\":1}}],[\"也是不同的两个类\",{\"1\":{\"16\":1}}],[\"也是尽量使用小写字母开头的单词\",{\"1\":{\"5\":1}}],[\"也可以采用基于\",{\"1\":{\"229\":1}}],[\"也可以是向量\",{\"1\":{\"174\":1}}],[\"也可以是无限长的trajectory\",{\"1\":{\"92\":1}}],[\"也可以象征性\",{\"1\":{\"53\":1}}],[\"也可以被子类访问\",{\"1\":{\"17\":1}}],[\"也可以用来将我们的类进行分类\",{\"1\":{\"16\":1}}],[\"也可以理解为是所有对象共享的内容\",{\"1\":{\"13\":1}}],[\"也可以直接在定义时赋值\",{\"1\":{\"9\":1}}],[\"也可以不同\",{\"1\":{\"8\":1}}],[\"一些特性\",{\"1\":{\"228\":1}}],[\"一些细节\",{\"1\":{\"215\":1,\"229\":1}}],[\"一些状态可能很少被访问\",{\"1\":{\"207\":1}}],[\"一些问题\",{\"0\":{\"141\":1}}],[\"一致也是可以的\",{\"1\":{\"197\":1}}],[\"一定可以遍历所给定的\",{\"1\":{\"157\":1}}],[\"一定要用equals\",{\"1\":{\"53\":1}}],[\"一样\",{\"1\":{\"155\":1}}],[\"一样才行\",{\"1\":{\"68\":1}}],[\"一种则是有所偏向\",{\"1\":{\"225\":1}}],[\"一种是将所有状态视为同等重要\",{\"1\":{\"225\":1}}],[\"一种是通过迭代算法来求解\",{\"1\":{\"141\":1}}],[\"一种是可以直接通过矩阵求逆进行求解\",{\"1\":{\"141\":1}}],[\"一种迭代策略\",{\"1\":{\"120\":1}}],[\"一开始创建时\",{\"1\":{\"54\":1}}],[\"一般化的推广\",{\"1\":{\"142\":1}}],[\"一般情况\",{\"1\":{\"38\":1}}],[\"一般情况下只是为了进行一些额外的初始化工作而已\",{\"1\":{\"65\":1}}],[\"一般情况下\",{\"1\":{\"13\":1}}],[\"一般遇到以下情况时才会会加载类\",{\"1\":{\"14\":1}}],[\"一般使用驼峰命名法最规范\",{\"1\":{\"5\":1}}],[\"一个\",{\"1\":{\"176\":1}}],[\"一个成员内部类\",{\"1\":{\"61\":1}}],[\"一个三行两列的数组\",{\"1\":{\"49\":1}}],[\"一个类可以附加很多个功能\",{\"1\":{\"28\":1}}],[\"一个类中可以包含多个同名的方法\",{\"1\":{\"8\":1}}],[\"一个对象改变了静态变量的值\",{\"1\":{\"13\":1}}],[\"一旦被声明为静态\",{\"1\":{\"13\":1}}],[\"有表示\",{\"1\":{\"228\":1}}],[\"有\",{\"1\":{\"152\":1}}],[\"有关\",{\"1\":{\"121\":1,\"225\":1}}],[\"有关注解我们会在最后一章进行介绍\",{\"1\":{\"25\":1}}],[\"有关锁的内容\",{\"1\":{\"24\":1}}],[\"有着不同程度的访问限制\",{\"1\":{\"17\":1}}],[\"有点混乱\",{\"1\":{\"16\":1}}],[\"有些时候并不需要创建那么多对象\",{\"1\":{\"36\":1}}],[\"有些时候\",{\"1\":{\"8\":1}}],[\"有时候我们的方法中可能会出现一些与成员变量重名的变量\",{\"1\":{\"7\":1}}],[\"1−∣a\",{\"1\":{\"160\":1}}],[\"1234\",{\"1\":{\"106\":1}}],[\"120和4\",{\"1\":{\"83\":1}}],[\"128~127之间的值自动装箱为integer类型的对象\",{\"1\":{\"36\":1}}],[\"128~127之间的所有值\",{\"1\":{\"36\":1}}],[\"128\",{\"1\":{\"36\":4}}],[\"1~2\",{\"1\":{\"83\":1}}],[\"1+σ2pkn​​gkn​​\",{\"1\":{\"82\":1}}],[\"1+ns​\",{\"1\":{\"82\":1}}],[\"1\",{\"0\":{\"92\":1,\"97\":1,\"99\":1,\"103\":1,\"111\":1,\"112\":2,\"113\":1,\"114\":1,\"116\":1,\"126\":1,\"128\":1,\"130\":1,\"135\":1,\"136\":2,\"137\":1,\"139\":1,\"143\":1,\"149\":1,\"150\":2,\"151\":1,\"152\":1,\"154\":1,\"159\":1,\"166\":1,\"167\":2,\"169\":1,\"174\":1,\"183\":1,\"185\":1,\"189\":1,\"193\":1,\"204\":1,\"206\":1,\"221\":1,\"223\":1},\"1\":{\"48\":1,\"49\":2,\"50\":1,\"55\":5,\"83\":1,\"86\":7,\"87\":1,\"92\":1,\"139\":1,\"143\":1,\"152\":2,\"160\":1,\"171\":2,\"174\":1,\"185\":1,\"189\":1}}],[\"166\",{\"1\":{\"37\":1}}],[\"1static\",{\"1\":{\"28\":1}}],[\"180π​\",{\"1\":{\"82\":1}}],[\"18\",{\"1\":{\"5\":1,\"9\":1,\"25\":4,\"28\":1,\"29\":1,\"30\":1}}],[\"10​a=ak∗​\",{\"1\":{\"136\":1,\"139\":1}}],[\"10​a=a∗a=a∗​\",{\"1\":{\"130\":1}}],[\"100\",{\"1\":{\"38\":2}}],[\"10\",{\"1\":{\"4\":1,\"9\":1,\"36\":5,\"38\":1,\"44\":1,\"45\":1,\"46\":1,\"47\":4,\"66\":1,\"67\":2,\"68\":2}}],[\"小王\",{\"1\":{\"28\":1}}],[\"小明\",{\"1\":{\"5\":1,\"9\":1,\"25\":4,\"29\":1,\"30\":1,\"61\":1}}],[\"小红\",{\"1\":{\"4\":1,\"61\":1}}],[\"岁了\",{\"1\":{\"5\":1}}],[\"+η▽w​f\",{\"1\":{\"177\":1}}],[\"+η​\",{\"1\":{\"172\":1,\"183\":1}}],[\"+ηk​\",{\"1\":{\"170\":1}}],[\"+γs\",{\"1\":{\"119\":1,\"150\":1}}],[\"+γe\",{\"1\":{\"115\":1,\"118\":1}}],[\"+rtt−rkn​​\",{\"1\":{\"83\":1}}],[\"+c2​\",{\"1\":{\"83\":1}}],[\"+ζ2​moskn​​rate\",{\"1\":{\"83\":1}}],[\"+test\",{\"1\":{\"61\":1}}],[\"+this\",{\"1\":{\"61\":1}}],[\"+name\",{\"1\":{\"61\":2}}],[\"+name+\",{\"1\":{\"5\":1}}],[\"+表示对前面这个字符匹配一次或多次\",{\"1\":{\"55\":1}}],[\"+=\",{\"1\":{\"54\":1}}],[\"+a\",{\"1\":{\"67\":5}}],[\"+array\",{\"1\":{\"45\":1}}],[\"+age+\",{\"1\":{\"5\":1}}],[\"+\",{\"1\":{\"8\":2,\"24\":2,\"36\":1,\"46\":1,\"54\":5,\"55\":2,\"64\":1,\"68\":1,\"81\":1,\"172\":1,\"183\":1}}],[\"果直接创建对象\",{\"1\":{\"4\":1}}],[\"变量名称\",{\"1\":{\"44\":4}}],[\"变量的值就是当前对象的存放值\",{\"1\":{\"5\":1}}],[\"变量的类型就是对应的类名\",{\"1\":{\"4\":1}}],[\"变量使用之前需要先赋值\",{\"1\":{\"4\":1}}],[\"就需要新的算法进行解决\",{\"1\":{\"169\":1}}],[\"就需要注意了\",{\"1\":{\"16\":1}}],[\"就直接去更新策略\",{\"1\":{\"155\":1}}],[\"就好了\",{\"1\":{\"136\":1}}],[\"就会报错\",{\"1\":{\"66\":1}}],[\"就会得到不同的对象了\",{\"1\":{\"36\":1}}],[\"就近原则\",{\"1\":{\"61\":1}}],[\"就表示这个是一个数组类型\",{\"1\":{\"44\":1}}],[\"就没办法了\",{\"1\":{\"38\":1}}],[\"就支持像这样编写\",{\"1\":{\"36\":1}}],[\"就不太可行\",{\"1\":{\"207\":1}}],[\"就不需要是\",{\"1\":{\"198\":1}}],[\"就不能确保所选择的\",{\"1\":{\"157\":1}}],[\"就不会执行接口的默认方法\",{\"1\":{\"28\":1}}],[\"就不一定需要实现\",{\"1\":{\"27\":1}}],[\"就像使用普通类型那样\",{\"1\":{\"30\":1}}],[\"就像在这个类定义的方法一样\",{\"1\":{\"17\":1}}],[\"就像下面这样\",{\"1\":{\"13\":1}}],[\"就是\",{\"1\":{\"160\":1}}],[\"就是前面\",{\"1\":{\"151\":1}}],[\"就是进行迭代\",{\"1\":{\"144\":1}}],[\"就是将一个已实现的方法\",{\"1\":{\"68\":1}}],[\"就是一个已经实现了抽象方法的对象\",{\"1\":{\"65\":1}}],[\"就是创建在内部的类\",{\"1\":{\"60\":1}}],[\"就是专门用于构造字符串的\",{\"1\":{\"54\":1}}],[\"就是0\",{\"1\":{\"44\":1}}],[\"就是null\",{\"1\":{\"44\":1}}],[\"就是精确到最后一位时\",{\"1\":{\"38\":1}}],[\"就是用于分割的\",{\"1\":{\"16\":1}}],[\"就是域名\",{\"1\":{\"16\":1}}],[\"就是要操作\",{\"1\":{\"4\":1}}],[\"就可以直接就创出对象\",{\"1\":{\"65\":1}}],[\"就可以使用tostring转换为字符串了\",{\"1\":{\"54\":1}}],[\"就可以表示这个是哪一个包里的类了\",{\"1\":{\"16\":1}}],[\"就可以执行定义好的方法了\",{\"1\":{\"5\":1}}],[\"就可以进行一定操作\",{\"1\":{\"4\":1}}],[\"我会学习\",{\"1\":{\"28\":1,\"29\":1}}],[\"我是学习方法\",{\"1\":{\"67\":2}}],[\"我是匿名内部类的实现\",{\"1\":{\"65\":1}}],[\"我是局部内部类\",{\"1\":{\"63\":1}}],[\"我是静态内部类\",{\"1\":{\"62\":1}}],[\"我是静态方法\",{\"1\":{\"13\":1,\"17\":1}}],[\"我是成员内部类\",{\"1\":{\"61\":2}}],[\"我是默认实现\",{\"1\":{\"28\":1}}],[\"我是工人\",{\"1\":{\"25\":1,\"27\":1}}],[\"我是代码块\",{\"1\":{\"9\":1}}],[\"我被构造了\",{\"1\":{\"9\":1}}],[\"我叫\",{\"1\":{\"5\":1}}],[\"我任性\",{\"1\":{\"4\":1}}],[\"我们便称为\",{\"1\":{\"229\":1}}],[\"我们定义\",{\"1\":{\"221\":1}}],[\"我们定义最优的策略为\",{\"1\":{\"221\":1}}],[\"我们定义的每一个状态其实就是一个public\",{\"1\":{\"30\":1}}],[\"我们先直接求解\",{\"1\":{\"215\":1}}],[\"我们是无法估计的\",{\"1\":{\"209\":1}}],[\"我们是引入了\",{\"1\":{\"178\":1}}],[\"我们很难直接获得\",{\"1\":{\"174\":1}}],[\"我们有以下几种方法\",{\"1\":{\"174\":1}}],[\"我们得到的观测值是\",{\"1\":{\"172\":1}}],[\"我们就可以通过\",{\"1\":{\"177\":1}}],[\"我们就可以得到\",{\"1\":{\"172\":1}}],[\"我们就可以使用biginteger来完成\",{\"1\":{\"38\":1}}],[\"我们采用的是\",{\"1\":{\"160\":1}}],[\"我们选取其\",{\"1\":{\"152\":1}}],[\"我们仍需要估计\",{\"1\":{\"152\":1}}],[\"我们不断地与环境进行交互\",{\"1\":{\"208\":1}}],[\"我们不能直接得到随机变量的值\",{\"1\":{\"172\":1}}],[\"我们不能通过之前的方法来求出q\",{\"1\":{\"150\":1}}],[\"我们不仅可以用来估计q\",{\"1\":{\"154\":1}}],[\"我们不仅可以通过构造方法\",{\"1\":{\"9\":1}}],[\"我们这里强行初始化为vπ0​​\",{\"1\":{\"143\":1}}],[\"我们还需要理解其所描述的最优策略π∗\",{\"1\":{\"129\":1}}],[\"我们还可以使用方法引用\",{\"1\":{\"67\":1}}],[\"我们通常将\",{\"1\":{\"100\":1}}],[\"我们通过使用\",{\"1\":{\"13\":1}}],[\"我们需要从经验池\",{\"1\":{\"215\":1}}],[\"我们需要思考使用\",{\"1\":{\"177\":1}}],[\"我们需要保证策略是不断提升\",{\"1\":{\"141\":1}}],[\"我们需要找到一个\",{\"1\":{\"99\":1}}],[\"我们需要明确指定一下\",{\"1\":{\"16\":1}}],[\"我们对无人机的发射功率有一个约束\",{\"1\":{\"82\":1}}],[\"我们同样需要使用对象来进行方法引用\",{\"1\":{\"68\":1}}],[\"我们首先需要创建对象\",{\"1\":{\"61\":1}}],[\"我们一般只会在类的内部自己使用\",{\"1\":{\"61\":1}}],[\"我们一般称为限定符\",{\"1\":{\"55\":1}}],[\"我们知道\",{\"1\":{\"52\":1}}],[\"我们要创建一个数组\",{\"1\":{\"44\":1}}],[\"我们如果直接让\",{\"1\":{\"36\":1}}],[\"我们如果想要在方法中访问到当前对象的属性\",{\"1\":{\"7\":1}}],[\"我们发现\",{\"1\":{\"36\":1}}],[\"我们直接调用父类的实现就可以了\",{\"1\":{\"29\":1}}],[\"我们直接使用成员变量即可\",{\"1\":{\"5\":1}}],[\"我们只能通过\",{\"1\":{\"170\":1}}],[\"我们只能去创建它的子类对象\",{\"1\":{\"27\":1}}],[\"我们只需要挑选在当前迭代下最大的\",{\"1\":{\"136\":1}}],[\"我们只需要在当前状态下\",{\"1\":{\"130\":1}}],[\"我们只需要在类名前面把完整的包名也给写上\",{\"1\":{\"16\":1}}],[\"我们只需要使用\",{\"1\":{\"5\":1}}],[\"我们会在第六章多线程部分中讲解\",{\"1\":{\"24\":1}}],[\"我们会在第五章集合类中使用到\",{\"1\":{\"24\":1}}],[\"我们会在jvm篇视频教程中进行介绍\",{\"1\":{\"24\":1}}],[\"我们将\",{\"1\":{\"211\":1}}],[\"我们将类放到包中\",{\"1\":{\"16\":1}}],[\"我们将变量p2赋值为p1的值\",{\"1\":{\"4\":1}}],[\"我们之前都是直接创建的类\",{\"1\":{\"16\":1}}],[\"我们的包就可以命名为com\",{\"1\":{\"16\":1}}],[\"我们的方法需要能够同时应对多种情况\",{\"1\":{\"8\":1}}],[\"我们在重写父类方法时\",{\"1\":{\"25\":1}}],[\"我们在自己的包中也建一个名为string的类\",{\"1\":{\"16\":1}}],[\"我们在\",{\"1\":{\"14\":1}}],[\"我们实际上是将\",{\"1\":{\"14\":1}}],[\"我们并不会通过一个具体的对象去修改和使用静态属性\",{\"1\":{\"13\":1}}],[\"我们也可以进行近似\",{\"1\":{\"229\":1}}],[\"我们也可以在类中添加代码块\",{\"1\":{\"9\":1}}],[\"我们也可以为构造方法设定参数\",{\"1\":{\"9\":1}}],[\"我们也可以手动声明\",{\"1\":{\"9\":1}}],[\"我们也可以不对任何对象进行引用\",{\"1\":{\"4\":1}}],[\"我们前面创建对象\",{\"1\":{\"9\":1}}],[\"我们可以认为\",{\"1\":{\"223\":1}}],[\"我们可以写出\",{\"1\":{\"186\":1}}],[\"我们可以修改为噪音\",{\"1\":{\"172\":1}}],[\"我们可以设计如下方程\",{\"1\":{\"172\":1}}],[\"我们可以确保其可以遍历所有的\",{\"1\":{\"159\":1}}],[\"我们可以生成一个\",{\"1\":{\"151\":1}}],[\"我们可以很轻松的求出各个情况下的q\",{\"1\":{\"150\":1}}],[\"我们可以分析出在该状态下采取哪个\",{\"1\":{\"121\":1}}],[\"我们可以下面的方式实现多种字符匹配\",{\"1\":{\"55\":1}}],[\"我们可以让他等于一个非常大的数字\",{\"1\":{\"38\":1}}],[\"我们可以直接创建静态内部类的对象\",{\"1\":{\"62\":1}}],[\"我们可以直接在类的内部定义成员内部类\",{\"1\":{\"61\":1}}],[\"我们可以直接将一个对应的基本类型值作为对应包装类型引用变量的值\",{\"1\":{\"36\":1}}],[\"我们可以直接通过接口名\",{\"1\":{\"28\":1}}],[\"我们可以为成员变量\",{\"1\":{\"17\":1}}],[\"我们可以通过一些特定的算法进行求解\",{\"1\":{\"169\":1}}],[\"我们可以通过前面所引入的\",{\"1\":{\"150\":1}}],[\"我们可以通过对应的迭代算法来求解贝尔曼最优公式\",{\"1\":{\"135\":1}}],[\"我们可以通过对象的引用来间接操作对象\",{\"1\":{\"4\":1}}],[\"我们可以通过包的形式将这些类进行分类存放\",{\"1\":{\"16\":1}}],[\"我们可以将该问题定义为一个\",{\"1\":{\"183\":1}}],[\"我们可以将这个问题可以转化为一个随机变量的方法\",{\"1\":{\"178\":1}}],[\"我们可以将\",{\"1\":{\"174\":1,\"176\":1,\"177\":1}}],[\"我们可以将方法标记为静态\",{\"1\":{\"13\":1}}],[\"我们可以将各种需要初始化的操作都在这里进行处理\",{\"1\":{\"9\":1}}],[\"我们可以理解为是属于这个类的\",{\"1\":{\"13\":1}}],[\"我们可以使用append方法来讲字符串拼接到后面\",{\"1\":{\"54\":1}}],[\"我们可以使用它来对字符串进行拼接\",{\"1\":{\"54\":1}}],[\"我们可以使用\",{\"1\":{\"9\":1,\"16\":1}}],[\"我们可以进行一次重载\",{\"1\":{\"8\":1}}],[\"修改为\",{\"1\":{\"215\":1}}],[\"修改一个对象的属性并不会影响到其他对象\",{\"1\":{\"4\":1}}],[\"修改对象的属性\",{\"1\":{\"4\":1}}],[\"访问第三行第二列的元素\",{\"1\":{\"49\":1}}],[\"访问元素\",{\"0\":{\"46\":1}}],[\"访问权限控制\",{\"0\":{\"17\":1}}],[\"访问类的静态变量\",{\"1\":{\"14\":1}}],[\"访问\",{\"1\":{\"4\":1}}],[\"如梯度上升算法\",{\"1\":{\"221\":1}}],[\"如何估计\",{\"0\":{\"151\":1}}],[\"如何将\",{\"1\":{\"149\":1}}],[\"如何在没有模型\",{\"1\":{\"148\":1}}],[\"如何确保策略\",{\"1\":{\"141\":1}}],[\"如何通过\",{\"1\":{\"141\":1}}],[\"如何处理等式右边的\",{\"0\":{\"130\":1}}],[\"如何求解\",{\"0\":{\"129\":1}}],[\"如何得到最优策略\",{\"1\":{\"126\":1}}],[\"如可以在target\",{\"1\":{\"92\":1}}],[\"如p\",{\"1\":{\"92\":1}}],[\"如a\",{\"1\":{\"87\":1}}],[\"如果\",{\"1\":{\"169\":1}}],[\"如果对于\",{\"1\":{\"169\":1}}],[\"如果ϵ=1\",{\"1\":{\"160\":1}}],[\"如果经过了\",{\"1\":{\"157\":1}}],[\"如果从其他状态出发\",{\"1\":{\"157\":1}}],[\"如果存在一个\",{\"1\":{\"157\":1}}],[\"如果该\",{\"1\":{\"154\":1}}],[\"如果γ接近0\",{\"1\":{\"92\":1}}],[\"如果agent在当前时刻t所执行的动作能够提高总mos\",{\"1\":{\"86\":1}}],[\"如果参数只有一个\",{\"1\":{\"67\":1}}],[\"如果有一个参数和返回值的话\",{\"1\":{\"67\":1}}],[\"如果有初始值\",{\"1\":{\"26\":1}}],[\"如果一个方法的参数需要的是一个接口的实现\",{\"1\":{\"67\":1}}],[\"如果一个接口中有且只有一个待实现的抽象方法\",{\"1\":{\"67\":1}}],[\"如果一个包中有多个类\",{\"1\":{\"16\":1}}],[\"如果修改了a\",{\"1\":{\"66\":1}}],[\"如果不是\",{\"1\":{\"66\":1}}],[\"如果想用外部变量\",{\"1\":{\"66\":1}}],[\"如果想通过对象的形式去使用他们\",{\"1\":{\"35\":1}}],[\"如果需要指定为外部的对象\",{\"1\":{\"61\":2}}],[\"如果需要完整学习正则表达式\",{\"1\":{\"55\":1}}],[\"如果需要使用更大的数组\",{\"1\":{\"45\":1}}],[\"如果我们强制让\",{\"1\":{\"197\":1}}],[\"如果我们想直接作为接口抽象方法的实现\",{\"1\":{\"67\":1}}],[\"如果我们想要表示一个范围内的字符\",{\"1\":{\"55\":1}}],[\"如果我们我们将其权限改为private\",{\"1\":{\"61\":1}}],[\"如果我们要使用成员内部类\",{\"1\":{\"61\":1}}],[\"如果我们仅仅是想要判断两个字符串的内容是否相同\",{\"1\":{\"53\":1}}],[\"如果直接使用加的话\",{\"1\":{\"54\":1}}],[\"如果程序中大量进行字符串的拼接似乎不太好\",{\"1\":{\"54\":1}}],[\"如果内容相同\",{\"1\":{\"53\":1}}],[\"如果同时存在其他参数\",{\"1\":{\"50\":1}}],[\"如果在范围内\",{\"1\":{\"36\":1}}],[\"如果值相同\",{\"1\":{\"36\":1}}],[\"如果方法体中只有一个返回语句\",{\"1\":{\"67\":1}}],[\"如果方法在接口中存在默认实现\",{\"1\":{\"28\":1}}],[\"如果方法内没有变量出现重名的情况\",{\"1\":{\"7\":1}}],[\"如果抽象方法在抽象类定义的是\",{\"1\":{\"27\":1}}],[\"如果子类都访问不了\",{\"1\":{\"27\":1}}],[\"如果父类的方法是\",{\"1\":{\"25\":1}}],[\"如果希望调用父类原本的方法实现\",{\"1\":{\"25\":1}}],[\"如果传入的对象为null\",{\"1\":{\"25\":1}}],[\"如果某个类中存在静态方法或是静态变量\",{\"1\":{\"17\":1}}],[\"如果没有出现歧义\",{\"1\":{\"16\":1}}],[\"如果是普通成员方法\",{\"1\":{\"68\":1}}],[\"如果是直接使用双引号创建的字符串\",{\"1\":{\"53\":1}}],[\"如果是基本数据类型\",{\"1\":{\"44\":1}}],[\"如果是基本类型\",{\"1\":{\"4\":1}}],[\"如果是多个单词\",{\"1\":{\"5\":1}}],[\"如果是引用类型\",{\"1\":{\"4\":1,\"44\":1}}],[\"如果是boolean的话\",{\"1\":{\"4\":1}}],[\"如\",{\"1\":{\"4\":1,\"152\":1,\"155\":1}}],[\"即在策略\",{\"1\":{\"225\":1}}],[\"即在状态s1​下采用动作a1​获得的奖励r=1的概率\",{\"1\":{\"92\":1}}],[\"即此时策略\",{\"1\":{\"221\":1}}],[\"即此时的discounted\",{\"1\":{\"92\":1}}],[\"即基于一个策略\",{\"1\":{\"208\":1}}],[\"即各个状态的可能性为∣s∣1​\",{\"1\":{\"207\":1}}],[\"即我通过一个策略与环境进行交互生成一系列经验\",{\"1\":{\"196\":1}}],[\"即我通过这个策略与环境进行交互生成一系列经验\",{\"1\":{\"195\":1}}],[\"即函数\",{\"1\":{\"170\":1}}],[\"即不需要完全精确地求出\",{\"1\":{\"155\":1}}],[\"即不具备terminal\",{\"1\":{\"92\":1}}],[\"即对于损失函数中的\",{\"1\":{\"206\":1}}],[\"即对于数据\",{\"1\":{\"153\":1}}],[\"即对于每个状态\",{\"1\":{\"152\":1}}],[\"即对于给定策略\",{\"1\":{\"120\":1}}],[\"即p\",{\"1\":{\"148\":1}}],[\"即j→∞\",{\"1\":{\"139\":1}}],[\"即求解右边的式子\",{\"1\":{\"136\":1}}],[\"即f\",{\"1\":{\"131\":1}}],[\"即a∗=argmaxa​q\",{\"1\":{\"130\":1}}],[\"即vπ​\",{\"1\":{\"122\":1}}],[\"即一个trajectory下的discounted\",{\"1\":{\"112\":1}}],[\"即无记忆的特性\",{\"1\":{\"93\":1}}],[\"即无人机作为空中基站\",{\"1\":{\"81\":1}}],[\"即表示具有终止状态terminal\",{\"1\":{\"92\":1}}],[\"即状态s1​采用动作a1​转到状态s2​的概率\",{\"1\":{\"92\":1}}],[\"即ξ=\",{\"1\":{\"87\":1}}],[\"即xuav​\",{\"1\":{\"86\":1}}],[\"即γ≥γkn​​\",{\"1\":{\"82\":1}}],[\"即需要考虑地面不同用户的具体需求\",{\"1\":{\"77\":1}}],[\"即之后不能修改\",{\"1\":{\"66\":1}}],[\"即使先估计了\",{\"1\":{\"152\":1}}],[\"即使用初始化块\",{\"1\":{\"65\":1}}],[\"即使是基本类型的数组\",{\"1\":{\"44\":1}}],[\"即使是最大的long类型\",{\"1\":{\"38\":1}}],[\"即使是两个不同的对象\",{\"1\":{\"25\":1}}],[\"即使强制类型转换\",{\"1\":{\"25\":1}}],[\"即使类名相同\",{\"1\":{\"16\":1}}],[\"即可以通过梯度下降\",{\"1\":{\"215\":1}}],[\"即可\",{\"1\":{\"5\":1,\"130\":1,\"150\":1}}],[\"即空指针异常\",{\"1\":{\"4\":1}}],[\"即\",{\"1\":{\"4\":1,\"27\":1,\"28\":1,\"47\":1,\"99\":1,\"136\":2,\"139\":2,\"141\":1,\"150\":1,\"151\":2,\"167\":1,\"172\":1,\"210\":1}}],[\"对应目标函数的真实梯度是\",{\"1\":{\"209\":1}}],[\"对应收敛性证明\",{\"1\":{\"177\":1}}],[\"对应一个向量\",{\"1\":{\"131\":1}}],[\"对应策略表示为\",{\"1\":{\"130\":1}}],[\"对应的vk+1​\",{\"1\":{\"136\":1}}],[\"对应的动作\",{\"1\":{\"130\":1}}],[\"对应的\",{\"1\":{\"115\":2,\"119\":1,\"139\":1,\"167\":1,\"189\":1}}],[\"对应的discounted\",{\"1\":{\"112\":1}}],[\"对应算法\",{\"0\":{\"101\":1}}],[\"对应\",{\"1\":{\"96\":2,\"227\":1}}],[\"对应状态中所有可选择的动作集合\",{\"1\":{\"92\":1}}],[\"对应s就是uav的部署位置\",{\"1\":{\"86\":1}}],[\"对应关系如下表\",{\"1\":{\"36\":1}}],[\"对应多个文件夹\",{\"1\":{\"16\":1}}],[\"对于策略\",{\"1\":{\"186\":1}}],[\"对于观测值g​\",{\"1\":{\"177\":1}}],[\"对于这个问题\",{\"1\":{\"174\":1}}],[\"对于这种成员内部类\",{\"1\":{\"61\":1}}],[\"对于函数\",{\"1\":{\"174\":1}}],[\"对于一个\",{\"1\":{\"154\":1}}],[\"对于贝尔曼最优公式而言\",{\"1\":{\"128\":1,\"129\":1}}],[\"对于所有状态s\",{\"1\":{\"119\":1}}],[\"对于不同的策略\",{\"1\":{\"113\":1}}],[\"对于每个\",{\"1\":{\"154\":1}}],[\"对于每个智能体\",{\"1\":{\"86\":1}}],[\"对于每一个状态\",{\"1\":{\"92\":1}}],[\"对于动作空间而言\",{\"1\":{\"87\":1}}],[\"对于用户kn​存在特定的信噪比目标γkn​​\",{\"1\":{\"82\":1}}],[\"对于用户kn​∈kn​\",{\"1\":{\"81\":1}}],[\"对于无人机的总发射功率也均匀地分配给每个用户\",{\"1\":{\"82\":1}}],[\"对于无人机n\",{\"1\":{\"81\":1,\"82\":1}}],[\"对于指定区域\",{\"1\":{\"81\":1}}],[\"对于已经实现的方法\",{\"1\":{\"67\":1}}],[\"对于\",{\"1\":{\"66\":1,\"121\":1,\"143\":3,\"150\":2,\"152\":1,\"154\":1,\"161\":1,\"203\":1,\"215\":1}}],[\"对于普通字符来说\",{\"1\":{\"55\":1}}],[\"对于变量来说\",{\"1\":{\"54\":1}}],[\"对于基本类型的数组来说\",{\"1\":{\"47\":1}}],[\"对于需要精确计算的场景\",{\"1\":{\"38\":1}}],[\"对于非常大的整数计算\",{\"1\":{\"38\":1}}],[\"对于引用类型\",{\"1\":{\"29\":1}}],[\"对于类中基本数据类型\",{\"1\":{\"29\":1}}],[\"对于人类的不同子类\",{\"1\":{\"28\":1}}],[\"对于成员方法\",{\"1\":{\"26\":1}}],[\"对于成员变量\",{\"1\":{\"26\":1}}],[\"对于对象而言\",{\"1\":{\"4\":1}}],[\"对其进行修改\",{\"1\":{\"9\":1}}],[\"对象再多\",{\"1\":{\"13\":1}}],[\"对象在创建之后\",{\"1\":{\"9\":1}}],[\"对象创建成功之后\",{\"1\":{\"4\":1}}],[\"对象实例的创建\",{\"1\":{\"4\":1}}],[\"对象\",{\"0\":{\"61\":1},\"1\":{\"4\":1,\"59\":1,\"68\":2}}],[\"=θt​+αe\",{\"1\":{\"229\":1}}],[\"=dtvπ​\",{\"1\":{\"223\":1}}],[\"=i=1∑n​yt​−q^​\",{\"1\":{\"215\":1}}],[\"=−2e\",{\"1\":{\"209\":1}}],[\"=−c1​ln\",{\"1\":{\"83\":1}}],[\"=∣s∣1​s∈s∑​\",{\"1\":{\"207\":1}}],[\"=qt​\",{\"1\":{\"189\":1,\"193\":1}}],[\"=vt​\",{\"1\":{\"185\":1}}],[\"=vπ0​​\",{\"1\":{\"143\":1}}],[\"=w−\",{\"1\":{\"183\":1}}],[\"=wk​−αk​▽w​f\",{\"1\":{\"177\":1}}],[\"=wk​−αk​e\",{\"1\":{\"174\":1}}],[\"=wk​−αk​\",{\"1\":{\"172\":1,\"183\":1}}],[\"=▽w​j\",{\"1\":{\"177\":1}}],[\"=g\",{\"1\":{\"170\":1,\"177\":1}}],[\"=k1​\",{\"1\":{\"167\":1}}],[\"=k0​−1dkn​​−α\",{\"1\":{\"82\":1}}],[\"=n1​∑i=1n​xi​\",{\"1\":{\"167\":1}}],[\"=1\",{\"1\":{\"152\":1,\"208\":1,\"223\":1}}],[\"=argmaxπ​∑a​π\",{\"1\":{\"152\":1}}],[\"=argmaxa​qπk​​\",{\"1\":{\"139\":1}}],[\"=argmaxa​qk​\",{\"1\":{\"136\":1}}],[\"=a∑​πk​\",{\"1\":{\"139\":1}}],[\"=a∑​π\",{\"1\":{\"116\":1,\"117\":3,\"118\":1,\"121\":1,\"186\":1}}],[\"=πargmax​a∑​πk​\",{\"1\":{\"139\":1}}],[\"=πargmax​a∑​π\",{\"1\":{\"136\":1}}],[\"=πmax​\",{\"1\":{\"134\":1,\"135\":1}}],[\"=x∗\",{\"1\":{\"131\":1}}],[\"=x\",{\"1\":{\"131\":1}}],[\"=maxa​qk​\",{\"1\":{\"136\":1}}],[\"=maxπ​\",{\"1\":{\"131\":1}}],[\"=maxπ​∑a​π\",{\"1\":{\"130\":1}}],[\"=mean\",{\"1\":{\"118\":1}}],[\"=∑a​π\",{\"1\":{\"122\":1}}],[\"=∑a​qπ​\",{\"1\":{\"121\":1}}],[\"=∑r​p\",{\"1\":{\"121\":1}}],[\"=r∑​rp\",{\"1\":{\"226\":1}}],[\"=r∑​p\",{\"1\":{\"150\":1}}],[\"=rπ​\",{\"1\":{\"119\":1}}],[\"=rt+1​+γgt+1​​\",{\"1\":{\"115\":1}}],[\"=rt+1​+γ\",{\"1\":{\"115\":1}}],[\"=s∈s∑​d\",{\"1\":{\"228\":1}}],[\"=s∈s∑​dπ​\",{\"1\":{\"208\":1}}],[\"=s\",{\"1\":{\"117\":3}}],[\"=sin−1\",{\"1\":{\"82\":1}}],[\"=es∼d\",{\"1\":{\"228\":1}}],[\"=es∼d​\",{\"1\":{\"226\":1,\"228\":1}}],[\"=es∼η\",{\"1\":{\"99\":1}}],[\"=e\",{\"1\":{\"113\":1,\"115\":2,\"121\":1,\"122\":2,\"148\":2,\"150\":1,\"151\":1,\"174\":1,\"177\":4,\"186\":3,\"188\":1,\"192\":1,\"206\":1,\"207\":1,\"208\":1,\"209\":1,\"214\":1,\"215\":3,\"224\":1,\"226\":1,\"228\":1}}],[\"=0\",{\"1\":{\"99\":1,\"169\":1,\"171\":2,\"172\":1,\"177\":2,\"183\":1}}],[\"=p\",{\"1\":{\"93\":2}}],[\"=3rtt+rkn​​\",{\"1\":{\"83\":1}}],[\"=ζ1​moskn​​delay\",{\"1\":{\"83\":1}}],[\"=γk0​σ2dkn​​α\",{\"1\":{\"82\":1}}],[\"=b2​ln\",{\"1\":{\"82\":1}}],[\"=bkn​​log2​\",{\"1\":{\"82\":1}}],[\"=b1​\",{\"1\":{\"82\":1}}],[\"=σ2pkn​​gkn​​\",{\"1\":{\"82\":1}}],[\"====​n→∞lim​n1​e\",{\"1\":{\"227\":1}}],[\"=========================\",{\"1\":{\"68\":1}}],[\"====================================\",{\"1\":{\"67\":2}}],[\"=================\",{\"1\":{\"68\":1}}],[\"==\",{\"1\":{\"24\":1,\"25\":2,\"29\":1,\"36\":3,\"53\":3}}],[\"=\",{\"1\":{\"4\":6,\"5\":3,\"7\":2,\"9\":16,\"13\":1,\"16\":1,\"25\":5,\"27\":4,\"28\":4,\"29\":2,\"30\":3,\"36\":10,\"37\":3,\"38\":6,\"44\":6,\"45\":1,\"46\":2,\"47\":8,\"48\":3,\"49\":1,\"50\":1,\"53\":10,\"54\":13,\"55\":2,\"61\":11,\"62\":2,\"63\":2,\"64\":1,\"65\":4,\"66\":3,\"67\":5,\"68\":6,\"81\":1,\"92\":1,\"130\":1,\"136\":1,\"139\":1,\"160\":1,\"172\":1,\"183\":1,\"228\":1}}],[\"n→∞lim​n1​e\",{\"1\":{\"227\":2}}],[\"n=0∑∞​βnrt+n​\",{\"1\":{\"86\":1}}],[\"n∈n=\",{\"1\":{\"86\":1}}],[\"n∈1\",{\"1\":{\"81\":1}}],[\"numbers\",{\"1\":{\"148\":1}}],[\"number\",{\"1\":{\"83\":1}}],[\"null\",{\"1\":{\"4\":1,\"25\":1,\"38\":1}}],[\"n0​为用户所在位置的加性高斯白噪声\",{\"1\":{\"82\":1}}],[\"not\",{\"1\":{\"233\":1}}],[\"notifyall\",{\"1\":{\"24\":1}}],[\"notify\",{\"1\":{\"24\":1}}],[\"novel\",{\"1\":{\"77\":1}}],[\"network\",{\"1\":{\"215\":5,\"217\":1}}],[\"networks\",{\"0\":{\"76\":1},\"1\":{\"77\":1,\"215\":1}}],[\"newestimatevt+1​\",{\"1\":{\"185\":1}}],[\"new\",{\"1\":{\"4\":3,\"5\":1,\"14\":1,\"16\":1,\"25\":4,\"28\":2,\"29\":1,\"30\":1,\"36\":4,\"37\":1,\"38\":1,\"44\":5,\"45\":1,\"46\":1,\"47\":4,\"50\":1,\"53\":4,\"54\":2,\"61\":7,\"62\":1,\"63\":1,\"65\":4,\"66\":1,\"68\":2}}],[\"n\",{\"0\":{\"190\":1},\"1\":{\"50\":1,\"55\":9,\"81\":2,\"84\":1,\"86\":2,\"143\":1,\"144\":1,\"215\":1}}],[\"nanos\",{\"1\":{\"24\":1}}],[\"native\",{\"1\":{\"24\":7}}],[\"name\",{\"1\":{\"4\":3,\"5\":2,\"7\":7,\"9\":14,\"13\":1,\"17\":1,\"25\":2,\"27\":6,\"28\":2,\"29\":2,\"30\":5,\"50\":1,\"61\":14,\"62\":4,\"63\":4,\"64\":2}}],[\"s∣s\",{\"1\":{\"228\":1}}],[\"s∣a\",{\"1\":{\"130\":1,\"131\":1}}],[\"s∑​dπ​\",{\"1\":{\"227\":1}}],[\"scalar\",{\"1\":{\"221\":1}}],[\"score\",{\"1\":{\"78\":1}}],[\"sgdw\",{\"0\":{\"179\":1}}],[\"sgd的目标是\",{\"1\":{\"177\":1}}],[\"sgd\",{\"0\":{\"175\":1,\"177\":1,\"178\":1},\"1\":{\"174\":1,\"177\":3,\"178\":2,\"209\":1,\"228\":1}}],[\"s5​\",{\"1\":{\"154\":1}}],[\"s5​a1​​\",{\"1\":{\"154\":1}}],[\"s2​\",{\"1\":{\"154\":4}}],[\"s2​a3​​s5​a1​​\",{\"1\":{\"154\":1}}],[\"s2​a4​​s1​a2​​s2​a3​​s5​a1​​\",{\"1\":{\"154\":1}}],[\"s2​∣s1​\",{\"1\":{\"92\":1}}],[\"s​=maxπ​∑a​π\",{\"1\":{\"131\":1}}],[\"soloving\",{\"1\":{\"165\":1}}],[\"solution\",{\"1\":{\"120\":2}}],[\"soft\",{\"0\":{\"159\":1},\"1\":{\"158\":1,\"159\":2,\"160\":1}}],[\"some\",{\"1\":{\"92\":1,\"189\":1}}],[\"sj​∣si​\",{\"1\":{\"119\":1}}],[\"sn​\",{\"1\":{\"119\":2}}],[\"sa​\",{\"1\":{\"189\":2}}],[\"sarsa\",{\"0\":{\"189\":1,\"190\":1,\"191\":1,\"212\":1},\"1\":{\"188\":3,\"189\":1,\"196\":1}}],[\"sa\",{\"1\":{\"165\":2}}],[\"samples\",{\"1\":{\"174\":1,\"215\":1}}],[\"sample\",{\"1\":{\"151\":1,\"172\":1,\"174\":1}}],[\"sampling\",{\"0\":{\"103\":1}}],[\"satisfying\",{\"1\":{\"171\":1}}],[\"satisfaction\",{\"1\":{\"77\":1}}],[\"satic\",{\"1\":{\"25\":1}}],[\"s0​\",{\"1\":{\"93\":2,\"185\":1}}],[\"s∈s​\",{\"1\":{\"208\":1}}],[\"s∈s\",{\"1\":{\"93\":1,\"136\":1,\"139\":2,\"186\":2}}],[\"s1​a2​​s2​a3​​s5​a1​​\",{\"1\":{\"154\":1}}],[\"s1​a2​​s2​a4​​s1​a2​​s2​a3​​s5​a1​​\",{\"1\":{\"154\":2}}],[\"s1​\",{\"1\":{\"119\":2,\"154\":4,\"185\":1}}],[\"s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​r=1→​a2​​s9​r=1→​a2​​s9​\",{\"1\":{\"92\":1}}],[\"s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​\",{\"1\":{\"92\":1}}],[\"s1​→a1​s2​\",{\"1\":{\"92\":1}}],[\"simplest\",{\"0\":{\"97\":1}}],[\"si​\",{\"1\":{\"92\":2,\"215\":2}}],[\"single\",{\"1\":{\"155\":1,\"227\":1}}],[\"sin\",{\"1\":{\"82\":1}}],[\"s=\",{\"1\":{\"92\":1}}],[\"space\",{\"1\":{\"86\":2,\"92\":3}}],[\"split\",{\"1\":{\"53\":2}}],[\"slove\",{\"0\":{\"120\":1}}],[\"slow\",{\"1\":{\"83\":1}}],[\"sleep\",{\"1\":{\"30\":4}}],[\"ss\",{\"1\":{\"65\":2}}],[\"syntax\",{\"1\":{\"55\":1}}],[\"system\",{\"1\":{\"4\":1,\"5\":1,\"9\":3,\"13\":2,\"17\":1,\"25\":2,\"27\":1,\"28\":2,\"29\":4,\"30\":1,\"36\":3,\"37\":1,\"38\":2,\"45\":1,\"46\":2,\"49\":1,\"50\":1,\"51\":1,\"53\":6,\"54\":3,\"55\":2,\"61\":5,\"62\":1,\"63\":1,\"64\":7,\"65\":3,\"66\":1,\"67\":3,\"68\":2}}],[\"s\",{\"0\":{\"225\":1},\"1\":{\"55\":4,\"82\":1,\"83\":1,\"84\":1,\"86\":5,\"93\":5,\"99\":4,\"100\":1,\"113\":2,\"114\":1,\"115\":1,\"117\":10,\"118\":7,\"119\":4,\"121\":13,\"122\":4,\"126\":3,\"128\":4,\"130\":2,\"136\":11,\"139\":13,\"148\":3,\"150\":9,\"151\":8,\"152\":4,\"155\":1,\"157\":6,\"160\":6,\"185\":2,\"186\":7,\"188\":2,\"189\":3,\"192\":1,\"193\":3,\"204\":2,\"205\":2,\"206\":3,\"207\":4,\"208\":8,\"209\":7,\"214\":5,\"215\":28,\"223\":6,\"225\":1,\"226\":8,\"227\":2,\"228\":12,\"229\":4}}],[\"short\",{\"1\":{\"36\":3}}],[\"supposed\",{\"1\":{\"77\":1}}],[\"super\",{\"1\":{\"23\":1,\"25\":3,\"27\":1,\"28\":1,\"29\":2,\"61\":2}}],[\"sub\",{\"1\":{\"53\":2}}],[\"substring\",{\"1\":{\"53\":2}}],[\"sum\",{\"1\":{\"8\":2,\"68\":5}}],[\"sequence\",{\"1\":{\"131\":1}}],[\"set\",{\"1\":{\"93\":3}}],[\"sets\",{\"1\":{\"93\":1}}],[\"setstatus\",{\"1\":{\"30\":2}}],[\"setname\",{\"1\":{\"7\":2}}],[\"sex\",{\"1\":{\"4\":1,\"5\":1,\"9\":14,\"13\":1,\"17\":1,\"25\":2,\"27\":6,\"28\":2,\"29\":2,\"50\":1}}],[\"steady\",{\"1\":{\"208\":1}}],[\"step\",{\"0\":{\"190\":1},\"1\":{\"139\":2,\"152\":2,\"227\":1}}],[\"st+2​→at+3​\",{\"1\":{\"112\":1,\"115\":1}}],[\"st+2​→at+2​rt+3​\",{\"1\":{\"112\":1,\"115\":1}}],[\"st+1​=s\",{\"1\":{\"117\":2}}],[\"st+1​→at+1​rt+2​\",{\"1\":{\"112\":1,\"115\":1}}],[\"st+1​∣at+1​\",{\"1\":{\"93\":3}}],[\"st+1​\",{\"1\":{\"86\":1,\"185\":4,\"189\":2,\"192\":1,\"193\":1,\"211\":2,\"212\":1,\"213\":2}}],[\"st​→at​rt+1​\",{\"1\":{\"112\":1,\"115\":1}}],[\"st​\",{\"1\":{\"93\":4,\"185\":12,\"189\":6,\"193\":5,\"209\":4,\"210\":4,\"211\":3,\"212\":2,\"213\":2,\"229\":1}}],[\"stochastic\",{\"0\":{\"173\":1},\"1\":{\"92\":1,\"126\":1,\"159\":1,\"165\":2}}],[\"stop\",{\"1\":{\"92\":1}}],[\"starts\",{\"0\":{\"153\":1,\"156\":1},\"1\":{\"153\":1,\"157\":2,\"158\":1}}],[\"starting\",{\"1\":{\"114\":1,\"121\":2}}],[\"start\",{\"1\":{\"83\":1}}],[\"stationary\",{\"0\":{\"208\":1},\"1\":{\"208\":2,\"225\":1}}],[\"static\",{\"1\":{\"4\":3,\"5\":1,\"13\":5,\"16\":2,\"17\":3,\"24\":2,\"28\":3,\"29\":1,\"30\":8,\"36\":5,\"37\":1,\"38\":2,\"44\":1,\"45\":1,\"46\":1,\"47\":2,\"48\":1,\"49\":1,\"50\":1,\"51\":1,\"53\":3,\"54\":3,\"55\":2,\"61\":2,\"62\":2,\"64\":10,\"65\":3,\"66\":1,\"67\":4,\"68\":5}}],[\"statrts的解释\",{\"0\":{\"157\":1}}],[\"statevalue\",{\"1\":{\"209\":1}}],[\"states中\",{\"1\":{\"92\":1}}],[\"states中限制action\",{\"1\":{\"92\":1}}],[\"states的任务\",{\"1\":{\"92\":1}}],[\"states的trajectory\",{\"1\":{\"92\":1}}],[\"states\",{\"1\":{\"92\":1,\"93\":1}}],[\"state\",{\"0\":{\"111\":1,\"113\":1,\"114\":1,\"120\":1,\"131\":1,\"184\":1,\"205\":1,\"223\":1},\"1\":{\"86\":2,\"92\":6,\"93\":4,\"110\":1,\"113\":4,\"114\":4,\"115\":1,\"118\":1,\"120\":1,\"121\":6,\"122\":2,\"125\":1,\"128\":1,\"129\":1,\"136\":1,\"139\":2,\"141\":1,\"143\":2,\"144\":1,\"148\":1,\"150\":3,\"152\":3,\"154\":6,\"155\":1,\"157\":2,\"159\":1,\"165\":1,\"184\":1,\"186\":2,\"188\":2,\"204\":1,\"208\":1,\"221\":1,\"223\":2,\"228\":1}}],[\"status>\",{\"1\":{\"30\":1}}],[\"status\",{\"1\":{\"30\":22,\"92\":1}}],[\"str5\",{\"1\":{\"54\":4}}],[\"str\",{\"1\":{\"53\":8,\"55\":5}}],[\"str4\",{\"1\":{\"53\":2,\"54\":4}}],[\"str3\",{\"1\":{\"53\":2,\"54\":4}}],[\"str2\",{\"1\":{\"53\":3,\"54\":4}}],[\"str1\",{\"1\":{\"53\":3,\"54\":4}}],[\"string的拼接会在编译时进行各种优化\",{\"1\":{\"54\":1}}],[\"stringbuilder\",{\"0\":{\"54\":1},\"1\":{\"54\":5}}],[\"string类重载了equals方法用于判断和比较内容是否相同\",{\"1\":{\"53\":1}}],[\"string本身也是一个类\",{\"1\":{\"53\":1}}],[\"strings这个变量就是一个string\",{\"1\":{\"50\":1}}],[\"strings\",{\"1\":{\"50\":3,\"53\":2}}],[\"string\",{\"0\":{\"53\":1},\"1\":{\"4\":5,\"5\":3,\"7\":3,\"9\":14,\"13\":4,\"16\":6,\"17\":3,\"24\":1,\"27\":8,\"28\":4,\"29\":3,\"30\":5,\"36\":4,\"37\":1,\"38\":2,\"44\":1,\"45\":1,\"46\":1,\"47\":6,\"48\":1,\"49\":1,\"50\":8,\"51\":2,\"53\":18,\"54\":14,\"55\":4,\"61\":10,\"62\":3,\"63\":2,\"64\":2,\"65\":4,\"66\":1,\"67\":3,\"68\":8}}],[\"study\",{\"1\":{\"28\":15,\"29\":2,\"30\":4,\"65\":6,\"66\":3,\"67\":18,\"68\":15}}],[\"student\",{\"1\":{\"25\":4,\"28\":2,\"29\":10,\"30\":6,\"65\":5}}],[\"a∈a∑​π\",{\"1\":{\"228\":1}}],[\"a∈a∑​▽θ​π\",{\"1\":{\"228\":2}}],[\"ak​=k1​是满足上面三个条件的\",{\"1\":{\"171\":1}}],[\"ak​→0不要过快\",{\"1\":{\"171\":1}}],[\"ak​→0\",{\"1\":{\"171\":1}}],[\"ak​\",{\"1\":{\"170\":1}}],[\"ak∗​∣s\",{\"1\":{\"152\":1}}],[\"ak∗​\",{\"1\":{\"139\":1}}],[\"a3​\",{\"1\":{\"154\":2}}],[\"a4​\",{\"1\":{\"154\":2}}],[\"a2​\",{\"1\":{\"154\":4}}],[\"a2c\",{\"0\":{\"98\":1}}],[\"a=ak∗​\",{\"1\":{\"136\":1,\"139\":1}}],[\"as\",{\"1\":{\"131\":1,\"177\":1}}],[\"associate\",{\"1\":{\"93\":1}}],[\"assisted\",{\"1\":{\"77\":1}}],[\"along\",{\"1\":{\"227\":1}}],[\"alogorithm\",{\"0\":{\"205\":1}}],[\"algorithms\",{\"0\":{\"209\":1},\"1\":{\"165\":1}}],[\"algorithm\",{\"0\":{\"135\":1,\"138\":1,\"142\":1,\"144\":1,\"145\":1,\"168\":1},\"1\":{\"131\":1,\"141\":2,\"143\":2,\"144\":1,\"149\":1,\"152\":1,\"171\":1}}],[\"all\",{\"1\":{\"114\":1,\"126\":1}}],[\"average\",{\"0\":{\"223\":1,\"226\":1},\"1\":{\"121\":2,\"227\":1}}],[\"a∑​π\",{\"1\":{\"118\":1,\"228\":1}}],[\"a∑​p\",{\"1\":{\"117\":1}}],[\"a∼π​\",{\"1\":{\"99\":3,\"228\":1}}],[\"advantage\",{\"0\":{\"98\":1}}],[\"a∣s\",{\"1\":{\"93\":1,\"99\":3,\"114\":1,\"116\":2,\"117\":5,\"118\":4,\"121\":3,\"122\":1,\"128\":2,\"130\":2,\"131\":1,\"136\":2,\"139\":3,\"152\":1,\"160\":1,\"186\":1,\"221\":1,\"226\":1,\"228\":14,\"229\":2}}],[\"any\",{\"1\":{\"126\":1,\"131\":1,\"159\":1}}],[\"an\",{\"1\":{\"92\":1,\"120\":1,\"121\":1}}],[\"and\",{\"0\":{\"76\":1},\"1\":{\"77\":2,\"87\":1,\"121\":1,\"122\":1,\"125\":1,\"126\":1,\"148\":1,\"215\":1}}],[\"at+1​\",{\"1\":{\"189\":2,\"212\":1,\"213\":1}}],[\"at\",{\"1\":{\"92\":1,\"93\":1}}],[\"at​∣st​\",{\"1\":{\"229\":1}}],[\"at​=a\",{\"1\":{\"116\":1,\"117\":2,\"121\":2,\"122\":1,\"148\":1,\"150\":1,\"151\":1,\"192\":1}}],[\"at​\",{\"1\":{\"86\":1,\"189\":8,\"193\":5,\"212\":2,\"213\":2,\"229\":1}}],[\"a1​\",{\"1\":{\"92\":2,\"93\":2,\"154\":1}}],[\"ai​\",{\"1\":{\"92\":1,\"215\":2}}],[\"actor\",{\"0\":{\"96\":1,\"97\":1,\"98\":1,\"102\":1,\"106\":1},\"1\":{\"96\":2}}],[\"actions\",{\"1\":{\"93\":1}}],[\"action\",{\"0\":{\"121\":1,\"188\":1,\"192\":1},\"1\":{\"86\":1,\"92\":3,\"93\":2,\"121\":8,\"122\":2,\"126\":2,\"130\":2,\"136\":1,\"139\":1,\"143\":2,\"148\":1,\"150\":5,\"152\":6,\"154\":9,\"155\":3,\"157\":4,\"159\":2,\"165\":1,\"188\":4,\"189\":1,\"192\":2,\"193\":1,\"204\":1}}],[\"academic\",{\"0\":{\"238\":1},\"2\":{\"88\":1,\"94\":1,\"107\":1,\"123\":1,\"132\":1,\"146\":1,\"163\":1,\"180\":1,\"201\":1,\"218\":1,\"231\":1}}],[\"awgn\",{\"1\":{\"82\":1}}],[\"aerial\",{\"1\":{\"77\":1}}],[\"aeiou\",{\"1\":{\"55\":1}}],[\"approximation\",{\"0\":{\"203\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1},\"1\":{\"165\":1,\"220\":1}}],[\"approximate\",{\"1\":{\"155\":1}}],[\"approach\",{\"1\":{\"87\":2}}],[\"appears\",{\"1\":{\"154\":1}}],[\"append\",{\"1\":{\"54\":7}}],[\"apple\",{\"1\":{\"65\":3}}],[\"abc\",{\"1\":{\"55\":2}}],[\"abcabccaa\",{\"1\":{\"55\":1}}],[\"abstract\",{\"1\":{\"27\":2,\"28\":1,\"65\":2}}],[\"aaa\",{\"1\":{\"54\":1}}],[\"aaaa\",{\"1\":{\"51\":1}}],[\"arg\",{\"1\":{\"51\":2}}],[\"args\",{\"1\":{\"4\":3,\"5\":1,\"13\":1,\"16\":2,\"17\":1,\"28\":2,\"29\":1,\"30\":1,\"36\":4,\"37\":1,\"38\":2,\"44\":1,\"45\":1,\"46\":1,\"47\":2,\"48\":1,\"49\":1,\"50\":1,\"51\":3,\"53\":3,\"54\":3,\"55\":2,\"61\":2,\"62\":1,\"64\":1,\"65\":3,\"66\":1,\"67\":3,\"68\":5}}],[\"arrav\",{\"1\":{\"47\":1}}],[\"array就是我们要遍历的数组\",{\"1\":{\"46\":1}}],[\"array\",{\"1\":{\"44\":2,\"45\":2,\"46\":4,\"47\":2}}],[\"arr\",{\"1\":{\"47\":8,\"49\":2}}],[\"a\",{\"1\":{\"4\":1,\"8\":4,\"28\":1,\"36\":7,\"38\":1,\"48\":3,\"50\":1,\"55\":6,\"61\":2,\"66\":3,\"67\":5,\"68\":5,\"77\":1,\"86\":3,\"87\":2,\"92\":7,\"93\":5,\"99\":2,\"114\":3,\"116\":1,\"117\":3,\"118\":6,\"121\":13,\"122\":2,\"125\":1,\"126\":1,\"128\":3,\"130\":1,\"131\":2,\"136\":6,\"139\":6,\"143\":1,\"148\":3,\"150\":11,\"151\":8,\"152\":3,\"154\":2,\"155\":2,\"157\":6,\"159\":1,\"165\":1,\"186\":1,\"188\":4,\"189\":3,\"192\":3,\"193\":4,\"208\":1,\"214\":3,\"215\":22,\"226\":5,\"228\":9,\"229\":4}}],[\"agent从一个状态出发\",{\"1\":{\"121\":1}}],[\"agent从一个状态出发可以得到的平均return\",{\"1\":{\"121\":1}}],[\"agent可能走出的全部轨迹\",{\"1\":{\"92\":1}}],[\"agent将获得负奖励\",{\"1\":{\"86\":1}}],[\"agent\",{\"1\":{\"86\":1,\"92\":2,\"121\":2,\"227\":1}}],[\"age已经初始化完\",{\"1\":{\"9\":1}}],[\"age\",{\"1\":{\"4\":1,\"5\":2,\"9\":18,\"13\":1,\"17\":1,\"25\":2,\"27\":6,\"28\":2,\"29\":2,\"50\":1}}],[\"a+b=c\",{\"1\":{\"0\":1}}],[\"年龄\",{\"1\":{\"4\":1,\"9\":1}}],[\"名字\",{\"1\":{\"4\":1}}],[\"pu\",{\"1\":{\"143\":1}}],[\"public\",{\"1\":{\"4\":4,\"5\":2,\"9\":4,\"13\":2,\"16\":5,\"17\":6,\"24\":10,\"25\":3,\"27\":8,\"28\":9,\"29\":6,\"30\":13,\"36\":5,\"37\":1,\"38\":2,\"44\":1,\"45\":1,\"46\":1,\"47\":2,\"48\":1,\"49\":1,\"50\":5,\"51\":1,\"53\":3,\"54\":3,\"55\":2,\"61\":15,\"62\":5,\"63\":5,\"64\":9,\"65\":12,\"66\":2,\"67\":3,\"68\":8}}],[\"pi\",{\"1\":{\"139\":1,\"141\":1,\"143\":1}}],[\"pπ​\",{\"1\":{\"119\":1}}],[\"pπ​∈rn×n\",{\"1\":{\"119\":1}}],[\"pkn​\",{\"1\":{\"84\":1,\"86\":1}}],[\"pkn​​=pmax​\",{\"1\":{\"82\":1}}],[\"poicy\",{\"0\":{\"198\":1}}],[\"point\",{\"1\":{\"131\":1}}],[\"positive\",{\"1\":{\"159\":1,\"170\":1}}],[\"possible\",{\"1\":{\"114\":1}}],[\"policies\",{\"1\":{\"158\":1,\"160\":2}}],[\"policy版本\",{\"1\":{\"213\":1}}],[\"policyevaluation\",{\"1\":{\"150\":1}}],[\"policy\",{\"0\":{\"102\":1,\"104\":1,\"126\":1,\"138\":1,\"142\":1,\"143\":1,\"144\":1,\"145\":1,\"155\":1,\"159\":1,\"160\":1,\"161\":1,\"194\":2,\"195\":1,\"196\":1,\"199\":1,\"220\":1},\"1\":{\"92\":2,\"93\":1,\"96\":2,\"118\":1,\"120\":1,\"125\":1,\"126\":2,\"136\":3,\"139\":2,\"141\":1,\"142\":1,\"143\":11,\"144\":2,\"149\":1,\"150\":1,\"152\":4,\"155\":3,\"159\":2,\"160\":2,\"161\":1,\"184\":1,\"188\":1,\"193\":1,\"194\":2,\"195\":2,\"196\":4,\"197\":3,\"198\":2,\"217\":1,\"220\":2,\"221\":1}}],[\"poor\",{\"1\":{\"83\":1}}],[\"power\",{\"1\":{\"82\":1}}],[\"pm​ax​\",{\"1\":{\"82\":1}}],[\"pmax​≥γσ2k0​dkn​​α\",{\"1\":{\"82\":1}}],[\"plos​μlos​+pnlos​μnlos​\",{\"1\":{\"82\":1}}],[\"plos​\",{\"1\":{\"82\":1}}],[\"ppolicy\",{\"1\":{\"197\":1}}],[\"ppp\",{\"1\":{\"65\":1}}],[\"pp\",{\"1\":{\"65\":2,\"66\":2}}],[\"python等等都是支持正则表达式的\",{\"1\":{\"55\":1}}],[\"pair\",{\"1\":{\"152\":1,\"154\":6,\"155\":1,\"157\":2,\"159\":1}}],[\"pattern\",{\"1\":{\"55\":1}}],[\"package\",{\"1\":{\"16\":3,\"29\":1,\"64\":3,\"65\":2}}],[\"pe\",{\"1\":{\"139\":2,\"141\":1,\"143\":1,\"150\":1}}],[\"periods\",{\"1\":{\"83\":1}}],[\"person\",{\"1\":{\"4\":8,\"5\":3,\"9\":8,\"13\":3,\"16\":1,\"17\":2,\"25\":10,\"27\":3,\"28\":1,\"29\":1,\"50\":5}}],[\"penguin\",{\"1\":{\"28\":1,\"64\":1,\"65\":4}}],[\"problems\",{\"1\":{\"165\":1}}],[\"probability\",{\"1\":{\"92\":1,\"93\":4,\"159\":1,\"171\":1,\"206\":1}}],[\"property\",{\"1\":{\"93\":1}}],[\"proposition1展示了无人机为相关用户提供可靠服务所需的高度的必要条件\",{\"1\":{\"82\":1}}],[\"proposition1\",{\"1\":{\"82\":1}}],[\"proposed\",{\"1\":{\"77\":1}}],[\"process\",{\"0\":{\"93\":1},\"1\":{\"93\":1,\"208\":1}}],[\"profession\",{\"1\":{\"27\":4}}],[\"protected\",{\"1\":{\"17\":2,\"24\":2,\"27\":7,\"65\":1}}],[\"print\",{\"1\":{\"46\":2}}],[\"println\",{\"1\":{\"4\":1,\"5\":1,\"9\":3,\"13\":2,\"17\":1,\"25\":2,\"27\":1,\"28\":2,\"29\":4,\"30\":1,\"36\":3,\"37\":1,\"38\":2,\"45\":1,\"49\":1,\"50\":1,\"51\":1,\"53\":6,\"54\":3,\"55\":2,\"61\":5,\"62\":1,\"63\":1,\"64\":7,\"65\":3,\"66\":1,\"67\":3,\"68\":2}}],[\"private\",{\"1\":{\"17\":2,\"24\":1,\"25\":1,\"27\":2,\"30\":2,\"61\":2,\"62\":1,\"63\":1,\"67\":1}}],[\"p2\",{\"1\":{\"4\":1,\"25\":3}}],[\"p1\",{\"1\":{\"4\":2,\"25\":3}}],[\"p\",{\"1\":{\"4\":4,\"5\":4,\"92\":1,\"93\":3,\"114\":2,\"117\":1,\"118\":2,\"136\":2,\"148\":1,\"150\":4,\"171\":1}}],[\"是未知的\",{\"1\":{\"229\":1}}],[\"是各个\",{\"1\":{\"223\":1}}],[\"是我们需要进行优化的\",{\"1\":{\"221\":1}}],[\"是我们不断进行更新的策略\",{\"1\":{\"194\":1}}],[\"是直接建立一个基于策略的目标函数来进行梯度上升的优化\",{\"1\":{\"220\":1}}],[\"是用来与环境进行交互\",{\"1\":{\"194\":1}}],[\"是qπ​\",{\"1\":{\"189\":1}}],[\"是关于\",{\"1\":{\"185\":1}}],[\"是常数\",{\"1\":{\"183\":1}}],[\"是否成立\",{\"1\":{\"177\":1}}],[\"是否是收敛的\",{\"0\":{\"145\":1}}],[\"是需要被优化的参数\",{\"1\":{\"174\":1}}],[\"是第\",{\"1\":{\"170\":2}}],[\"是针对\",{\"1\":{\"153\":1}}],[\"是针对一条trajectory所求的\",{\"1\":{\"114\":1}}],[\"是优于\",{\"1\":{\"141\":1}}],[\"是已知的\",{\"1\":{\"136\":1}}],[\"是依赖于策略π的\",{\"1\":{\"121\":1}}],[\"是由环境决定的\",{\"1\":{\"118\":1}}],[\"是一致的\",{\"1\":{\"114\":1,\"195\":1}}],[\"是一个期望值\",{\"1\":{\"174\":1}}],[\"是一个随机变量\",{\"1\":{\"174\":1}}],[\"是一个\",{\"1\":{\"170\":1}}],[\"是一个黑盒\",{\"1\":{\"170\":1}}],[\"是一个必要条件\",{\"1\":{\"157\":1}}],[\"是一个contraction\",{\"1\":{\"131\":1}}],[\"是一个有关状态s的函数\",{\"1\":{\"113\":1}}],[\"是一个非负整数\",{\"1\":{\"55\":2}}],[\"是基于一个给定策略\",{\"1\":{\"113\":1}}],[\"是\",{\"1\":{\"113\":1,\"122\":1,\"159\":1,\"186\":1,\"196\":2,\"197\":1,\"223\":1}}],[\"是在基于\",{\"1\":{\"96\":1}}],[\"是在一开始就确定的\",{\"1\":{\"45\":1}}],[\"是根据q\",{\"1\":{\"86\":1}}],[\"是最优的\",{\"1\":{\"157\":1}}],[\"是最大报文长度\",{\"1\":{\"83\":1}}],[\"是最顶层的类\",{\"1\":{\"24\":1}}],[\"是网页大小\",{\"1\":{\"83\":1}}],[\"是与传输速率有关的延迟时间\",{\"1\":{\"83\":1}}],[\"是无法访问到外部类的非静态内容的\",{\"1\":{\"62\":1}}],[\"是属于类的\",{\"1\":{\"62\":1}}],[\"是可以访问到外层的变量的\",{\"1\":{\"61\":1}}],[\"是可以的\",{\"1\":{\"47\":1}}],[\"是对象所有的\",{\"1\":{\"61\":1}}],[\"是对一类事物的描述\",{\"1\":{\"4\":1}}],[\"是匹配所有空白符\",{\"1\":{\"55\":1}}],[\"是不同的\",{\"1\":{\"113\":1,\"196\":1}}],[\"是不是感觉非常简洁\",{\"1\":{\"67\":1}}],[\"是不是有点太浪费了\",{\"1\":{\"54\":1}}],[\"是不能\",{\"1\":{\"47\":1}}],[\"是不支持自动装箱和拆箱的\",{\"1\":{\"47\":1}}],[\"是不允许的\",{\"1\":{\"8\":1}}],[\"是为了完成某件事情而存在的\",{\"1\":{\"5\":1}}],[\"是类的一个具体化个体\",{\"1\":{\"4\":1}}],[\"是某一类事物实际存在的每个个体\",{\"1\":{\"4\":1}}],[\"是抽象的\",{\"1\":{\"4\":1}}],[\"类名\",{\"1\":{\"68\":3}}],[\"类名的首字母通常是大写的\",{\"1\":{\"4\":1}}],[\"类似\",{\"1\":{\"65\":1}}],[\"类似于一个插件\",{\"1\":{\"28\":1}}],[\"类似于c++中的namespace\",{\"1\":{\"16\":1}}],[\"类似于c++指针的情况\",{\"1\":{\"4\":1}}],[\"类的内部类它会单独生成一个\",{\"1\":{\"64\":1}}],[\"类的创建\",{\"1\":{\"4\":1}}],[\"类型的\",{\"1\":{\"50\":1}}],[\"类型\",{\"1\":{\"44\":8}}],[\"类只能继承一个\",{\"1\":{\"28\":1}}],[\"类可以实现这个接口\",{\"1\":{\"28\":1}}],[\"类除了具有属性外\",{\"1\":{\"5\":1}}],[\"类\",{\"0\":{\"24\":1,\"53\":1,\"54\":1,\"62\":1},\"1\":{\"4\":1,\"26\":1,\"59\":1}}],[\"类与对象4\",{\"0\":{\"33\":1}}],[\"类与对象3\",{\"0\":{\"20\":1}}],[\"类与对象2\",{\"0\":{\"12\":1}}],[\"类与对象\",{\"0\":{\"4\":1}}],[\"类与对象1\",{\"0\":{\"3\":1}}],[\"d0​\",{\"1\":{\"225\":1}}],[\"dqn\",{\"0\":{\"214\":1},\"1\":{\"215\":1,\"217\":1}}],[\"dπ​\",{\"1\":{\"208\":2,\"225\":1}}],[\"data\",{\"1\":{\"154\":1}}],[\"daily\",{\"0\":{\"234\":1},\"2\":{\"1\":1}}],[\"daily1\",{\"0\":{\"0\":1}}],[\"dpg\",{\"0\":{\"106\":1}}],[\"difference\",{\"0\":{\"182\":1}}],[\"distributon\",{\"0\":{\"207\":1},\"1\":{\"208\":3}}],[\"distributed\",{\"1\":{\"148\":1}}],[\"distribution\",{\"0\":{\"208\":1},\"1\":{\"93\":1,\"206\":1,\"208\":1,\"225\":1}}],[\"discounted\",{\"1\":{\"92\":1,\"115\":1,\"186\":1,\"210\":1}}],[\"divide\",{\"1\":{\"38\":1}}],[\"dkn​​\",{\"1\":{\"82\":1}}],[\"dkn​​=hn2​\",{\"1\":{\"81\":1}}],[\"dkn​\",{\"1\":{\"82\":1}}],[\"dynamic\",{\"1\":{\"77\":1,\"118\":1}}],[\"driven\",{\"1\":{\"77\":1}}],[\"down\",{\"1\":{\"81\":1}}],[\"doxy\",{\"1\":{\"55\":1}}],[\"does\",{\"1\":{\"55\":1}}],[\"do\",{\"1\":{\"55\":3}}],[\"double\",{\"1\":{\"8\":3,\"36\":2}}],[\"deep\",{\"0\":{\"214\":1},\"1\":{\"214\":1}}],[\"describes\",{\"1\":{\"208\":1}}],[\"descent\",{\"0\":{\"173\":1},\"1\":{\"174\":2}}],[\"design\",{\"0\":{\"76\":1}}],[\"decision\",{\"0\":{\"93\":1}}],[\"decode\",{\"1\":{\"37\":1}}],[\"deterministic\",{\"0\":{\"106\":1,\"178\":1},\"1\":{\"87\":1,\"92\":1,\"126\":2}}],[\"demonstrating\",{\"1\":{\"77\":1}}],[\"deployment\",{\"0\":{\"76\":1},\"1\":{\"77\":1}}],[\"delete\",{\"1\":{\"54\":1}}],[\"default\",{\"1\":{\"28\":2}}],[\"d\",{\"0\":{\"225\":1},\"1\":{\"25\":1,\"27\":1,\"83\":3,\"148\":1,\"223\":2,\"225\":5}}],[\"d1\",{\"2\":{\"2\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(et(t,v[s],n)):e==="search"?self.postMessage(tt(t,v[s],n)):self.postMessage({suggestions:et(t,v[s],n),results:tt(t,v[s],n)})};
//# sourceMappingURL=index.js.map
