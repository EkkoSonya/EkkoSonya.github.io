const L=Object.entries,st=Object.fromEntries,nt="ENTRIES",T="KEYS",R="VALUES",_="";class k{set;_type;_path;constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===_)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==_).join("")}value(){return E(this._path).node.get(_)}result(){switch(this._type){case R:return this.value();case T:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],ot=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return W(e,t,s,n,i,1,o,""),n},W=(e,t,s,n,o,u,i,r)=>{const d=u*i;t:for(const c of e.keys())if(c===_){const a=o[d-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let h=0;h<c.length;++h,++a){const g=c[h],m=i*a,p=m-i;let l=o[m];const f=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let F=f;F<y;++F){const D=g!==t[F],w=o[p+F]+ +D,A=o[p+F+1]+1,z=o[m+F]+1,V=o[m+F+1]=Math.min(w,A,z);V<l&&(l=V)}if(l>s)continue t}W(e.get(c),t,s,n,o,a,i,r+c)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=M(n);for(const i of o.keys())if(i!==_&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ut(this._tree,t)}entries(){return new k(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ot(this._tree,t,s)}get(t){const s=I(this._tree,t);return s!==void 0?s.get(_):void 0}has(t){const s=I(this._tree,t);return s!==void 0&&s.has(_)}keys(){return new k(this,T)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,O(this._tree,t).set(_,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);return n.set(_,s(n.get(_))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);let o=n.get(_);return o===void 0&&n.set(_,o=s()),o}values(){return new k(this,R)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==_&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},I=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==_&&t.startsWith(s))return I(e.get(s),t.slice(s.length))},O=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==_&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const d=e.get(u);if(r===u.length)e=d;else{const c=new Map;c.set(u.slice(r),d),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ut=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(_),s.size===0)q(n);else if(s.size===1){const[o,u]=s.entries().next().value;$(n,o,u)}}},q=e=>{if(e.length===0)return;const[t,s]=M(e);if(t.delete(s),t.size===0)q(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==_&&$(e.slice(0,-1),n,o)}},$=(e,t,s)=>{if(e.length===0)return;const[n,o]=M(e);n.set(o+t,s),n.delete(o)},M=e=>e[e.length-1],it=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},rt=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,S="or",N="and",ct="and_not",lt=(e,t)=>{e.includes(t)||e.push(t)},P=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},G=({score:e},{score:t})=>t-e,ht=()=>new Map,b=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},H=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,dt={[S]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),P(n.terms,u)}}return e},[N]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);P(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[ct]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},at=(e,t,s,n,o,u)=>{const{k:i,b:r,d}=u;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/o)))},ft=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},J=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},gt=(e,t,s,n)=>{if(!e._index.has(n)){J(e,s,t,n);return}const o=e._index.fetch(n,ht),u=o.get(t);u==null||u.get(s)==null?J(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},mt={k:1.2,b:.7,d:.5},pt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(rt),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},U={combineWith:S,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:mt},Ft={combineWith:N,prefix:(e,t,s)=>t===s.length-1},_t={batchSize:1e3,batchWait:10},K={minDirtFactor:.1,minDirtCount:20},yt={..._t,...K},X=Symbol("*"),At=(e,t)=>{const s=new Map,n={...e._options.searchOptions,...t};for(const[o,u]of e._documentIds){const i=n.boostDocument?n.boostDocument(u,"",e._storedFields.get(o)):1;s.set(o,{score:i,terms:[],match:{}})}return s},Y=(e,t=S)=>{if(e.length===0)return new Map;const s=t.toLowerCase(),n=dt[s];if(!n)throw new Error(`Invalid combination operator: ${t}`);return e.reduce(n)||new Map},B=(e,t,s,n,o,u,i,r,d=new Map)=>{if(o==null)return d;for(const c of Object.keys(u)){const a=u[c],h=e._fieldIds[c],g=o.get(h);if(g==null)continue;let m=g.size;const p=e._avgFieldLength[h];for(const l of g.keys()){if(!e._documentIds.has(l)){gt(e,h,l,s),m-=1;continue}const f=i?i(e._documentIds.get(l),s,e._storedFields.get(l)):1;if(!f)continue;const y=g.get(l),F=e._fieldLength.get(l)[h],D=at(y,m,e._documentCount,F,p,r),w=n*a*f*D,A=d.get(l);if(A){A.score+=w,lt(A.terms,t);const z=H(A.match,s);z?z.push(c):A.match[s]=[c]}else d.set(l,{score:w,terms:[t],match:{[s]:[c]}})}}return d},Ct=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((l,f)=>({...l,[f]:H(n.boost,f)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:c,prefix:a}={...U.weights,...i},h=e._index.get(t.term),g=B(e,t.term,t.term,1,h,o,u,d);let m,p;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const l=t.fuzzy===!0?.2:t.fuzzy,f=l<1?Math.min(r,Math.round(t.term.length*l)):l;f&&(p=e._index.fuzzyGet(t.term,f))}if(m)for(const[l,f]of m){const y=l.length-t.term.length;if(!y)continue;p?.delete(l);const F=a*l.length/(l.length+.3*y);B(e,t.term,l,F,f,o,u,d,g)}if(p)for(const l of p.keys()){const[f,y]=p.get(l);if(!y)continue;const F=c*l.length/(l.length+y);B(e,t.term,l,F,f,o,u,d,g)}return g},Q=(e,t,s={})=>{if(t===X)return At(e,s);if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(g=>Q(e,g,a));return Y(h,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:d}=i,c=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(ft(i)).map(a=>Ct(e,a,i));return Y(c,i.combineWith)},Z=(e,t,s={})=>{const n=Q(e,t,s),o=[];for(const[u,{score:i,terms:r,match:d}]of n){const c=r.length||1,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(d),queryTerms:r,match:d};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return t===X&&s.boostDocument==null&&e._options.searchOptions.boostDocument==null||o.sort(G),o},Et=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Z(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=u,d.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:d}]of n)o.push({suggestion:u,terms:r,score:i/d});return o.sort(G),o};class wt{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?yt:t.autoVacuum;this._options={...pt,...t,autoVacuum:s,searchOptions:{...U,...t.searchOptions||{}},autoSuggestOptions:{...Ft,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=K,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new wt(a);h._documentCount=t,h._nextId=s,h._documentIds=b(n),h._idToShortId=new Map,h._fieldIds=o,h._fieldLength=b(u),h._avgFieldLength=i,h._storedFields=b(r),h._dirtCount=d||0,h._index=new C;for(const[g,m]of h._documentIds)h._idToShortId.set(m,g);for(const[g,m]of e){const p=new Map;for(const l of Object.keys(m)){let f=m[l];c===1&&(f=f.ds),p.set(parseInt(l,10),b(f))}h._index.set(g,p)}return h},j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let h="";i===0?h=c.length>20?`… ${c.slice(-20)}`:c:a?h=c.length+i>100?`${c.slice(0,100-i)}… `:c:h=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,h&&o.push(h),i+=h.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let d=s.indexOf(n,u);if(d===-1)return null;for(;d>=0;){const c=d+n.length;if(r(e.slice(u,d)),u=c,i>100)break;d=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},xt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),bt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),tt=(e,t,s={})=>{const n={};return Z(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...s}).forEach(o=>{const{id:u,terms:i,score:r}=o,d=u.includes("@"),c=u.includes("#"),[a,h]=u.split(/[#@]/),g=Number(a),m=i.sort((l,f)=>l.length-f.length).filter((l,f)=>i.slice(f+1).every(y=>!y.includes(l))),{contents:p}=n[g]??={title:"",contents:[]};if(d)p.push([{type:"customField",id:g,index:h,display:m.map(l=>o.c.map(f=>j(f,l))).flat().filter(l=>l!==null)},r]);else{const l=m.map(f=>j(o.h,f)).filter(f=>f!==null);if(l.length&&p.push([{type:c?"heading":"title",id:g,...c&&{anchor:h},display:l},r]),"t"in o)for(const f of o.t){const y=m.map(F=>j(f,F)).filter(F=>F!==null);y.length&&p.push([{type:"text",id:g,...c&&{anchor:h},display:y},r])}}}),L(n).sort(([,o],[,u])=>"max"==="total"?xt(o,u):bt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=it(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},et=(e,t,s={})=>Et(t,e,{fuzzy:.2,...s}).map(({suggestion:n})=>n),v=st(L(JSON.parse("{\"/\":{\"documentCount\":191,\"nextId\":191,\"documentIds\":{\"0\":\"2\",\"1\":\"2@0\",\"2\":\"2@1\",\"3\":\"3\",\"4\":\"3#主要动机\",\"5\":\"3#主要贡献\",\"6\":\"3#主要内容\",\"7\":\"3#系统结构\",\"8\":\"3#基本设置\",\"9\":\"3#信号模型\",\"10\":\"3#quality-of-experience-model\",\"11\":\"3#优化问题建立\",\"12\":\"3#解决方案\",\"13\":\"3#无人机的3d部署\",\"14\":\"3#无人机的动态移动设计\",\"15\":\"3@0\",\"16\":\"3@1\",\"17\":\"4\",\"18\":\"4#强化学习框架图\",\"19\":\"4#_1-基本概念\",\"20\":\"4#_2-markov-decision-process-mdp\",\"21\":\"4@0\",\"22\":\"4@1\",\"23\":\"5\",\"24\":\"5#_1-the-simplest-actor-critic-qac\",\"25\":\"5#_2-advantage-actor-critic-a2c\",\"26\":\"5#_2-1-baseline\",\"27\":\"5#_2-2-最好的-baseline\",\"28\":\"5#_2-3-对应算法\",\"29\":\"5#_3-off-policy-actor-critic\",\"30\":\"5#_3-1-重要性采样-importance-sampling\",\"31\":\"5#_3-2-off-policy\",\"32\":\"5#_3-3-伪代码\",\"33\":\"5#_4-deterministic-actor-critic-dpg\",\"34\":\"5@0\",\"35\":\"5@1\",\"36\":\"6\",\"37\":\"6#核心内容\",\"38\":\"6#_1-state-value\",\"39\":\"6#_1-1\",\"40\":\"6#_1-2-state-value\",\"41\":\"6#_1-3-state-value-与-return-的区别\",\"42\":\"6#_2-bellman-equation\",\"43\":\"6#_2-1-the-mean-of-immediate-rewards\",\"44\":\"6#_2-2-the-mean-of-future-rewards\",\"45\":\"6#_2-3-bellman-equation\",\"46\":\"6#_2-4-bellman-equation-matrix-vector-form\",\"47\":\"6#_3-why-to-slove-state-value\",\"48\":\"6#_4-action-value\",\"49\":\"6#_5-总结\",\"50\":\"6@0\",\"51\":\"6@1\",\"52\":\"7\",\"53\":\"7#_1-optimal-policy\",\"54\":\"7#_2-bellman-optimality-equation-boe\",\"55\":\"7#_2-1-基本形式\",\"56\":\"7#_2-2-如何求解\",\"57\":\"7#_2-2-1-如何处理等式右边的-最优策略\",\"58\":\"7#_2-求解-state-value\",\"59\":\"7@0\",\"60\":\"7@1\",\"61\":\"8\",\"62\":\"8#_1-value-iteration-algorithm\",\"63\":\"8#_1-1-具体步骤\",\"64\":\"8#_1-2-伪代码\",\"65\":\"8#_2-policy-iteration-algorithm\",\"66\":\"8#_2-1-算法描述\",\"67\":\"8#_2-2-伪代码\",\"68\":\"8#_2-3-一些问题\",\"69\":\"8#_3-truncated-policy-iteration-algorithm\",\"70\":\"8#_3-1-value-iteration-与-policy-iteration-算法比较\",\"71\":\"8#_3-2-truncated-policy-iteration-algorithm\",\"72\":\"8#truncated-policy-iteration-algorithm-是否是收敛的\",\"73\":\"8@0\",\"74\":\"8@1\",\"75\":\"9\",\"76\":\"9#_1-mc-basic\",\"77\":\"9#_1-1-算法思路\",\"78\":\"9#_1-2-如何估计\",\"79\":\"9#_1-3-具体算法\",\"80\":\"9#_2-mc-exploring-starts\",\"81\":\"9#_2-1-episode-的高效利用\",\"82\":\"9#_2-2-高效地更新-policy\",\"83\":\"9#_2-3-mc-exploring-starts\",\"84\":\"9#_2-4-exploring-statrts的解释\",\"85\":\"9#_3-mc-eplison-greedy\",\"86\":\"9#_3-1-soft-policy\",\"87\":\"9#_3-2-greedy-policy\",\"88\":\"9#_3-3-greedy-policy-引入-mc-based-算法中\",\"89\":\"9#_3-3-算法流程\",\"90\":\"9@0\",\"91\":\"9@1\",\"92\":\"10\",\"93\":\"10#_1-引言\",\"94\":\"10#_1-1-求均值的方法\",\"95\":\"10#_2-robbins-monto-rm-algorithm\",\"96\":\"10#_2-1-问题引入\",\"97\":\"10#_2-2-算法介绍\",\"98\":\"10#_2-3-收敛性分析\",\"99\":\"10#_2-4-应用于-mean-estimation-中\",\"100\":\"10#_3-stochastic-gradient-descent\",\"101\":\"10#_3-1-问题引入\",\"102\":\"10#_3-2-sgd-分析\",\"103\":\"10#mean-estimation-问题转化\",\"104\":\"10#sgd-正确性和收敛性分析\",\"105\":\"10#_3-3-sgd-另一种问题描述方法-deterministic-formulation\",\"106\":\"10#_3-4-bgd-mbgd-sgdw\",\"107\":\"10@0\",\"108\":\"10@1\",\"109\":\"11\",\"110\":\"11#_1-引入\",\"111\":\"11#_2-td-learning-of-state-value\",\"112\":\"11#_2-1-算法描述\",\"113\":\"11#_2-2-算法分析\",\"114\":\"11#_2-3-td-算法-与-mc-算法的比较\",\"115\":\"11#_3-td-learning-of-action-value\",\"116\":\"11#_3-1-sarsa\",\"117\":\"11#_3-2-n-step-sarsa\",\"118\":\"11#_3-3-expected-sarsa\",\"119\":\"11#_4-td-learning-of-optimal-action-value\",\"120\":\"11#_4-1-q-learning\",\"121\":\"11#_4-2-off-policy-on-policy\",\"122\":\"11#on-policy\",\"123\":\"11#off-policy\",\"124\":\"11#_4-3-q-learning-伪代码\",\"125\":\"11#off-poicy-版本\",\"126\":\"11#on-policy-版本\",\"127\":\"11#_5-td-算法的统一形式和总结\",\"128\":\"11@0\",\"129\":\"11@1\",\"130\":\"12\",\"131\":\"12#_1-引入\",\"132\":\"12#_2-alogorithm-of-state-value-estimation\",\"133\":\"12#_2-1-obejctive-function\",\"134\":\"12#uniform-distributon\",\"135\":\"12#stationary-distribution\",\"136\":\"12#_2-2-optimization-algorithms-优化算法\",\"137\":\"12#monte-carlo-learning-with-function-approximation\",\"138\":\"12#td-learning-with-function-approximation\",\"139\":\"12#_3-sarsa-with-function-approximation\",\"140\":\"12#_4-q-learning-with-function-approximation\",\"141\":\"12#_5-deep-q-learning-dqn\",\"142\":\"12#优化方法\",\"143\":\"12#经验回放-replay-buffer\",\"144\":\"12#伪代码\",\"145\":\"12@0\",\"146\":\"12@1\",\"147\":\"13\",\"148\":\"13#_1-基本思路\",\"149\":\"13#_2-目标函数定义\",\"150\":\"13#_2-1-average-state-value\",\"151\":\"13#另一种表达\",\"152\":\"13#d-s-的选择\",\"153\":\"13#_2-2-average-return-value\",\"154\":\"13#另一种表达-1\",\"155\":\"13#_3-目标函数梯度求解\",\"156\":\"13#_4-reinforce-梯度上升算法\",\"157\":\"13#reinforce-算法\",\"158\":\"13@0\",\"159\":\"13@1\",\"160\":\"14\",\"161\":\"14#类与对象\",\"162\":\"14#方法的创建与使用\",\"163\":\"14#方法的进阶使用\",\"164\":\"14#this-的使用\",\"165\":\"14#方法的重载\",\"166\":\"14#构造方法\",\"167\":\"14@0\",\"168\":\"14@1\",\"169\":\"15\",\"170\":\"15#静态变量和静态方法\",\"171\":\"15#静态变量初始化\",\"172\":\"15#包的访问与控制\",\"173\":\"15#包的声明和导入\",\"174\":\"15#访问权限控制\",\"175\":\"15@0\",\"176\":\"15@1\",\"177\":\"16\",\"178\":\"16#封装-继承和多态\",\"179\":\"16#封装\",\"180\":\"16#继承\",\"181\":\"16#多态\",\"182\":\"16@0\",\"183\":\"16@1\",\"184\":\"17\",\"185\":\"18\",\"186\":\"19\",\"187\":\"20\",\"188\":\"21\",\"189\":\"22\",\"190\":\"23\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,1],\"1\":[null,null,1],\"2\":[null,null,1],\"3\":[10,6],\"4\":[1,45],\"5\":[1,29],\"6\":[1],\"7\":[1],\"8\":[1,44],\"9\":[1,105],\"10\":[4,82],\"11\":[1,34],\"12\":[1],\"13\":[1,157],\"14\":[1,67],\"15\":[null,null,1],\"16\":[null,null,6],\"17\":[2],\"18\":[1,1],\"19\":[2,118],\"20\":[6,54],\"21\":[null,null,1],\"22\":[null,null,1],\"23\":[4,19],\"24\":[7,2],\"25\":[6,8],\"26\":[3,28],\"27\":[3,8],\"28\":[3,3],\"29\":[5,10],\"30\":[6,3],\"31\":[4,4],\"32\":[2,1],\"33\":[6,3],\"34\":[null,null,1],\"35\":[null,null,1],\"36\":[2],\"37\":[1,5],\"38\":[3],\"39\":[1,16],\"40\":[4,27],\"41\":[7,36],\"42\":[3,35],\"43\":[8,10],\"44\":[7,23],\"45\":[4,47],\"46\":[8,34],\"47\":[6,34],\"48\":[3,72],\"49\":[2,29],\"50\":[null,null,1],\"51\":[null,null,1],\"52\":[2,16],\"53\":[3,36],\"54\":[6],\"55\":[3,28],\"56\":[2,8],\"57\":[5,21],\"58\":[4,75],\"59\":[null,null,1],\"60\":[null,null,1],\"61\":[4,6],\"62\":[4,18],\"63\":[2,53],\"64\":[3,1],\"65\":[4,3],\"66\":[3,70],\"67\":[2,1],\"68\":[3,44],\"69\":[5,6],\"70\":[7,71],\"71\":[6,23],\"72\":[5,1],\"73\":[null,null,1],\"74\":[null,null,1],\"75\":[6,51],\"76\":[3,16],\"77\":[2,61],\"78\":[3,33],\"79\":[3,49],\"80\":[4,10],\"81\":[4,63],\"82\":[3,52],\"83\":[5,1],\"84\":[4,33],\"85\":[4,9],\"86\":[4,25],\"87\":[4,39],\"88\":[7,12],\"89\":[2,1],\"90\":[null,null,1],\"91\":[null,null,1],\"92\":[2,33],\"93\":[2],\"94\":[2,32],\"95\":[5],\"96\":[3,17],\"97\":[2,31],\"98\":[3,44],\"99\":[6,41],\"100\":[4],\"101\":[3,57],\"102\":[4],\"103\":[3,8],\"104\":[2,67],\"105\":[6,11],\"106\":[5,1],\"107\":[null,null,1],\"108\":[null,null,1],\"109\":[4],\"110\":[2,50],\"111\":[6,10],\"112\":[3,36],\"113\":[2,37],\"114\":[7],\"115\":[6,23],\"116\":[3,37],\"117\":[5,3],\"118\":[3,2],\"119\":[7,21],\"120\":[4,24],\"121\":[6,7],\"122\":[2,9],\"123\":[2,17],\"124\":[5,17],\"125\":[3,10],\"126\":[3,1],\"127\":[3,2],\"128\":[null,null,1],\"129\":[null,null,1],\"130\":[6,11],\"131\":[2,12],\"132\":[6,9],\"133\":[4,15],\"134\":[2,22],\"135\":[2,52],\"136\":[4,40],\"137\":[6,19],\"138\":[5,15],\"139\":[5,10],\"140\":[6,13],\"141\":[6,21],\"142\":[1,110],\"143\":[4,3],\"144\":[1,13],\"145\":[null,null,1],\"146\":[null,null,1],\"147\":[5,10],\"148\":[2,38],\"149\":[2],\"150\":[5,22],\"151\":[1,5],\"152\":[3,21],\"153\":[4,20],\"154\":[1,27],\"155\":[2,60],\"156\":[3,40],\"157\":[2,1],\"158\":[null,null,1],\"159\":[null,null,1],\"160\":[2],\"161\":[1,85],\"162\":[1,56],\"163\":[1],\"164\":[2,23],\"165\":[1,20],\"166\":[1,75],\"167\":[null,null,1],\"168\":[null,null,1],\"169\":[2],\"170\":[1,52],\"171\":[1,27],\"172\":[1],\"173\":[1,90],\"174\":[1,60],\"175\":[null,null,1],\"176\":[null,null,1],\"177\":[2],\"178\":[2],\"179\":[1],\"180\":[1],\"181\":[1],\"182\":[null,null,1],\"183\":[null,null,1],\"184\":[1,3],\"185\":[1],\"186\":[1],\"187\":[1],\"188\":[1],\"189\":[1],\"190\":[1]},\"averageFieldLength\":[3.208315059982989,29.50948139313956,1],\"storedFields\":{\"0\":{\"h\":\"daily1\",\"t\":[\"a+b=c\"]},\"1\":{\"c\":[\"daily\"]},\"2\":{\"c\":[\"d1\"]},\"3\":{\"h\":\"Reinforcement Learning in Multiple-UAV Networks:Deployment and Movement Design\",\"t\":[\"2019 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY\"]},\"4\":{\"h\":\"主要动机\",\"t\":[\"A novel framework is proposed for quality of experience driven deployment and dynamic movement of multiple unmanned aerial vehicles (UAVs).\",\"过去研究大多没有基于用户的移动(movement of users)来考虑无人机的机动性，更多地是考虑多架无人机的二维部署或单架无人机在地面用户保持静止情况下的部署。\",\"考虑QoE, 而不是仅考虑吞吐量(throughput)，即需要考虑地面不同用户的具体需求。(QoE is invoked for demonstrating the users’ satisfaction, and it is supposed to be considered in UAV-assisted wireless networks)\",\"该文设计的是3D部署，过去研究主要考虑的是2D部署。\"]},\"5\":{\"h\":\"主要贡献\",\"t\":[\"提出了一个理想的由QoE驱动的多无人机协助通信框架。该框架将无人机部署在三维空间内，以 mean opinion score(MOS) 为指标。通过优化无人机的部署和动态移动来解决总用户MOS最大化问题。\",\"提出解决总用户MOS最大化问题的三步骤: \",\"通过GAK-mean算法获得初始单元划分。\",\"设计一种基于 q-learning 的部署方法，在初始时间假设用户处于静止下不断调整 UAVs 3D位置进行优化处理。\",\"设计一种基于 q-learning 的无人机3D动态运动设计算法。\",\"该文基于q-learning的方案来解决无人机的NP-hard 3D部署和移动问题，并与传统的基于遗传的学习算法进行对比。\",\"该文提出的算法具较快的收敛性，与K-means和IGK算法比具有较低的复杂度。\"]},\"6\":{\"h\":\"主要内容\"},\"7\":{\"h\":\"系统结构\"},\"8\":{\"h\":\"基本设置\",\"t\":[\"考虑无人机辅助无线网络的下行链路传输(down-link transmission)，即无人机作为空中基站。\",\"对于指定区域，会将其划分为N个簇，其中用户表示为K=K1​,…,KN​，其中KN​表示划分到集群N的用户，N∈1,2,…,N。\",\"每个用户只能属于一个集群，Kn​∩Kn′​=ϕ,n′=n,\",\"在任意时刻t，同一无人机通过FDMA同时为同一集群中的多个用户提供服务\",\"对于用户kn​∈Kn​，其坐标表示为wkn​​=[xkn​​(t),ykn​​(t)]T∈R2×1\",\"对于无人机n(飞行速度恒定)，其垂直高度表示为hn​(t)∈[hmin​,hmax​],0≤t≤Ts​，其水平坐标表示为qn​(t)=[xn​(t),yn​(t)]T∈R2×1,0≤t≤Ts​\",\"无人机n与用户kn​在时间t的距离表示为:\",\"dkn​​=hn2​(t)+[xn​(t)−xkn​​(t)]2+[yn​(t)−ykn​​(t)]2​\"]},\"9\":{\"h\":\"信号模型\",\"t\":[\"无人机往往有更高的LoS链接概率，该文中表示为:\",\"PLoS​(θkn​​)=b1​(π180​θkn​​−ζ)b2​PNLoS​=1−PLoS​\",\"其中θkn​​(t)=sin−1[dkn​(t)​hn​(t)​]，表示无人机与用户之间的仰角。b1​,b2​,ζ是由环境决定的常数。在实际应用中，为了在LoS信道概率和路径损耗之间取得平衡，需要合理选择无人机n的垂直高度hn​(t)。\",\"在时间t，从无人机n到用户kn​的信道功率增益(the channel power gain)为:\",\"gkn​​(t)=K0​−1dkn​​−α[t](PLos​μLoS​+PNLos​μNLoS​)−1\",\"其中K0​=(c4πfc​​)2，α是表示路径损耗指数(常数)，μLoS​,μNLoS​是表示LoS和NLoS链路的衰减因子，fc​是载波频率，c是光速。\",\"对于无人机n，其可用带宽为Bn​，将其平均分配给其∣Kn​∣个关联用户，其每个用户带宽表示为: Bkn​​=Bn​/Kn​. 该文中不同集群所利用的频谱是不同的，且无人机向关联用户的发射功率是恒定的。 同样，对于无人机的总发射功率也均匀地分配给每个用户，pkn​​=Pmax​/Kn​\",\"由于不同集群的频谱不同，可以减轻无人机对用户接收到的干扰。因此，在时刻t关联到无人机n的地面用户kn​的接受到的信噪比表示为:\",\"Γkn​​(t)=σ2pkn​​gkn​​(t)​\",\"其中σ2=Bkn​​N0​, N0​为用户所在位置的加性高斯白噪声(AWGN)的功率谱密度。\",\"为了满足不同用户传输速率要求，对于用户kn​存在特定的信噪比目标γkn​​, 即Γ≥γkn​​.\",\"由此，存在Lemma1： 为了保证所有用户都能连接到网络，我们对无人机的发射功率有一个约束，可以表示为\",\"Pmax​≥γσ2K0​dkn​​α(t)μNLoS​\",\"根据香农定理: 信道容量C=B∗log(1+NS​)，且传输率永远都不可能超过信道容量C。 因此对于用户kn​的在时刻t的传输速率rkn​​(t)，表示为rkn​​(t)=Bkn​​log2​[1+σ2pkn​​gkn​​(t)​].\",\"Proposition1: 无人机n的高度需满足:\",\"dkn​​(t)sin[180π​(ζ+eM(t))]≤hn​(t)≤(γK0​σ2μLoS​Pmax​​)\",\"其中\",\"M(t)=b2​ln(b1​(μLoS​−μNLoS​)S(t)​−μLoS​−μNLoS​μNLoS​​​S(t)=γK0​σ2dkn​​α(t)Pm​ax​\",\"Proposition1展示了无人机为相关用户提供可靠服务所需的高度的必要条件。 可知，其高度的下界是距离dkn​​(t)的函数；高度的上界是最大发射功率Pmax​的函数。 因此，随着无人机与用户之间距离和发射功率的变化，需要调整相应无人机的高度，以向用户提供可靠的服务。\"]},\"10\":{\"h\":\"Quality-of-Experience Model\",\"t\":[\"由于不同用户对于传输速率的需求是不同的，所以在无人机辅助通信网络中我们需要考虑QoE模型。\",\"在该文中，采用MOS作为用户QoS衡量的标准，具体如下:\",\"MOSkn​​(t)=ζ1​MOSkn​​delay(t)+ζ2​MOSkn​​rate(t)\",\"其中，ζ1​,ζ2​是系数，且ζ1​+ζ2​=1。\",\"根据MOS数值，共划分5个等级: excellent(4.5) very good(2~3.5) fair(1~2) poor(1)。\",\"在该文中考虑的是网页浏览应用传输情况，因此MOSkn​​delay(t)可以忽略，因此，此时的MOS模型定义如下:\",\"MOSkn​​(t)=−C1​ln[d(rkn​​(t))]+C2​\",\"d(rkn​​(t))是与传输速率有关的延迟时间，MOSkn​​(t)为t时刻的MOS评分，取值范围从1−4.5。C1​和C2​是通过分析web浏览应用程序的实验结果确定的常数，分别设为1.120和4.6746。\",\"d(rkn​​(t))=3RTT+rkn​​(t)FS​+L(rkn​​MSS​)+RTT−rkn​​(t)2MSS(2L−1)​\",\"其中，RTT[s]表示round trip time(数据包从发送端-接收端-发送端的时间)，FS[bit]是网页大小，MSS[bit]是最大报文长度，L=min[L1​,L2​]表示 the number of slow start cycles with idle periods。\",\"L1​=log2​(MSSrkn​​RTT​+1)−1,L2​=log2​(2MSSFS​+1)−1.\",\"用户rkn​​在一段时间Ts​内的MOS总和为:\",\"MOSrkn​​​=t=0∑Ts​​MOSkn​​(t)\"]},\"11\":{\"h\":\"优化问题建立\",\"t\":[\"假设功率Q=qn​(t),0≤t≤Ts​, 高度H=hn​(t),0≤t≤Ts​\",\"本文目的是优化无人机在每个时隙的位置，从而最大化所有用户的总MOS值。具体表述如下:\",\"C,Q,Hmax​MOStotal​=∑n=1N​∑kn​=1Kn​​∑t=0Ts​​MOSkn​​(t)s.t.Kn​∩Kn′​=ϕ,n′=n,∀n,hmin​≤hn​(t)≤hmax​,∀t,∀n,Γkn​(t)​≥γkn​​,∀t,∀kn​,∑kn​=1Kn​​pkn​​(t)≤Pmax​,∀t,∀kn​,pkn​(t)​≥0,∀kn​,∀t,​\",\"该优化问题是一个non-convex问题，因为目标函数对于无人机的3D坐标是非凸的。\",\"总用户的MOS取决于无人机的发射功率、数量和位置(水平位置和高度)。\"]},\"12\":{\"h\":\"解决方案\"},\"13\":{\"h\":\"无人机的3D部署\",\"t\":[\"考虑以下场景，将上述优化问题简化:\",\"无人机n以可变高度悬停在用户上方，用户是保持静态的。 每架无人机的带宽和发射功率都均匀分配给每个用户。 因此我们将优化问题简化为区域分割问题。\",\"描述如下: 但即使仅考虑用户聚类，该问题依然是NP-hard问题\",\"C,Q,Hmax​MOStotal​=∑n=1N​∑kn​=1Kn​​MOSkn​​(t)s.t.Kn​∩Kn′​=ϕ,n′=n,∀n,hmin​≤hn​(t)≤hmax​,∀t,∀n,Γkn​(t)​≥γkn​​,∀t,∀kn​,∑kn​=1Kn​​pkn​​(t)≤Pmax​,∀t,∀kn​,pkn​(t)​≥0,∀kn​,∀t,​\",\"无人机-用户关联策略(用户区域划分算法)\",\"采用基于遗传算法的GAK-means算法 由于特定用户的MOS与该用户与无人机之间的距离有关，因此GAK-means可以视为获得无人机部署的低复杂度方案。\",\"根据N个用户，根据遗传算法找到CN​个最优个体作为簇的中心。\",\"将无人机部署在每个中心内，再将用户划分给距离最近的无人机\",\"重复步骤，再找到新的簇的各中心，再根据欧几里得距离重新划分，直到各个簇的成员没有太大变化，划分完毕。\",\"无人机3D部署算法\",\"根据所给定的用户划分情况，目标是获得无人机的最佳3D位置，来最大化MOS总和。 由于GAK-means的优化目标是最小化无人机与对应集群用户的欧氏距离，MOS主要是有关传输速率rkn​​的函数，因此MOS不仅与欧氏距离有关，还与LoS的概率有关。\",\"采用Q-learning算法\",\"智能体(agent): UAVn,n∈N={1,2,…,N}\",\"状态(state): 对于每个智能体，其状态为其3D坐标，定义为ξ=(xUAV​,yUAV​,hUAV​)\",\"状态空间(state space S): 这里采用离散化空间坐标，即xUAV​:{0,1,…,Xd​},yUAV​:0,1,…,Yd​,hUAV​:{hmin​,…,hmax​}，所以状态其实共有(XD​+1)×(Yd​+1)×(hmax​−hmin​+1)个\",\"动作空间(action space): 每次无人机会根据当前状态st​∈S，按照所给定策略J来执行一个动作at​∈A从而获得奖励rt​以及下一个状态st+1​ 该论文中在精度和模型复杂型上作出平衡，共考虑7个方向。 (1,0,0)：右转 (−1,0,0)：左转 (0,1,0)：前进 (0,−1,0)：后退 (0,0,1)：上行 (0,0,−1)：下行 (0,0,0)：静止\",\"状态转换模型: 当执行动作at​时，从状态st​到st+1​，并获得奖励rt​的这一过程可以用条件转移概率p(st+1​,rt​∣st​,at​)来表示。 Q-learning的优化目标是最大化长期收益\",\"Gt​=E[n=0∑∞​βnrt+n​]\",\"奖励(reward): 如果agent在当前时刻t所执行的动作能够提高总MOS，则无人机将获得正奖励。否则，agent将获得负奖励。\",\"xt​=⎩⎨⎧​1,−0.1,−1,​ifMOSnew​>MOSold​ifMOSnew​=MOSold​ifMOSnew​<MOSold​​\",\"具体代码：（策略为贪心策略）\",\"算法1\",\"个人理解：\",\"通过K-means来划分各个无人机所管理的用户簇。无人机的位置初始化也是随机部署的\",\"但每个无人机所管理的用户不同，其目标也应该不一样，不能用同一个Q-table管理，这里是每个无人机都有一张自己的Q-table，来进行迭代？ 还是同一张Q-table，只不过根据区域划分，不同的无人机agent的Q(s,a)的s是有范围的？(个人感觉是这个)\",\"最终输出的结果，应该是无人机最终停的位置即是部署的最佳位置(因为q-learning是优化长期目标)，发现在该位置静止是最优的，表示是最佳部署位置。\",\"最终输出结果，是根据Q-table来找出对应q(s,a)当a为静止时，最大的q(s,a)值，对应s就是UAV的部署位置\"]},\"14\":{\"h\":\"无人机的动态移动设计\",\"t\":[\"考虑用户在每个时隙移动的情况，由于用户在每个时隙都处于漫游状态，因此随着用户位置的变化，每个集群中无人机的最优位置也会发生变化，无人机需要进行移动。\",\"在本文中不考虑用户移动到其他集群的情况 因为在不考虑用户自由穿梭集群的情况，对于动作空间而言，仅需要考虑无人机的7个移动方向即可；但若考虑集群情况，动作空间包含两个部分：选择移动方向和选择关联用户。设无人机总数为N，∣Kn​∣为第n个簇的用户总数，则用户的关联动作数为2N∑n=1N​∣Kn​∣，∑n=1N​∣Kn​∣是总用户数，每个用户都需要判断是否与每个无人机关联，因此是2N 则总动作空间的大小为7+2N∑n=1N​∣Kn​∣会导致动作空间过大，Q-table过大。\",\"1.用户漫游模型 在设计无人机的移动之前，需考虑用户的移动性，这里有多种mobility modles可选择，如a deterministic approach, a hybrid approach, and a random walk model. 在本文中，采用的是the random walk model(Markovian mobility model) 每个用户的移动方向均匀分布在左、右、前、后四个方向。 用户的速度设为[0,cmax​]，其中cmax​表示用户的最大速度。\",\"2.基于q-learning的移动算法 与基于q-learning的部署算法不同的是，在此情况下，状态除了要考虑无人机的3D位置外，还需要考虑所有用户的2D位置。即ξ={xUAV​,yUAV​,hUAV​,xuser​,yuser​}(xuser​,yuser​)由用户的初始位置和运动模型决定，(xUAV​,yUAV​,hUAV​)由无人机的位置和它们在最后时隙采取的动作决定.\",\"训练阶段: \",\"测试阶段:\\n\"]},\"15\":{\"c\":[\"academic\"]},\"16\":{\"c\":[\"UAV\",\"IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY\"]},\"17\":{\"h\":\"RL1 - 基本概念\"},\"18\":{\"h\":\"强化学习框架图\",\"t\":[\"主要框架\"]},\"19\":{\"h\":\"1. 基本概念\",\"t\":[\"State(状态)：The status of the agent with respect to the environment.\",\"State Space(状态空间): 所有状态的集合。S={si​}i=1n​。\",\"Action(动作): 对于每一个状态，都有可选择的动作。\",\"Action space of a state: 对应状态中所有可选择的动作集合。A(si​)={ai​}i=1n​\",\"State transition(状态转换): s1​→a1​s2​。定义了agent与环境的交互行为。\",\"State transition probability: p(s2​∣s1​,a1​)，即状态s1​采用动作a1​转到状态s2​的概率。\",\"Policy π: 指导agent在当前状态下选择哪个动作。\",\"Reward(奖励): 在执行一个动作后获得的一个常数(依赖于当前状态和所采取的动作)。同样可以用条件概率的形式进行描述，如p(r=1∣s1​,a1​)，即在状态s1​下采用动作a1​获得的奖励r=1的概率。\",\"Trajectory：a state-action-reward chain.(可以有限，也可以是无限长的trajectory) s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​. 个人理解，trajectory是在策略给定下，agent可能走出的全部轨迹，并非只是一个单一的轨迹。\",\"Return of a trajectory：将对应的轨迹所获得的所有reward的总和，可以粗步衡量一个策略的好坏。\",\"Discounted return(of a trajectory)：为了应对具有无限步的trajectory的return=∞的情况。 s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​r=1→​a2​​s9​r=1→​a2​​s9​…. 此时该trajectory的return=0+0+0+1+1+⋯=∞。 引入discount rate, γ∈[0,1). 此时对应的discountedrate=0+γ0+γ20+γ31+γ41+⋯=γ31−γ1​ 显然，如果γ接近0，即此时的discounted return越短视，注重近期的reward；γ接近1，更远视，更注重长远的reward。\",\"Episode(trial)：When interacting with the environment following a policy, the agent may stop at some terminal states. The resulting trajectory is called an episode(or a trial)/ 即表示具有终止状态terminal states的trajectory，通常是具有有限步长的trajectory. 同理，这样的任务称为episodic tasks。\",\"continuing tasks：即不具备terminal states的任务，会与环境一直交互下去。 可以通过设置将episodic tasks转换成continuing tasks，如可以在target states中限制action space，控制其一直待在target states中。 Deterministic — Stochastic\"]},\"20\":{\"h\":\"2.Markov decision process(MDP)\",\"t\":[\"关键元素：\",\"Sets： \",\"State：the set of states S\",\"Action：the set of actions A(s) is associate for state s∈S\",\"Reward：the set of rewards R(s,a).\",\"Probability distribution： \",\"State transition probability p(s′∣s,a): 表示在状态s下采取动作a，转换到状态s′的概率。\",\"Reward probability p(r∣s,a): 表示在状态s下采取动作a，获得reward r 的概率。\",\"Policy：at state s, the probability to choose action a is π(a∣s). 表示在各状态执行各动作的概率。\",\"Markov property：即无记忆的特性。 p(st+1​∣at+1​,st​,…,a1​,s0​)=p(st+1​∣at+1​,st​)r(st+1​∣at+1​,st​,…,a1​,s0​)=p(rt+1​∣at+1​,st​)\",\"Markov process：在policy是确定的情况下，MDP就变为MP。\"]},\"21\":{\"c\":[\"academic\"]},\"22\":{\"c\":[\"强化学习\"]},\"23\":{\"h\":\"RL10 - Actor-Critic 方法\",\"t\":[\"actor: 对应 policy update\",\"critic: 对应 policy evaluation 或者 value evaluation\",\"20240830184236\",\"显然，是在基于 策略梯度上升 算法的基础上，将对于 Q 值的估计通过一个网络来进行描述，这个便成为 critic, 而对应的策略梯度上升算法就是对应 actor。\",\"20240830184312\"]},\"24\":{\"h\":\"1. The simplest actor-critic (QAC)\",\"t\":[\"20240830184330\",\"20240830184424\"]},\"25\":{\"h\":\"2. Advantage actor-critic (A2C)\",\"t\":[\"核心思想：在 QAC 的基础上来引入偏置量(baseline)，从而减小方差，提升采样的效率。\"]},\"26\":{\"h\":\"2.1 baseline\",\"t\":[\"在策略梯度算法中引入一个 baseline, 不会影响所求的梯度。 即:\",\"▽θ​J(θ)​=ES∼η,A∼π​[▽θ​ln(A∣S,θ)qπ​(S,A)]=ES∼η,A∼π​[▽θ​ln(A∣S,θ)qπ​(S,A)−b(S)]​\",\"证明: 要证明加入baseline成立，只需要保证:\",\"ES∼η,A∼π​[▽θ​ln(A∣S,θ)b(S)]=0\",\"20240830185127\",\"作用:\",\"因此，我们需要找到一个 baseline 来保证这个梯度的方差最小即可。\"]},\"27\":{\"h\":\"2.2 最好的 baseline\",\"t\":[\"20240830185324\",\"在实际情况中，我们通常将 baseline 设置为 vπ​(s)\"]},\"28\":{\"h\":\"2.3 对应算法\",\"t\":[\"20240830185537\",\"20240830185556\",\"20240830185629\"]},\"29\":{\"h\":\"3. off-policy actor-critic\",\"t\":[\"通过 重要性采样 的方法，将处于 另一分布下 的策略所采集的数据来 运用到 策略更新 中。\"]},\"30\":{\"h\":\"3.1 重要性采样 (Importance sampling)\",\"t\":[\"20240830200056\",\"20240830200118\",\"20240830200138\"]},\"31\":{\"h\":\"3.2 off-policy\",\"t\":[\"20240830200248\",\"20240830200305\",\"20240830200320\",\"20240830200343\"]},\"32\":{\"h\":\"3.3 伪代码\",\"t\":[\"20240830200406\"]},\"33\":{\"h\":\"4. Deterministic actor-critic (DPG)\",\"t\":[\"1234\",\"20240830200608\",\"20240830200624\"]},\"34\":{\"c\":[\"academic\"]},\"35\":{\"c\":[\"强化学习\"]},\"36\":{\"h\":\"RL2 - 贝尔曼公式\"},\"37\":{\"h\":\"核心内容\",\"t\":[\"state value\",\"the Bellman equation\"]},\"38\":{\"h\":\"1.State value\"},\"39\":{\"h\":\"1.1\",\"t\":[\"引入随机变量后对应的discounted return的描述。 即一个trajectory下的discounted return。 由此可以推导出一个多步的trajectory:\",\"St​→At​Rt+1​,St+1​→At+1​Rt+2​,St+2​→At+2​Rt+3​,St+2​→At+3​…\",\"对应的discounted return为：Gt​=Rt+1​+γRt+2​+γ2Rt+3​+…\",\"γ 为discounted rate\",\"Gt​也是一个随机变量\"]},\"40\":{\"h\":\"1.2 State value\",\"t\":[\"State value 是 Gt​ 的期望, 也称为 state value function 表示为 The expection(expected value or mean) of Gt​:\",\"vπ​(s)=E[Gt​∣St​=s]\",\"是一个有关状态s的函数.\",\"vπ​(s) 是基于一个给定策略 π , 对于不同的策略，所得到的 state value 是不同的.\",\"state value 可以用来衡量一个状态的价值.\"]},\"41\":{\"h\":\"1.3 State value 与 return 的区别\",\"t\":[\"Return 是针对一条trajectory所求的，而 State value 则是对多个 trajectory 求 return 再求平均值。 The state value is the mean of all possible returns that can be obtained starting from a state. 只有当所有东西都是确定性的(π(a∣s),p(r∣s,a),p(s′∣s,a))，state value 与 return 是一致的.\"]},\"42\":{\"h\":\"2. Bellman equation\",\"t\":[\"用来描述所有状态的state value的关系. 根据一个 random trajectory:\",\"St​→At​Rt+1​,St+1​→At+1​Rt+2​,St+2​→At+2​Rt+3​,St+2​→At+3​…\",\"对应的 discounted return Gt​ 为:\",\"Gt​​=Rt+1​+γRt+2​+γ2Rt+3​+…=Rt+1​+γ(Rt+2​+γRt+3​+…)=Rt+1​+γGt+1​​\",\"因此，对应的 state value 为:\",\"vπ​(s)​=E[Gt​∣St​=s]=E[Rt+1​+γGt+1​∣St​=s]=E[Rt+1​∣St​=s]+γE[Gt+1​∣St​=s]​\",\"需要推导E[Rt+1​∣St​=s]和E[Gt+1​∣St​=s]的计算即可。\"]},\"43\":{\"h\":\"2.1 The mean of immediate rewards:\",\"t\":[\"E[Rt+1​∣St​=s]​=a∑​π(a∣s)E[Rt+1​∣St​=s,At​=a]=a∑​π(a∣s)r∑​p(r∣s,a)r​\"]},\"44\":{\"h\":\"2.2 The mean of future rewards:\",\"t\":[\"E[Gt+1​∣St​=s]​=s′∑​E[Gt+1​∣St​=s,St+1​=s′]=s′∑​E[Gt+1​∣St+1​=s′](无记忆性)=s′∑​vπ​(s′)p(s′∣s)=s′∑​vπ​(s′)a∑​p(s′∣s,a)π(a∣s)​\",\"个人推导：\",\"E[Gt+1​∣St​=s]​=a∑​π(a∣s)E[Gt+1​∣St​=s,At​=a]=a∑​π(a∣s)s′∑​E[Gt+1​∣St​=s,At​=a,St+1​=s′]=a∑​π(a∣s)s′∑​p(s′∣s,a)E[Gt+1​∣St+1​=s′]=a∑​π(a∣s)s′∑​p(s′∣s,a)vπ​(s′)​\"]},\"45\":{\"h\":\"2.3 Bellman equation\",\"t\":[\"vπ​(s)​=E[Rt+1​∣St​=s]+γE[Gt+1​∣St​=s],=mean of immediate rewards a∑​π(a∣s)r∑​p(r∣s,a)r​​+mean of future rewards γa∑​π(a∣s)s′∑​p(s′∣s,a)vπ​(s′)​​,=a∑​π(a∣s)[r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)],∀s∈S.​\",\"该式子针对状态空间中的所有状态均成立.\",\"通过 Bootstrapping , 可以求解 state value.\",\"π(a∣s) 表示一个给定的策略. 求解Bellman equation 称为策略评估(Policy evaluation).\",\"p(r∣s,a),p(s′∣s,a) 是由环境决定的(dynamic model|environment model). 后续可能是未知的(model-free)，需要通过采样解决.\"]},\"46\":{\"h\":\"2.4 Bellman equation (Matrix-vector form)\",\"t\":[\" 此时,对于所有状态s，对应的 Bellman equation 为\",\"vπ​(s)=rπ​(s)+γs′∑​pπ​(s′∣s)vπ​(s′)​\",\"将所有状态的 Bellman equation 整合，重新修改为 matrix-vector form.\",\"vπ​=rπ​+γPπ​vπ​​\",\"其中,\",\"vπ​=[vπ​(s1​),…,vπ​(sn​)]T∈Rn\",\"rπ​=[rπ​(s1​),…,rπ​(sn​)]T∈Rn\",\"Pπ​∈Rn×n, where [Pπ​]ij​=pπ​(sj​∣si​), 表示状态转移矩阵.\"]},\"47\":{\"h\":\"3. Why to slove state value\",\"t\":[\"为了进行 Policy evaluation, 即对于给定策略，求出其对应状态的 state value 的过程。\",\"通过 Bellman euqation 进行求解。\",\"The closed-form solution(不常用):\",\"vπ​=(I−γpπ​)−1rπ​​\",\"An iterative solution(一种迭代策略):\",\"vk+1​=rπ​+γPπ​vk​​\",\"可以最开始均初始化为 0 , 然后进行不断迭代，可以得到一个序列v0​,v1​,v2​,…. 最终可以证明：vk​→vπ​=(I−γpπ​)−1rπ​,k→∞\"]},\"48\":{\"h\":\"4. Action value\",\"t\":[\"State value: agent从一个状态出发可以得到的平均return. the average return the agent can get starting from a state\",\"Action value: agent从一个状态出发，采取一个指定的action可以得到的平均return。 the average return the agent can get starting from a state and taking an action.\",\"通过求解 action value 我们可以分析出在该状态下采取哪个 action 收益最大. Action value 定义:\",\"qπ​(s,a)=E[Gt​∣St​=s,At​=a]​\",\"同样地，qπ​(s,a)是依赖于策略π的，并且与状态 s 和动作 a 有关.\",\"vπ​(s)E[Gt​∣St​=s]​​=a∑​qπ​(s,a)E[Gt​∣St​=s,At​=a]​​π(a∣s)\",\"因此，vπ​(s)=∑a​qπ​(s,a)π(a∣s) 由于,\",\"vπ​(s)=a∑​π(a∣s)[r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)]​\",\"所以，qπ​(s,a)=∑r​p(r∣s,a)r+γ∑s′​p(s′∣s,a)vπ​(s′)\",\"实际意义是：在当前状态s下采取动作 a 所获得的均值，加上 γ 的转到下一个状态的 state value 加权均值。\",\"引入 action value 后，对于 state value 实际意义的解释：在当前状态s下，根据策略π, 所有可能动作的 action value 的加权均值。\",\"state value 和 action value 可以互相转化。\"]},\"49\":{\"h\":\"5. 总结\",\"t\":[\"State value: vπ​(s)=E[Gt​∣St​=s]\",\"Action value: qπ​(s,a)=E[Gt​∣St​=s,At​=a]\",\"State value 是 action value 的根据策略π加权平均，即vπ​(s)=∑a​π(a∣s)q(s,a)\",\"The Bellman equation (elementwise form and matrix-vector form)\",\"求解 the Bellman equation (2种方法)\"]},\"50\":{\"c\":[\"academic\"]},\"51\":{\"c\":[\"强化学习\"]},\"52\":{\"h\":\"RL3 - 贝尔曼最优公式\",\"t\":[\"Core concepts: optimal state value and optimal policy\",\"A fundamental tool: the Bellman optimality equation (BOE)\"]},\"53\":{\"h\":\"1. Optimal policy\",\"t\":[\"最优策略的定义: A policy π∗ is optimal if π∗(s)≥vπ​(s) for all s and for any other policy π. 需要确定几件事:\",\"最优策略是否存在 存在，根据 the contraction mapping Theorem.\",\"最优策略是否唯一 唯一，根据 the contraction mapping Theorem.\",\"最优策略是 stochastic 还是 deterministic deterministic 且 greedy\",\"如何得到最优策略 选取状态中最大的 action value 作为下一步的 action\"]},\"54\":{\"h\":\"2. Bellman optimality equation (BOE)\"},\"55\":{\"h\":\"2.1 基本形式\",\"t\":[\"对于贝尔曼最优公式而言，其策略π表示的是最优策略，除了需要求解 state value 外，还需要求解最优策略π.elementwise form:\",\"vπ​(s)​=πmax​a∑​π(a∣s)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vπ​(s′)),∀s∈S=πmax​a∑​π(a∣s)q(a,s),∀s∈S​\",\"matrix-vector foem:\",\"v=πmax​(rπ​+γPπ​v)​\"]},\"56\":{\"h\":\"2.2 如何求解\",\"t\":[\"对于贝尔曼最优公式而言，区别于贝尔曼公式，只是求解各状态的 state value, 我们还需要理解其所描述的最优策略π∗ 具体分两步:\"]},\"57\":{\"h\":\"2.2.1 如何处理等式右边的 (最优策略)\",\"t\":[\"vπ​(s)=maxπ​∑a​π(a∣s)q(s,a), 为了让右边取到最大值的情况，我们只需要在当前状态下，保证选取最大的 action value 即可，对应策略表示为:\",\"π(a∣s)={10​a=a∗a=a∗​\",\"其中a∗表示在该状态下计算出来的最大 action value 对应的动作，即a∗=argmaxa​q(s∣a)\"]},\"58\":{\"h\":\"2. 求解 state value\",\"t\":[\"将 BOE 转换为 v=f(v) 的形式，其中f(v):=maxπ​(rπ​+γPπ​v)f(v)对应一个向量, [f(v)]s​=maxπ​∑a​π(a∣s)q(s∣a),∀s∈S\",\"求解方法：\",\"Fix point: f(x)=x\",\"Contraction mapping(contractive function): ∣∣f(x1​)−f(x2​)∣∣≤γ∣∣x1​−x2​∣∣\",\"由此可以根据Contraction Mapping Theorem: For any equation that has the form of x=f(x), if f is a contraction mapping, then\",\"Existence: 存在不动点x∗，满足f(x∗)=x∗\",\"Uniqueness: 不动点x∗是唯一的\",\"Algorithm: Consider a sequence xk​ where xk+1​=f(xk​), then xk​→x∗ as k→∞. Moreover, the convergence rate is exponentially fast.\",\"因此，可以通过Contraction Mapping Theorem来求解贝尔曼最优公式，因为其满足该理论，即f(v)是一个contraction mapping。\"]},\"59\":{\"c\":[\"academic\"]},\"60\":{\"c\":[\"强化学习\"]},\"61\":{\"h\":\"RL4 - 值迭代和策略迭代(动态规划)\",\"t\":[\"贝尔曼最优公式:\",\"v=f(v)=πmax​(rπ​+γPπ​v)\"]},\"62\":{\"h\":\"1. Value iteration algorithm\",\"t\":[\"根据 chapter 3 中涉及的 contraction mapping theorem, 我们可以通过对应的迭代算法来求解贝尔曼最优公式\",\"vk+1​=f(vk​)=πmax​(rπ​+γPπ​vk​),k=1,2,3…\",\"这种迭代算法称为 value iteration.\"]},\"63\":{\"h\":\"1.1 具体步骤\",\"t\":[\"共分为 2 步：\",\"Policy update 这步是更新策略π，即求解右边的式子，πk+1​=argmaxπ​(rπ​+γPπ​vk​), 其中vk​是给定的。 其对应的 elementwise form:\",\"πk+1​(s)=πargmax​a∑​π(a∣s)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)v(s′)),s∈S 由于 p(s′∣s,a),p(r∣s,a),v(s′) 是已知的，显然，这里的最优策略πk+1​是一个 greedy policy，我们只需要挑选在当前迭代下最大的 action value 就好了, 即:\",\"πk+1​(a∣s)={10​a=ak∗​(s)a=ak∗​(s)​ 其中ak∗​(s)=argmaxa​qk​(a,s).\",\"value update 根据 Policy update 的策略πk+1​, 求解下一步的vk+1​, 即\",\"vk+1​=rπk+1​​+γPπk+1​​vk​这里的vk​并不是 state value 由于πk+1​是 greedy 的，对应的vk+1​(s)=maxa​qk​(a,s)\"]},\"64\":{\"h\":\"1.2 伪代码\",\"t\":[\"20240810190018\"]},\"65\":{\"h\":\"2. Policy iteration algorithm\",\"t\":[\"算法迭代示意图:\",\"π0​PE​vπ0​​PI​π1​PE​vπ1​​PI​π2​PE​vπ2​​PI​…\"]},\"66\":{\"h\":\"2.1 算法描述\",\"t\":[\"首先随机设计一个初始的策略π0​\",\"Step 1: policy evaluation (PE) 策略评估 该步骤是用来计算当前策略 πk​ 的 state value. 可以通过 Bellman equation 进行求解，即:\",\"vπk​​=rπk​​+γPπk​​vπk​​\",\"根据对应的 Elementwise form:\",\"vπk​(j+1)​(s)=a∑​πk​(a∣s)(r∑​p(r∣s,a)r+s′∑​p(s′∣s,a)vπk​(j)​(s′)),s∈S\",\"由此进行迭代，直到设置的收敛条件为止，即j→∞ 或者 ∣∣vπk+1​(j+1)​(s)−vπk​(j)​(s)∣∣≤δ.\",\"Step 2: policy improvement (PI) 策略提升 该步骤是根据 PE 所求出的 state value, 根据 action value，来提升当前策略 πk​\",\"πk+1​=πargmax​(rπ​+γPπ​vπk​​)\",\"对应的 Elementwise form:\",\"πk+1​(s)=πargmax​a∑​πk​(a∣s)qπk​​(s,a)(r∑​p(r∣s,a)r+s′∑​p(s′∣s,a)vπk​​(s′))​​,s∈S\",\"这里，显然是可以通过一个 greedy 的策略来进行选择，即:\",\"πk+1​(a∣s)={10​a=ak∗​(s),a=ak∗​(s).​\",\"其中 aK∗​(s)=argmaxa​qπk​​(s,a).\"]},\"67\":{\"h\":\"2.2 伪代码\",\"t\":[\"20240811002219\"]},\"68\":{\"h\":\"2.3 一些问题\",\"t\":[\"在 PE 步骤中，如何通过 Bellman equation 得到 state value vπk​​. 根据 chapter 2 中求解 Bellman equation 的方法 一种是可以直接通过矩阵求逆进行求解，即 vπk​​=(I−γPπk​​)−1rπk​​，实际不常用. 一种是通过迭代算法来求解\",\"vπk​(j+1)​=rπk​​+γPπk​​vπk​(j)​\",\"在 PI 步骤中，如何确保策略 πk+1​ 是优于 πk​的.\",\"为什么这个迭代算法最终可以找到最优策略 每次迭代都会使得策略进行提升，那么\",\"vπ0​​≤vπ1​​≤vπ2​​⋯≤vπk​​≤⋯≤v∗\",\"我们需要保证策略是不断提升，且最终会收敛到最优策略v∗\",\"policy iteration algorithm 与 value iteration algorithm 之间存在什么关系.\"]},\"69\":{\"h\":\"3. Truncated policy iteration algorithm\",\"t\":[\"该算法是 value iteration 以及 policy iteration 一般化的推广\"]},\"70\":{\"h\":\"3.1 value iteration 与 policy iteration 算法比较\",\"t\":[\"Policy iteration: 需要初始化策略π0​, 之后进行迭代\",\"Policy evaluation (PE): 通过 Bellman equation 求解当前策略的 state value.\",\"vπk​​=rπk​​+γPπk​​vπk​​\",\"内嵌迭代算法求解.\",\"Policy improvement (PI): 考虑 greedy 策略求解, 选取当前状态下最大的 action value.\",\"πk+1​=πargmax​(rπ​+γPπ​vπk​​)\",\"Value iteration: 需要初始化猜测的 state value v0​\",\"Policy update (PU): 考虑 greedy 策略求解, 选取当前状态下最大的 action value.\",\"πk+1​=πargmax​(rπ​+γPπ​vk​)\",\"Value update (VU): 进行迭代\",\"vk+1​=rπk+1​​+γPπk+1​​vk​\",\"两个算法迭代过程十分类似: Policy iteration:\",\"π0​PE​vπ0​​PI​π1​PE​vπ1​​PI​π2​PE​vπ2​​PI​…\",\"Value iteration:\",\"u0​PU​π1′​VU​u1​PU​π2′​VU​u2​PU​…\",\"Policy iteration algorithm\",\"Value iteration algorithm\",\"Comments\",\"1) Policy:\",\"π0​\",\"N/A\",\"2) Value:\",\"vπ0​​=rπ0​​+γPπ0​​vπ0​​\",\"v0​:=vπ0​​\",\"对于 policy iteration，vπ0​​是通过迭代算法来求的; 而 value iteration 我们这里强行初始化为vπ0​​，方便后续比较\",\"3) Policy:\",\"π1​=argmaxπ​(rπ​+γPπ​vπ0​​)\",\"π1​=argmaxπ​(rπ​+γPπ​vπ0​​)\",\"在策略更新上，这两个算法是一致的。\",\"4) Value:\",\"vπ1​​=rπ1​​+γPπ1​​vπ1​​\",\"v1​=rπ1​​+γPπ1​​v0​\",\"对于 Policy iteration 而言, 这里需要通过迭代算法来精确求出 vπ1​​; 对于 Value iteration，则只是进行一次带入求解。\",\"5) Policy:\",\"π2​=argmaxπ​(rπ​+γPπ​vπ1​​)\",\"π2′​=argmaxπ​(rπ​+γPπ​v1​)\",\"⋮\",\"⋮\",\"⋮\",\"⋮\"]},\"71\":{\"h\":\"3.2 Truncated policy iteration algorithm\",\"t\":[\"20240811010933\",\"显然，在求解 Bellman equation 中，Value iteration 只是进行了一步求解，而 Policy iteration 进行了无穷多步来进行了真实的求解 state value，显然在现实运行算法中是无法做到的。 因此 Truncated policy iteration algorithm 就是进行迭代 n 步来求解。\"]},\"72\":{\"h\":\"truncated policy iteration algorithm 是否是收敛的\",\"t\":[\"20240811011334\"]},\"73\":{\"c\":[\"academic\"]},\"74\":{\"c\":[\"强化学习\"]},\"75\":{\"h\":\"RL5 - 蒙特卡洛方法 (Monte Carlo) model-free\",\"t\":[\"如何在没有模型 (即p(r∣s,a),p(s′∣s,a)等均未知) 的情况下进行估计 通过 Monte Carlo estimation. 其核心思想是： 若有一系列(i.i.d)样本采样，得到一个样本序列x1​,x2​,…,xN​ 那么对于随机变量X的估计可以为：\",\"E[x]≈xˉ=N1​j=1∑N​xj​\",\"该方法成立的数学依据是 大数定理 (Law of Large Numbers)样本必须是独立同分布(iid, independent and identically distributed)\",\"为什么考虑 mean estimation. 因为无论是 state value 还是 action value 其原始定义都是从期望出发的。\",\"vπ​(s)=E[Gt​∣St​=s];qπ​(s,a)=E[Gt​∣St​=s,At​=a]\"]},\"76\":{\"h\":\"1. MC Basic\",\"t\":[\"最简单的示例算法，用于解释 MC 的原理，但现实场景中不太经常使用，效率过低。\",\"核心思想：如何将 Policy iteration algorithm 转换为 model-free 的情况。\"]},\"77\":{\"h\":\"1.1 算法思路\",\"t\":[\"Policy iteration 算法的核心是 先根据当前策略计算出各个状态的 state value， 再将 state value 转换为 action value，更新策略的步骤就是选择此时 action value 最大的 action.\",\"{Policyevaluation:vπk​​=rπk​​+γPπk​​vπk​​Policyimprovement:πk+1​=argmaxπ​(rπ​+γPπ​vπk​​)​\",\"显然其核心关键就是在 PE 中 通过迭代算法求解 Bellman equation 的 state value后：\",\"对于 model-based 的情况, 因为 p(r∣s,a),p(s′∣s,a) 已知，我们可以很轻松的求出各个情况下的q(s,a)，从而选择每个状态下最大的 action value 即可。\",\"qπk​​(s,a)=r∑​p(r∣s,a)+γs′∑​p(s′∣s,a)vπk​​(s)\",\"对于 model-free 的情况，此时 p(r∣s,a),p(s′∣s,a) 未知，我们不能通过之前的方法来求出q(s,a)，需要从 action value 的定义出发，即：\",\"qπk​​(s,a)=E[Gt​∣St​=s,At​=a]\",\"从此可以发现，我们可以通过前面所引入的 mean estimation 方法，来进行求解 q(s,a).\"]},\"78\":{\"h\":\"1.2 如何估计\",\"t\":[\"从指定的 (s,a) 出发，根据策略 πk​, 我们可以生成一个 episode.\",\"这个 episode 的 return 为 g(s,a).\",\"显然，g(s,a) 就是前面 Gt​ 的一个 sample.\",\"假设我们有了一系列 从状态 s 出发, 采取动作 a 的 episodes, 即 g(j)(s,a). 那么我们可以对 qπk​​(s,a) 进行估计，即\",\"qπk​​(s,a)=E[Gt​∣St​=s,At​=a]≈N1​i=1∑N​g(i)(s,a).\"]},\"79\":{\"h\":\"1.3 具体算法\",\"t\":[\"与 Policy iteration algorithm 步骤类似 首先初始化一个随机的策略π0​，然后进行迭代，对于 kth 迭代，有：\",\"Step 1: Policy evaluation. 求在策略πk​下所有的 action value, q(s,a). 具体求解方法，如 1.2 节所述，只不过我们此时需要遍历所有的 action-state pair. 为什么不去求 state value，因为最终策略更新的核心仍然是 action value, 即使先估计了 state value, 我们仍需要估计 action value.\",\"Step 2: Policy improvement. 这是来求解 πk+1​(s)=argmaxπ​∑a​π(a∣s)qπk​​(s,a),foralls∈S 这个仍然与之前一致，采用 greedy policy，即对于每个状态，我们选取其 action value 最大的 action.πk+1​(ak∗​∣s)=1，其中ak∗​=argmaxa​qπk​​(s,a)\",\"20240811233346\"]},\"80\":{\"h\":\"2. MC Exploring Starts\",\"t\":[\"MC Exploring Starts 是针对 MC Basic 的一些改进，即对于数据(experience)更加高效利用。\"]},\"81\":{\"h\":\"2.1 Episode 的高效利用\",\"t\":[\"Visit: every time a state-action pair appears in the episode, it is called a visit of that state-action pair.\",\"考虑一个 episode, 跟随策略π,\",\"s1​a2​​s2​a4​​s1​a2​​s2​a3​​s5​a1​​…\",\"对于 MC-Basic, 这一条 episode 仅用作估计 state-action pair (s1​,a2​) 的 action value q(s1​,a2​)，但存在一定的浪费, 对于一个 episode, 可以拆分为多个 episode, 从而进行多次利用.\",\"s1​a2​​s2​a4​​s1​a2​​s2​a3​​s5​a1​​…s2​a4​​s1​a2​​s2​a3​​s5​a1​​…s1​a2​​s2​a3​​s5​a1​​…s2​a3​​s5​a1​​…s5​a1​​…​[originalepisode][episodestartingfrom(s2​,a4​)][episodestartingfrom(s1​,a2​)][episodestartingfrom(s2​,a3​)][episodestartingfrom(s5​,a1​)]​\",\"这样，我们不仅可以用来估计q(s1​,a2​), 还可以估计q(s2​,a4​),q(s2​,a3​)…\",\"Data-efficient methods:\",\"first-visit method 记录在 episode 中第一次出现的 state-action pair, 如果该 state-action pair 再次出现, 不记录 action value 估计中.\",\"every-visit method 对于每个 state-action pair, 都记录 action value 估计中.\"]},\"82\":{\"h\":\"2.2 高效地更新 Policy\",\"t\":[\"什么时候更新策略也是一个影响效率的因素。\",\"方法1：如 MC Based 一样，在收集到了足够多的 从给定的 state-action pair 出发的 episodes 后, 通过 mean estimation 估计了q(s,a)后, 才进行更新。 缺点，等候时间过长，只有当所有 episodes 均收集完，才能进行 策略更新。\",\"方法2：直接 uses the return of a single episode to approximate the action value. 这类算法统称为：Generalized policy iteration (GPI). 它会在 Policy-evaluation 和 policy-improvement 中不断切换，即不需要完全精确地求出 action value，就直接去更新策略。\"]},\"83\":{\"h\":\"2.3 MC Exploring Starts\",\"t\":[\"20240812004534\"]},\"84\":{\"h\":\"2.4 Exploring Statrts的解释\",\"t\":[\"Exploring 表示对于每一个 action-state pair (s,a), 都需要有多个 episodes, 这样才能去估计相应的qπ​(s,a). 如果存在一个 action value 未能访问，就不能确保所选择的 action 是最优的。\",\"Starts 表示对于对应 action-state pair (s,a) 的 episodes，每次都是从对应的状态 s 出发，选择对应的动作 a 进行的采样。 如果从其他状态出发，得到的 episode，如果经过了 (s,a)，那么这称为 visit , 但目前无法保证 visit 一定可以遍历所给定的 (s,a).\",\"据目前而言，Exploring Starts 是一个必要条件.\"]},\"85\":{\"h\":\"3. MC Eplison-Greedy\",\"t\":[\"将 Exploring Starts 条件转换掉，通过采取 Soft Policies 的方法。\"]},\"86\":{\"h\":\"3.1 Soft Policy\",\"t\":[\"A policy is called soft if the probability to take any action is positive. 显然 soft policy 是 stochastic 的，并且如果按照这样一个策略，在 episode 足够长的情况下，我们可以确保其可以遍历所有的 state-action pair.\"]},\"87\":{\"h\":\"3.2 -greedy policy\",\"t\":[\"在这里，我们采用的是 ϵ-greedy policies, 其属于 soft policies.\",\"π(a∣s)={1−∣A(s)∣ϵ​(∣A(s)∣−1),∣A(s)∣ϵ​,​forthegreedyaction,fortheother∣A(s)∣−1action,​\",\"其中 ϵ∈[0,1] 且 ∣A(s)∣ 为状态 s 的动作数量.ϵ-greedy policy 可以平衡 exploitation 和 exploration. 显然ϵ=0, policy 就是 greedy 的; 如果ϵ=1, 此时就是随机策略，其探索性就很强.\"]},\"88\":{\"h\":\"3.3 -greedy policy 引入 MC-based 算法中\",\"t\":[\"对于 MC Basic 以及 MC Exploring 中的 policy improvement 中，找的是在所有可能策略中的最优策略，因此是一个确定的贪心策略。\",\"20240812011140\"]},\"89\":{\"h\":\"3.3 算法流程\",\"t\":[\"20240812010538\"]},\"90\":{\"c\":[\"academic\"]},\"91\":{\"c\":[\"强化学习\"]},\"92\":{\"h\":\"RL6 - 随机近似理论与随机梯度下降算法\",\"t\":[\"针对 mean estimation 问题进行研究，因为在 RL 中 无论是 state value 还是 action value 其定义都是一个均值 (means)\",\"Stochastic approximation(SA): SA refers to a broad class of stochastic iterative algorithms soloving root finding or optimization problems.\"]},\"93\":{\"h\":\"1. 引言\"},\"94\":{\"h\":\"1.1 求均值的方法\",\"t\":[\"第一种：直接通过 E[x]≈xˉ:=N1​∑i=1N​xi​，进行估计，只有当样本全部收集完才能估计.\",\"第二种: 增量式的迭代算法. 假设:\",\"wk+1​=k1​i=1∑k​xi​,k=1,2,…\",\"对应的\",\"wk​=k−11​i=1∑k−1​xi​,k=2,3,…\",\"那么，wk+1​可以由wk​推导出来，即\",\"wk+1​​=k1​∑i=1k​xi​​=k1​(∑i=1k−1​xi​+xk​)=k1​((k−1)wk​+xk​)​=wk​−k1​(wk​−xk​)​\",\"因此，wk+1​=wk​−k1​(wk​−xk​)\"]},\"95\":{\"h\":\"2. Robbins-Monto(RM) algorithm\"},\"96\":{\"h\":\"2.1 问题引入\",\"t\":[\"假设我们需要求解如下方程:\",\"g(w)=0\",\"其中, w∈R 且需要被求解出来，g:R→R 为一个函数方程. 显然，如果对于 g(w) 已知的情况，我们可以通过一些特定的算法进行求解。 如果 g(w) 未知，就需要新的算法进行解决。\"]},\"97\":{\"h\":\"2.2 算法介绍\",\"t\":[\"RM 算法就可以用来求解当 g(w) 未知时的情况，即函数 g(w) 是一个黑盒，我们只能通过 输入序列: wk​, 得到含有噪音的观测值序列: g​(wk​,ηk​) 具体解决如下:\",\"wk+1​=wk​−ak​g​(wk​,ηk​),k=1,2,3,…\",\"其中:\",\"wk​ 是第 k 次方程根的估计.\",\"g​(wk​,ηk​)=g(wk​)+ηk​ 是第 k 次的观测值(含噪音).\",\"ak​ 是一个 positive coefficient.\"]},\"98\":{\"h\":\"2.3 收敛性分析\",\"t\":[\"Robbins-Monro Theorem In the Robbins-Monro algorithm, if\",\"0<c1​≤▽w​g(w)≤c2​,forallw; 要求g(w)必须是递增的，确保根是存在且唯一的。\",\"∑k=1∞​ak​=∞ 且 ∑k=1∞​ak2​<∞;∑k=1∞​ak2​=∞ 保证 ak​→0,k→0∑k=1∞​ak​=∞ 保证 ak​→0不要过快.\",\"E[ηk​∣Hk​]=0 且 E[ηk2​∣Hk​]<∞; 其中Hk​=wk​,wk−1​,…, 那么 wk​ converges with probability 1 (w.p.1) to the root w∗ satisfying g(w∗)=0.\",\"ak​=k1​是满足上面三个条件的. 但实际上我们往往是选择一个非常小的常数。\"]},\"99\":{\"h\":\"2.4 应用于 mean estimation 中\",\"t\":[\"比如我们要估计某个随机变量X的 E[X] 我们可以设计如下方程:\",\"g(w)≐w−E[X].\",\"那么只要求解 g(w)=0, 我们就可以得到 E[X] 的值。 同样，我们不能直接得到随机变量的值，而是对应的样本 x，sample of X. 即，我们得到的观测值是:\",\"g​(w,x)≐w−x\",\"我们可以修改为噪音 η 的形式，\",\"g​(w,η)​=w−x​=w−x+E[X]−E[X]=(w−E[X])+(E[X]−x)​≐g(w)+η​\",\"因此我们可以通过 RM 算法来进行求解\",\"wk+1​=wk​−αk​g​(wk​,ηk​)=wk​−αk​(wk​−xk​)\"]},\"100\":{\"h\":\"3. Stochastic gradient descent\"},\"101\":{\"h\":\"3.1 问题引入\",\"t\":[\"需要求解一个优化问题:\",\"wargmin​J(w)=E[f(w,X)]\",\"其中，\",\"w 是需要被优化的参数\",\"X 是一个随机变量\",\"w 和 X 可以是标量，也可以是向量. 对于函数 f(⋅) 输出为标量.\",\"对于这个问题，我们有以下几种方法:\",\"Method 1: 梯度下降法 (gradient descent, GD)\",\"wk+1​=wk​−αk​▽w​E[f(wk​,X)]=wk​−αk​E[▽w​f(wk​,X)]\",\"但由于 j(w) 是一个期望值，我们很难直接获得.\",\"Method 2: batch gradient descent (BGD) 借用 MC 的思想，我们可以将:\",\"E[▽w​f(wk​,X)]≈n1​i=1∑n​▽w​f(wk​,xi​).\",\"因此\",\"wk+1​=wk​−αk​n1​i=1∑n​▽w​f(wk​,xi​)\",\"但需要大量的 samples 收集完毕才能进行一次迭代.\",\"Method 3: 随机梯度下降(SGD) 考虑能否仅用一次 sample 进行迭代.\",\"wk+1​=wk​−αk​▽w​f(wk​,xk​)\",\"但能否保证其精确度，以及是否可以到最后优化的成果。\"]},\"102\":{\"h\":\"3.2 SGD 分析\"},\"103\":{\"h\":\"mean estimation 问题转化\",\"t\":[\"我们可以将 均值估计 问题 转化为 一个 优化问题 进行求解：\",\"20240814014058\"]},\"104\":{\"h\":\"SGD 正确性和收敛性分析\",\"t\":[\"从 GD 到 SGD:\",\"wk+1​=wk​−αk​E[▽w​f(wk​,X)]⇓wk+1​=wk​−αk​▽w​f(wk​,x)​\",\"显然我们可以将 ▽w​f(wk​,x) 视为 E[▽w​f(wk​,x)] 的一个观测值(含噪声):\",\"▽w​f(wk​,x)=E[▽w​f(wk​,x)]+η▽w​f(wk​,x)−E[▽w​f(wk​,x)]​​\",\"因为\",\"▽w​f(wk​,x)=E[▽w​f(wk​,x)]\",\"因此，我们需要思考使用 SGD 时wk​→w∗ as k→∞ 是否成立。\",\"我们可以将 SGD 视为一个特殊情况下的 RM 算法 SGD的目标是 minimize\",\"J(w)=E[f(w,X)]\",\"而最小值问题，往往可以转化为导数为 0 的情况,\",\"▽w​J(w)=E[▽w​f(w,X)]=0\",\"显然，可以参考 RM 算法, 让\",\"g(w)=▽w​J(w)=E[▽w​f(w,X)]\",\"从而转换为一个 root-finding 问题. 相应的，对于观测值g​(w,η),\",\"g~​(w,η)​=∇w​f(w,x)=g(w)E[∇w​f(w,X)]​​+η∇w​f(w,x)−E[∇w​f(w,X)]​​.​\",\"因此，我们就可以通过 RM 算法进行求解g(w)=0,\",\"wk+1​=wk​−αk​g​(wk​,ηk​)=wk​−αk​▽w​f(wk​,xk​)\",\"对应收敛性证明\"]},\"105\":{\"h\":\"3.3 SGD 另一种问题描述方法 (deterministic formulation)\",\"t\":[\"在之前关于使用 SGD 算法的问题描述中，我们是引入了 随机变量 和 期望的情况. 我们可以将这个问题可以转化为一个随机变量的方法，从而引入 SGD 算法.\"]},\"106\":{\"h\":\"3.4 BGD MBGD SGDw\",\"t\":[\"20240814230747\"]},\"107\":{\"c\":[\"academic\"]},\"108\":{\"c\":[\"强化学习\"]},\"109\":{\"h\":\"RL7 - Temporal-Difference Learning\"},\"110\":{\"h\":\"1. 引入\",\"t\":[\"考虑一个复杂的均值估计问题: 计算\",\"ω=E[R+γv(X)],\",\"其中, R, X 均是随机变量，γ 是常数，v(⋅) 表示一个函数。 显然我们仍然可以通过 RM 算法进行求解，假设我们可以得到有关随机变量 R, X 的采样 {x},{r}\",\"g(w)g~​(w,η)​=w−E[R+γv(X)]=w−[r+γv(x)]=(w−E[R+γv(X)])+(E[R+γv(X)]−[r+γv(x)])≐g(w)+η​\",\"因此，我们可以将该问题定义为一个 root-finding 问题: g(w)=0. 相应的 RM 算法为:\",\"wk+1​=wk​−αk​g~​(wk​,ηk​)=wk​−αk​[wk​−[rk​+γv(xk​)]]\"]},\"111\":{\"h\":\"2. TD Learning of state value\",\"t\":[\"求解给定策略 π 的 state value，这样就可以与 policy improvement 结合去寻找最优策略。\"]},\"112\":{\"h\":\"2.1 算法描述\",\"t\":[\"算法所需的数据(experience): 根据给定的策略 π 所生成的数据 (s0​,r1​,s1​,…,st​,rt+1​,st+1​,…) or {(st​,rt+1​,st+1​)}\",\"相应的算法是:\",\"vt+1​(st​)vt+1​(s)​=vt​(st​)−αt​(st​)[vt​(st​)−[rt+1​+γvt​(st+1​)]]=vt​(s),∀s=st​,​\",\"其中 t=0,1,2,…, vt​(st​)是关于 vπ​(st​) 的估计。\",\"newestimatevt+1​(st​)​​=currentestimatevt​(st​)​​−αt​(st​)[vt​(st​)−TDtargetvt​ˉ​[rt+1​+γvt​(st+1​)]​​]​TDerrorδt​​\"]},\"113\":{\"h\":\"2.2 算法分析\",\"t\":[\"TD 算法是用来求解一个 给定策略 π 的 Bellman equation.\",\"根据 state value 的定义，对于策略 π 的 state value\",\"vπ​(s)=E[R+γG∣S=s],s∈S\",\"其中 G 是 discounted return。\",\"E[G∣S=s]=a∑​π(a∣s)s′∑​p(s′∣s,a)vπ​(s)=E[vπ​(S′)∣S=s]\",\"因此，我们可以写出 Bellman equation 的新形式，称为 Bellman expection equation\",\"vπ​(s)=E[E+γvπ​(S′)∣S=s],s∈S\"]},\"114\":{\"h\":\"2.3 TD 算法 与 MC 算法的比较\"},\"115\":{\"h\":\"3. TD Learning of action value\",\"t\":[\"Sarsa (state-action-reward-state-action) Sarsa 算法其目的是用于直接估计 action value, 从而可以在 policy improvement 中直接根据 action value 进行更新即可。\",\"Sarsa 算法同样是来求解 Bellman equation:\",\"qπ​(s,a)=E[R+γqπ​(S′,A′)∣s,a],∀s,a\"]},\"116\":{\"h\":\"3.1 Sarsa\",\"t\":[\"假设我们具有 some experience {(st​,at​,rt+1​,st+1​,at+1​)} 对应的 Sarsa 算法如下来进行估计 action value:\",\"qt+1​(st​,at​)qt+1​(s,a)​=qt​(sa​,at​)−αt​(st​,at​)[qt​(sa​,at​)−[rt+1​+γqt​(st+1​,at+1​)]]=qt​(s,a),∀(s,a)=(st​,at​)​\",\"其中 t=0,1,2,…, qt​(st​,at​) 是qπ​(st​,at​)的估计。\",\"收敛性情况\",\"20240817000114\",\"伪代码\",\"20240817000134\",\"20240817000230\"]},\"117\":{\"h\":\"3.2 n-step Sarsa\",\"t\":[\"20240817000500\",\"20240817000601\",\"20240817000642\"]},\"118\":{\"h\":\"3.3 Expected Sarsa\",\"t\":[\"20240817000331\",\"20240817000409\"]},\"119\":{\"h\":\"4. TD Learning of optimal action value\",\"t\":[\"Q-learning 算法是用来解决 action value 形式下的贝尔曼最优公式 (Bellman optimality equation in terms of action value)\",\"q(s,a)=E[Rt+1​+γamax​q(St+1​,a)∣St​=s,At​=a],∀s,a\"]},\"120\":{\"h\":\"4.1 Q-learning\",\"t\":[\"Q-learning 直接估计的是 optimal action value，因此不需要进行 policy improvement。\",\"qt+1​(st​,at​)qt+1​(s,a)​=qt​(st​,at​)−αt​(st​,at​)[qt​(st​,at​)−[rt+1​+γa∈Amax​qt​(st+1​,a)]]=qt​(s,a),∀(s,a)=(st​,at​)​\"]},\"121\":{\"h\":\"4.2. off-policy | on-policy\",\"t\":[\"behavior policy: 是用来与环境进行交互，从而生成经验数据的策略\",\"target policy: 是我们不断进行更新的策略，最终优化的策略\"]},\"122\":{\"h\":\"on - policy\",\"t\":[\"该算法中 behavior policy 和 target policy 是一致的，即我通过这个策略与环境进行交互生成一系列经验，在通过经验来更新这个策略。\"]},\"123\":{\"h\":\"off - policy\",\"t\":[\"该算法中 behavior policy 和 target policy 是不同的，即我通过一个策略与环境进行交互生成一系列经验。再通过这些经验来不断改进更新另一个策略，这另一个策略会更新到最优的策略。\",\"Sarsa，MC 是 on-policy 的 Q-learning 是 off-policy 的\"]},\"124\":{\"h\":\"4.3 Q-learning 伪代码\",\"t\":[\"因为 Q-learning 是 off-policy 的，因此，如果我们强制让 target policy 与 behavior ppolicy 一致也是可以的，此时也可以是 on-policy 的。\"]},\"125\":{\"h\":\"off-poicy 版本\",\"t\":[\"20240818182057\",\"此时 target policy 就不需要是 ϵ−greedy 策略了，因为不需要 target policy 进行生成数据。\"]},\"126\":{\"h\":\"on-policy 版本\",\"t\":[\"20240818181917\"]},\"127\":{\"h\":\"5. TD 算法的统一形式和总结\",\"t\":[\"20240818182301\",\"20240818182231\"]},\"128\":{\"c\":[\"academic\"]},\"129\":{\"c\":[\"强化学习\"]},\"130\":{\"h\":\"RL8 - 值函数近似(Value Function Approximation)\",\"t\":[\"对于 q-value 的估计从 基于表格的 (tabular representation) 转换到 基于函数的 (function representation)\"]},\"131\":{\"h\":\"1. 引入\",\"t\":[\"通过使用一个函数来进行拟合 state values 或者 action values: v^(s,w)≈vπ​(s)， 其中w∈Rm是参数向量。\",\"可以提高存储效率\",\"提高泛化能力\"]},\"132\":{\"h\":\"2. Alogorithm of state value estimation\",\"t\":[\"目标: 寻找一个最优的参数w，使得v^(s,w)最接近真实的vπ​(s).\",\"共两步:\",\"定义目标函数\",\"优化目标函数的算法\"]},\"133\":{\"h\":\"2.1 Obejctive function\",\"t\":[\"J(w)=E[(vπ​(S)−v^(S,w))2]\",\"分析随机变量 S 的 probability distribution (即对于损失函数中的 expection 需要考虑怎样对状态进行平均):\"]},\"134\":{\"h\":\"uniform distributon\",\"t\":[\"认为所有状态都是同等重要的，即各个状态的可能性为∣S∣1​ 因此这种情况下的 objective function 可以写成:\",\"J(w)=E[(vπ​(S)−v^(S,w))2]=∣S∣1​s∈S∑​(vπ​(s)−v^(s,w))2\",\"但实际情况可能并不是所有状态的概率都是一致的，基于给定策略下，一些状态可能很少被访问，另一些则频繁被访问，因此采用这种 objective function 就不太可行。\"]},\"135\":{\"h\":\"stationary distribution\",\"t\":[\"stationary: 表示是一种长时间的交互行为\",\"distributon: 表示是 状态 的分布\",\"通常也称为 steady-state distributon or limiting distributon.\",\"describes the long-run behavior of a Markov process. 即基于一个策略，我们不断地与环境进行交互，最终会达到一个平稳的状态，此时可以分析每一个状态在这个策略下的概率。\",\"设 {dπ​(s)}s∈S​ 表示 基于策略 π 下的 stationary distribution。其中 dπ​(s)≥0 且 ∑s∈S​dπ​(s)=1\",\"那么此时的 objective function 可以表示为：\",\"J(w)=E[(vπ​(S)−v^(S,w))2]=s∈S∑​dπ​(s)(vπ​(s)−v^(s,w))2\",\"20240820181406\",\"20240820181718\"]},\"136\":{\"h\":\"2.2 Optimization algorithms 优化算法\",\"t\":[\"目前的优化算法只是在估计给定策略的 statevalue\",\"minisize obejctive function J(w), 采用 梯度下降 算法:\",\"wk+1​=wk​−αk​▽w​J(wk​)\",\"对应目标函数的真实梯度是：\",\"▽w​j(w)​=▽w​E[(vπ​(S)−v^(S,w))2]=E[▽w​(vπ​(S)−v^(S,w))2]=−2E[(vπ​(S)−v^(S,w))▽w​v^(S,w)]​\",\"这里包含了一个 Expection，因此可以考虑 SGD 方法进行求解：\",\"wk+1​=wk​+αk​(vπ​(st​)−v^(st​,wt​))▽w​vt​^​(st​,wt​)\",\"其中st​是随机变量S的一个样本。 但这里还有一个难点，vπ​(st​) 我们是无法估计的，这是我们所求的量，因此需要用近似算法来进行替代，从而使得算法可行。\"]},\"137\":{\"h\":\"Monte Carlo learning with function approximation\",\"t\":[\"设 gt​ 表示在一个 episode 中，从状态 st​ 出发的 discounted return。因此我们用 gt​ 来近似 vπ​(st​), 即：\",\"wk+1​=wk​+αk​(gt​−v^(st​,wt​))▽w​vt​^​(st​,wt​)\"]},\"138\":{\"h\":\"TD Learning with function approximation\",\"t\":[\"在 TD 算法中，我们将 rt+1​+γv^(st+1​,wt​) 来近似 vπ​(st​), 因此对应算法为：\",\"wk+1​=wk​+αk​(rt+1​+γv^(st+1​,wt​)−v^(st​,wt​))▽w​vt​^​(st​,wt​)\"]},\"139\":{\"h\":\"3. Sarsa with function approximation\",\"t\":[\"wk+1​=wk​+αk​(rt+1​+γq^​(st+1​,at+1​,wt​)−q^​(st​,at​,wt​))▽w​qt​^​(st​,at​,wt​)\",\"20240820184127\"]},\"140\":{\"h\":\"4. Q-learning with function approximation\",\"t\":[\"wk+1​=wk​+αk​(rt+1​+γa∈A(st+1​)max​q^​(st+1​,at+1​,wt​)−q^​(st​,at​,wt​))▽w​qt​^​(st​,at​,wt​)\",\"on-policy版本：\",\"20240820184405\"]},\"141\":{\"h\":\"5. Deep Q-learning (DQN)\",\"t\":[\"Deep Q-learning 目的是最小化目标函数(objective/loss function):\",\"J(w)=E[(R+γa∈A(S′)max​q^​(S′,a,w)−q^​(S,A,w))2]\",\"其中 (S,A,R,S′) 均是随机变量。\"]},\"142\":{\"h\":\"优化方法\",\"t\":[\"采用梯度下降。\",\"J(w)=E[(R+γa∈A(S′)max​q^​(S′,a,w)−q^​(S,A,w))2]\",\"对于 q^​(S,A,w) 求解梯度还是很好求的。 但对于 maxa∈A(S′)​q^​(S′,a,w) 其求解梯度比较难求，在 DQN 中采用一个 固定 的方法进行解决。 尝试将 y≐R+γmaxa∈A(S′)​q^​(S′,a,w) 中的 w 进行固定求解，具体如下：\",\"引入两个网络：\",\"main network q^​(s,a,w)w 会一直进行更新，根据梯度下降的公式。\",\"target network q^​(s′,a,wT​) 并不是一直进行更新，而是等 main network 更新一定次数后，将该网络的 w 复制到 wT​ 中\",\"将 objective function 修改为：\",\"J(w)=E[(R+γa∈A(S′)max​q^​(S′,a,wT​)−q^​(S,A,w))2]\",\"在计算 main network q^​(s,a,w) 的梯度时，将 q^​(S′,a,wT​) 中的 wT​ 固定不动，因此左侧那个类似 TD target 的就不是有关 w 的函数，不用进行求导，从而方便计算。 然后在更新了一定次数之后，在将 wT​=w 进行赋值。\",\"因此对应的损失函数的梯度可以修改为：\",\"▽w​J​=E[−2(R+γa∈A(S′)max​q^​(S′,a,wT​)−q^​(S,A,w))▽w​q^​(S,A,w)]=E[−2(YT​−q^​(S,A,w))▽w​q^​(S,A,w)]​\",\"一些细节:\",\"w 和 wT​ 表示 the main and target networks 的参数，在初始化的时候是设为相同的。\",\"在每一次迭代时，我们需要从经验池 (the replay buffer) 中取出一定数量的样本 (a mini-batch of samples {(s,a,r,s')}) 进行训练。\",\"网络的输入包括 状态 s 和 动作 a. 在训练求解梯度时，我们先直接求解 target network 的输出，视为 yT​≐r+γmaxa∈A(s′)​q^​(s′,a,wT​)。 然后我们通过 mini-batch 样本 {(s,a,yT​)}, 通过梯度的算法来最小化对应的损失函数, 假设有 N 个样本，那么对应的损失函数求解为：\",\"J(w)=i=1∑N​yT​−q^​(s,a,w) 即可以通过梯度下降，来更新参数值\",\"wt+1​=wt​+αt​N1​i=1∑N​(yT​−q^​(si​,ai​,wt​))⋅▽w​q^​(si​,ai​,wt​)\"]},\"143\":{\"h\":\"经验回放 (replay buffer)\",\"t\":[\"20240820230827\",\"20240820230920\",\"20240820230944\"]},\"144\":{\"h\":\"伪代码\",\"t\":[\"20240820231024\",\"但在发表 DQN 的文章中，不太一样，在原文是 on-policy 且 main network 的输出是不一样的。\",\"20240820231205\"]},\"145\":{\"c\":[\"academic\"]},\"146\":{\"c\":[\"强化学习\"]},\"147\":{\"h\":\"RL9 - 策略梯度法(Policy gradient)\",\"t\":[\"之前介绍的方法都是 value-based 的方法，从这章开始时基于 policy-based 的方法。\",\"policy function approximation 是直接建立一个基于策略的目标函数来进行梯度上升的优化。\"]},\"148\":{\"h\":\"1. 基本思路\",\"t\":[\"将基于表格表示的策略 转换为 基于函数表示的策略。 即此时策略 π 可以描述为：\",\"π(a∣s,θ)\",\"其中，θ∈Rm表示参数向量，是我们需要进行优化的。\",\"当策略是以表格的形式保存时，我们定义最优的策略为 在该策略下的所有 state value 都是最大的。\",\"当策略是以函数的形式存在时，我们定义 最优的策略 为 可以最大化一个确定的常数指标(certain scalar metrics).\",\"Policy gradient 的基本步骤：\",\"确定 metrics/objective function，来定义最优的策略：J(θ)\",\"进行优化，如梯度上升算法\",\"θt+1​=θt​+α▽θ​J(θt​)\"]},\"149\":{\"h\":\"2. 目标函数定义\"},\"150\":{\"h\":\"2.1 average state value\",\"t\":[\"vˉπ​=s∈S∑​d(s)vπ​(s)=dTvπ​\",\"vˉπ​ 显然是 state value 的加权平均。\",\"d(s)≥0 是各个 state 的权重\",\"∑s∈S​d(s)=1, 我们可以认为 d(s) 是 概率分布，因此该指标可以描述为:\",\"vˉπ​=ES∼d​[vπ​(S)]\"]},\"151\":{\"h\":\"另一种表达\",\"t\":[\"J(θ)=E[t=0∑∞​γtRt+1​]\",\"20240826173749\"]},\"152\":{\"h\":\"d(s)的选择\",\"t\":[\"d 与策略 π 无关 这种情况我们将 d 表示为 d0​, vˉπ​ 表示为 vˉπ0​. 这种情况下的 d 可以根据对各个状态的重要程度进行选择： 一种是将所有状态视为同等重要，一种则是有所偏向。\",\"d 与策略 π 有关 d 表示为 dπ​(s), 即在策略 π 下的 stationary distribution。\"]},\"153\":{\"h\":\"2.2 average return value\",\"t\":[\"rˉπ​=s∈S∑​dπ​(s)rπ​(s)=ES∼d​[rπ​(s)]\",\"其中:\",\"rπ​(s)r(s,a)​≐a∈A∑​π(a∣s)r(s,a)=E[R∣s,a]=r∑​rp(r∣s,a)​\",\"rπ​(s)表示在策略π下 状态s时可以得到的平均reward。r(s,a)表示在单步情况下(在状态s采用动作a)时的平均reward。\"]},\"154\":{\"h\":\"另一种表达\",\"t\":[\"假设 agent 跟随一个 给定的策略 然后生成了一个 trajectory以及对应的 rewards (Rt+1​,Rt+2​,…)\",\"对应 average single-step reward along this trajectory is\",\"====​n→∞lim​n1​E[Rt+1​+Rt+2​+⋯+Rt+n​∣St​=s0​]n→∞lim​n1​E[k=1∑n​Rt+k​∣St​=s0​]n→∞lim​n1​E[k=1∑n​Rt+k​]s∑​dπ​(s)rπ​(s)rˉπ​​\"]},\"155\":{\"h\":\"3. 目标函数梯度求解\",\"t\":[\"这里在视频没有详细介绍，只给出了梯度的公式：\",\"▽θ​J(θ)​=s∈S∑​η(s)a∈A∑​▽θ​π(a∣s,θ)qπ​(s,a)=E[▽θ​lnπ(A∣S,θ)qπ​(S,A)]​\",\"其中\",\"J(θ) 可以是 vˉπ​,rˉπ​,vˉπ0​ 任何一种。\",\"\\\"=\\\" 有表示 严格等于 近似 以及 成比例等于\",\"η 表示 state 的权重或者分布\",\"具体推导过程:\",\"▽θ​lnπ(a∣s,θ)▽θ​J(θ)​=π(a∣s,θ)▽θ​π(a∣s,θ)​=s∈S∑​d(s)a∈A∑​▽θ​π(a∣s,θ)qπ​(s,a)=s∈S∑​d(s)a∈A∑​π(a∣s,θ)▽θ​lnπ(a∣s,θ)qπ​(s,a)=ES∼d​[a∑​π(s∣S,θ)▽θ​lnπ(a∣S,θ)qπ​(S,a)]=ES∼d,A∼π​[▽θ​lnπ(A∣S,θ)qπ​(S,A)]≐E[▽θ​lnπ(A∣S,θ)qπ​(S,A)]​\",\"根据这个式子我们就可以通过 SGD 方法，从而可以进行近似求解：\",\"▽θ​J(θ)≈▽θ​lnπ(a∣s,θ)qπ​(s,a)\",\"一些特性 这里的策略是随机性 的，因为我们需要计算的是 lnπ(a∣s,θ), 因此我们需要保证对于所有的 s,a,θ\",\"π(a∣s,θ)≥0\",\"20240826180244\"]},\"156\":{\"h\":\"4. REINFORCE 梯度上升算法\",\"t\":[\"梯度上升算法的本质就是最大化目标函数 J(θ)\",\"θt+1​​=θt​+α▽θ​J(θ)=θt​+αE[▽θ​lnπ(A∣S,θt​)qπ​(S,A)]​\",\"而对应的真实梯度可以用一个估计的梯度来替代:\",\"θt+1​=θt​+α▽θ​lnπ(at​∣st​,θt​)qπ​(s,a)\",\"但还存在 qπ​(s,a) 是未知的，我们也可以进行近似：\",\"θt+1​=θt​+α▽θ​lnπ(a∣s,θt​)qt​(st​,at​)\",\"这里可以用不同的方法来近似 qπ​(s,a).\",\"Monte-Carlo based method， 我们便称为 REINFORCE\",\"也可以采用基于 TD 的算法 或者 其他的算法。\",\"一些细节\",\"20240826181340\",\"20240826181538\",\"20240826181638\"]},\"157\":{\"h\":\"REINFORCE 算法\",\"t\":[\"20240826181712\"]},\"158\":{\"c\":[\"academic\"]},\"159\":{\"c\":[\"强化学习\"]},\"160\":{\"h\":\"Java - 类与对象1\"},\"161\":{\"h\":\"类与对象\",\"t\":[\"类: 是对一类事物的描述，是抽象的、概念上的定义.对象: 是某一类事物实际存在的每个个体，因而也被称为实例（instance）， 是类的一个具体化个体.\",\"类的创建: 类名的首字母通常是大写的.\",\"public class Person {//这里定义的人类具有三个属性，名字、年龄、性别 String name; //直接在类中定义变量，表示类具有的属性 int age; String sex; } \",\"对象实例的创建 new Person() :\",\"public static void main(String[] args) { Person p = new Person(); } \",\"对于对象而言，其变量名存储的是对象的引用（类似于c++指针的情况），并非是所对应的对象本身，即\",\"public static void main(String[] args) { //这里的a存放的是具体的某个值 int a = 10; //创建一个变量指代我们刚刚创建好的对象，变量的类型就是对应的类名 //这里的p1存放的是对象的引用，而不是本体，我们可以通过对象的引用来间接操作对象 Person p1 = new Person(); Person p2 = p1; // 我们将变量p2赋值为p1的值，那么实际上只是传递了对象的引用，而不是对象本身的复制 } \",\"在创建了对象之后，就可以进行一定操作，如: 访问、修改对象的属性. 不同对象的属性是分开独立存放的，每个对象都有一个自己的空间，修改一个对象的属性并不会影响到其他对象. 关于对象类型的变量，我们也可以不对任何对象进行引用：\",\"public static void main(String[] args) { Person p = null; //此时变量没有引用任何对象 p.name = \\\"小红\\\"; //我任性，就是要操作 System.out.println(p.name); } \",\"会出现异常，即空指针异常. 对象创建成功之后，它的属性没有进行赋值，但是我们前面说了，变量使用之前需要先赋值，那么创建对象之后能否直接访问呢？ 果直接创建对象，那么对象的属性都会存在初始值，如果是基本类型，那么默认是统一为0（如果是boolean的话，默认值为false）如果是引用类型，那么默认是null。\"]},\"162\":{\"h\":\"方法的创建与使用\",\"t\":[\"类除了具有属性外，还可以定义一些方法来描述同一类的行为。 方法是语句的集合，是为了完成某件事情而存在的。 方法名称同样可以随便起，但是规则跟变量的命名差不多，也是尽量使用小写字母开头的单词，如果是多个单词，一般使用驼峰命名法最规范。\",\"方法的定义如下:\",\"返回值类型 方法名称() { 方法体... } \",\"具体而言:\",\"public class Person { String name; int age; String sex; //自我介绍只需要完成就行，没有返回值，所以说使用void void hello(){ //完成自我介绍需要执行的所有代码就在这个花括号中编写 //这里编写代码跟我们之前在main中是一样的（实际上main就是一个函数） //自我介绍需要用到当前对象的名字和年龄，我们直接使用成员变量即可，变量的值就是当前对象的存放值 System.out.println(\\\"我叫 \\\"+name+\\\" 今年 \\\"+age+\\\" 岁了！\\\"); } } \",\"方法的调用:\",\"public static void main(String[] args) { Person p = new Person(); p.name = \\\"小明\\\"; p.age = 18; p.hello(); //我们只需要使用 . 运算符，就可以执行定义好的方法了，只需要 .方法名称() 即可 } \"]},\"163\":{\"h\":\"方法的进阶使用\"},\"164\":{\"h\":\"this 的使用\",\"t\":[\"有时候我们的方法中可能会出现一些与成员变量重名的变量：\",\"void setName(String name) { name = name; //出现重名时，优先使用作用域最接近的 //这里实际上是将方法参数的局部变量name赋值为本身 } \",\"我们如果想要在方法中访问到当前对象的属性，那么可以使用this关键字，来明确表示当前类的示例对象本身：\",\"void setName(String name) { this.name = name; //让当前对象的name变量值等于参数传入的值 } \",\"当然，如果方法内没有变量出现重名的情况，那么默认情况下可以不使用this关键字来明确表示当前对象：\",\"String getName() { return name; //这里没有使用this，但是当前作用域下只有对象属性的name变量，所以说直接就使用了 } \"]},\"165\":{\"h\":\"方法的重载\",\"t\":[\"有些时候，参数类型可能会多种多样，我们的方法需要能够同时应对多种情况。\",\"一个类中可以包含多个同名的方法，但是需要的形式参数不一样，方法的返回类型，可以相同，也可以不同，但是仅返回类型不同，是不允许的！\",\"int sum(int a, int b){ return a + b; } double sum(double a, double b){ //为了支持小数加法，我们可以进行一次重载 return a + b; } \"]},\"166\":{\"h\":\"构造方法\",\"t\":[\"我们前面创建对象，都是直接使用new关键字就能直接搞定了，但是我们发现，对象在创建之后，各种属性都是默认值，那么能否实现在对象创建时就为其指定名字、年龄、性别呢？ 要在对象创建时进行处理，我们可以使用**构造方法（构造器）**来完成。\",\"构造方法不需要填写返回值，并且方法名称与类名相同，默认情况下每个类都会自带一个没有任何参数的无参构造方法（只是不用我们去写，编译出来就自带）当然，我们也可以手动声明，对其进行修改：\",\"public class Person { String name; int age; String sex; Person(){ //构造方法不需要指定返回值，并且方法名称与类名相同 name = \\\"小明\\\"; //构造方法会在对象创建时执行，我们可以将各种需要初始化的操作都在这里进行处理 age = 18; sex = \\\"男\\\"; } } \",\"构造方法会在new的时候自动执行, 当然，我们也可以为构造方法设定参数：\",\"public class Person { String name; int age; String sex; Person(String name, int age, String sex){ //跟普通方法是一样的 this.name = name; this.age = age; this.sex = sex; } } \",\"注意，在我们自己定义一个构造方法之后，会覆盖掉默认的那一个无参构造方法，除非我们手动重载一个无参构造，否则要创建这个类的对象，必须调用我们自己定义的构造方法.\",\"当然，要给成员变量设定初始值，我们不仅可以通过构造方法，也可以直接在定义时赋值：\",\"public class Person { String name = \\\"未知\\\"; //直接赋值，那么对象构造好之后，属性默认就是这个值 int age = 10; String sex = \\\"男\\\"; } \",\"这里需要特别注意，成员变量的初始化，并不是在构造方法之后，而是在这之前就已经完成了.\",\"Person(String name, int age, String sex){ System.out.println(this.age); // 在赋值之前看看是否有初始值 // 这里是 this.age 而非 age // 此时this.age已经初始化完，但还未复制，this.age = 0 this.name = name; this.age = age; this.sex = sex; } \",\"我们也可以在类中添加代码块，代码块同样会在对象构造之前进行，在成员变量初始化之后执行：\",\"public class Person { String name; int age; String sex; { System.out.println(\\\"我是代码块\\\"); //代码块中的内容会在对象创建时仅执行一次 } Person(String name, int age, String sex){ System.out.println(\\\"我被构造了\\\"); this.name = name; this.age = age; this.sex = sex; } } \"]},\"167\":{\"c\":[\"code\"]},\"168\":{\"c\":[\"java\"]},\"169\":{\"h\":\"Java - 类与对象2\"},\"170\":{\"h\":\"静态变量和静态方法\",\"t\":[\"Static 静态的内容，我们可以理解为是属于这个类的，也可以理解为是所有对象共享的内容。 我们通过使用 static 关键字来声明一个变量或一个方法为静态的，一旦被声明为静态，那么通过这个类创建的所有对象，操作的都是同一个目标，也就是说，对象再多，也只有这一个静态的变量或方法。 一个对象改变了静态变量的值，那么其他的对象读取的就是被改变的值。\",\"一般情况下，我们并不会通过一个具体的对象去修改和使用静态属性，而是通过这个类去使用：\",\"public class Person { String name; int age; String sex; static String info; //这里我们定义一个info静态变量 } \",\"public static void main(String[] args) { Person.info = \\\"让我看看\\\"; System.out.println(Person.info); } \",\"同样的，我们可以将方法标记为静态：\",\"static void test(){ System.out.println(\\\"我是静态方法\\\"); } \",\"静态方法同样是属于类的，而不是具体的某个对象，所以说，就像下面这样:\",\"因为静态方法属于类的，所以说我们在静态方法中，无法获取成员变量的值, 同样的，在静态方法中，无法使用this关键字，因为this关键字代表的是当前的对象本身。 但是静态方法是可以访问到静态变量的.\"]},\"171\":{\"h\":\"静态变量初始化\",\"t\":[\"我们实际上是将 .class 文件丢给 JVM 去执行的，而每一个 .class 文件其实就是我们编写的一个类，我们在 Java 中使用一个类之前， JVM 并不会在一开始就去加载它，而是在需要时才会去加载（优化）一般遇到以下情况时才会会加载类：\",\"访问类的静态变量，或者为静态变量赋值\",\"new 创建类的实例（隐式加载）\",\"调用类的静态方法\",\"子类初始化时\",\"其他的情况会在讲到反射时介绍\",\"所有被标记为静态的内容，会在类刚加载的时候就分配，而不是在对象创建的时候分配，所以说静态内容一定会在第一个对象初始化之前完成加载。\"]},\"172\":{\"h\":\"包的访问与控制\"},\"173\":{\"h\":\"包的声明和导入\",\"t\":[\"包其实就是用来区分类位置的东西，也可以用来将我们的类进行分类（类似于C++中的namespace）随着我们的程序不断变大，可能会创建各种各样的类，他们可能会做不同的事情，那么这些类如果都放在一起的话，有点混乱，我们可以通过包的形式将这些类进行分类存放。\",\"包的命名规则同样是英文和数字的组合，最好是一个域名的格式，比如我们经常访问的 www.baidu.com ，后面的 baidu.com 就是域名，我们的包就可以命名为com.baidu，其中的.就是用于分割的，对应多个文件夹，比如com.test\",\"20240815234719\",\"我们之前都是直接创建的类，所以说没有包这个概念，但是现在，我们将类放到包中，就需要注意了： 需要通过关键字 package，用于指定当前类所处的包的，注意，所处的包和对应的目录是一一对应的。\",\"package com.test; //在放入包中，需要在类的最上面添加package关键字来指明当前类所处的包 public class Main { //将Main类放到com.test这个包中 public static void main(String[] args) { } } \",\"当我们使用同一个包中的类时，直接使用即可（之前就是直接使用的，因为都直接在一个缺省的包中） 而当我们需要使用其他包中的类时，需要先进行导入才可以： 需要通过关键字 import 导入我们需要使用的类，当然，只有在类不在同一个包下时才需要进行导入，如果一个包中有多个类，我们可以使用*表示导入这个包中全部的类:\",\"import com.test.entity.Person; //使用import关键字导入其他包中的类 import com.test.entity.*; \",\"Java会默认导入java.lang这个包下的所有类，因此我们不需要手动指定。\",\"不同类的重名问题 在不同包下的类，即使类名相同，也是不同的两个类：\",\"package com.test.entity; public class String { //我们在自己的包中也建一个名为String的类 } \",\"由于默认导入了系统自带的String类，并且也导入了我们自己定义的String类，那么此时就出现了歧义，编译器不知道到底我们想用的是哪一个String类，所以说我们需要明确指定：\",\"public class Main { public static void main(java.lang.String[] args) { //主方法的String参数是java.lang包下的，我们需要明确指定一下，只需要在类名前面添加包名就行了 com.test.entity.String string = new com.test.entity.String(); } } \",\"我们只需要在类名前面把完整的包名也给写上，就可以表示这个是哪一个包里的类了，当然，如果没有出现歧义，默认情况下包名是可以省略的，可写可不写。\"]},\"174\":{\"h\":\"访问权限控制\",\"t\":[\"Java中引入了访问权限控制（可见性），我们可以为成员变量、成员方法、静态变量、静态方法甚至是类指定访问权限，不同的访问权限，有着不同程度的访问限制：\",\"private - 私有，标记为私有的内容无法被除当前类以外的任何位置访问。\",\"什么都不写 - 默认，默认情况下，只能被类本身和同包中的其他类访问。\",\"protected - 受保护，标记为受保护的内容可以能被类本身和同包中的其他类访问，也可以被子类访问（子类我们会在下一章介绍）\",\"public - 公共，标记为公共的内容，允许在任何地方被访问。\",\"当前类\",\"同一个包下的类\",\"不同包下的子类\",\"不同包下的类\",\"public\",\"✅\",\"✅\",\"✅\",\"✅\",\"protected\",\"✅\",\"✅\",\"✅\",\"❌\",\"默认\",\"✅\",\"✅\",\"❌\",\"❌\",\"private\",\"✅\",\"❌\",\"❌\",\"❌\",\"默认的情况下，在当前包以外的其他包中无法访问。\",\"如果某个类中存在静态方法或是静态变量，那么我们可以通过静态导入的方式将其中的静态方法或是静态变量直接导入使用，但是同样需要有访问权限的情况下才可以：\",\"public class Person { String name; int age; String sex; public static void test(){ System.out.println(\\\"我是静态方法！\\\"); } } \",\"静态导入：\",\"import static com.test.entity.Person.test; //静态导入test方法 public class Main { public static void main(String[] args) { test(); //直接使用就可以，就像在这个类定义的方法一样 } } \"]},\"175\":{\"c\":[\"code\"]},\"176\":{\"c\":[\"java\"]},\"177\":{\"h\":\"Java - 类与对象3\"},\"178\":{\"h\":\"封装 继承和多态\"},\"179\":{\"h\":\"封装\"},\"180\":{\"h\":\"继承\"},\"181\":{\"h\":\"多态\"},\"182\":{\"c\":[\"code\"]},\"183\":{\"c\":[\"java\"]},\"184\":{\"h\":\"\",\"t\":[\"404 Not Found\"]},\"185\":{\"h\":\"Daily\"},\"186\":{\"h\":\"UAV\"},\"187\":{\"h\":\"Academic\"},\"188\":{\"h\":\"强化学习\"},\"189\":{\"h\":\"Java\"},\"190\":{\"h\":\"Code\"}},\"dirtCount\":0,\"index\":[[\"多态\",{\"0\":{\"181\":1}}],[\"继承\",{\"0\":{\"180\":1}}],[\"继承和多态\",{\"0\":{\"178\":1}}],[\"封装\",{\"0\":{\"178\":1,\"179\":1}}],[\"❌\",{\"1\":{\"174\":6}}],[\"✅\",{\"1\":{\"174\":10}}],[\"允许在任何地方被访问\",{\"1\":{\"174\":1}}],[\"公共\",{\"1\":{\"174\":1}}],[\"子类我们会在下一章介绍\",{\"1\":{\"174\":1}}],[\"子类初始化时\",{\"1\":{\"171\":1}}],[\"标记为公共的内容\",{\"1\":{\"174\":1}}],[\"标记为受保护的内容可以能被类本身和同包中的其他类访问\",{\"1\":{\"174\":1}}],[\"标记为私有的内容无法被除当前类以外的任何位置访问\",{\"1\":{\"174\":1}}],[\"受保护\",{\"1\":{\"174\":1}}],[\"什么都不写\",{\"1\":{\"174\":1}}],[\"什么时候更新策略也是一个影响效率的因素\",{\"1\":{\"82\":1}}],[\"私有\",{\"1\":{\"174\":1}}],[\"主方法的string参数是java\",{\"1\":{\"173\":1}}],[\"主要框架\",{\"1\":{\"18\":1}}],[\"主要内容\",{\"0\":{\"6\":1}}],[\"主要贡献\",{\"0\":{\"5\":1}}],[\"主要动机\",{\"0\":{\"4\":1}}],[\"编译器不知道到底我们想用的是哪一个string类\",{\"1\":{\"173\":1}}],[\"编译出来就自带\",{\"1\":{\"166\":1}}],[\"使用import关键字导入其他包中的类\",{\"1\":{\"173\":1}}],[\"使得v^\",{\"1\":{\"132\":1}}],[\"导入我们需要使用的类\",{\"1\":{\"173\":1}}],[\"比如com\",{\"1\":{\"173\":1}}],[\"比如我们经常访问的\",{\"1\":{\"173\":1}}],[\"比如我们要估计某个随机变量x的\",{\"1\":{\"99\":1}}],[\"他们可能会做不同的事情\",{\"1\":{\"173\":1}}],[\"包其实就是用来区分类位置的东西\",{\"1\":{\"173\":1}}],[\"包的命名规则同样是英文和数字的组合\",{\"1\":{\"173\":1}}],[\"包的声明和导入\",{\"0\":{\"173\":1}}],[\"包的访问与控制\",{\"0\":{\"172\":1}}],[\"调用类的静态方法\",{\"1\":{\"171\":1}}],[\"隐式加载\",{\"1\":{\"171\":1}}],[\"创建类的实例\",{\"1\":{\"171\":1}}],[\"创建一个变量指代我们刚刚创建好的对象\",{\"1\":{\"161\":1}}],[\"文件其实就是我们编写的一个类\",{\"1\":{\"171\":1}}],[\"文件丢给\",{\"1\":{\"171\":1}}],[\"去执行的\",{\"1\":{\"171\":1}}],[\"操作的都是同一个目标\",{\"1\":{\"170\":1}}],[\"静态导入test方法\",{\"1\":{\"174\":1}}],[\"静态导入\",{\"1\":{\"174\":1}}],[\"静态方法甚至是类指定访问权限\",{\"1\":{\"174\":1}}],[\"静态方法同样是属于类的\",{\"1\":{\"170\":1}}],[\"静态变量\",{\"1\":{\"174\":1}}],[\"静态变量初始化\",{\"0\":{\"171\":1}}],[\"静态变量和静态方法\",{\"0\":{\"170\":1}}],[\"静态的内容\",{\"1\":{\"170\":1}}],[\"静止\",{\"1\":{\"13\":1}}],[\"代码块中的内容会在对象创建时仅执行一次\",{\"1\":{\"166\":1}}],[\"代码块同样会在对象构造之前进行\",{\"1\":{\"166\":1}}],[\"成员方法\",{\"1\":{\"174\":1}}],[\"成员变量的初始化\",{\"1\":{\"166\":1}}],[\"成比例等于\",{\"1\":{\"155\":1}}],[\"属性默认就是这个值\",{\"1\":{\"166\":1}}],[\"必须调用我们自己定义的构造方法\",{\"1\":{\"166\":1}}],[\"必须是递增的\",{\"1\":{\"98\":1}}],[\"除非我们手动重载一个无参构造\",{\"1\":{\"166\":1}}],[\"除了需要求解\",{\"1\":{\"55\":1}}],[\"注意\",{\"1\":{\"166\":1,\"173\":1}}],[\"注重近期的reward\",{\"1\":{\"19\":1}}],[\"跟普通方法是一样的\",{\"1\":{\"166\":1}}],[\"跟随一个\",{\"1\":{\"154\":1}}],[\"跟随策略π\",{\"1\":{\"81\":1}}],[\"男\",{\"1\":{\"166\":2}}],[\"默认的情况下\",{\"1\":{\"174\":1}}],[\"默认\",{\"1\":{\"174\":2}}],[\"默认情况下\",{\"1\":{\"174\":1}}],[\"默认情况下包名是可以省略的\",{\"1\":{\"173\":1}}],[\"默认情况下每个类都会自带一个没有任何参数的无参构造方法\",{\"1\":{\"166\":1}}],[\"默认值为false\",{\"1\":{\"161\":1}}],[\"构造器\",{\"1\":{\"166\":1}}],[\"构造方法会在new的时候自动执行\",{\"1\":{\"166\":1}}],[\"构造方法会在对象创建时执行\",{\"1\":{\"166\":1}}],[\"构造方法不需要指定返回值\",{\"1\":{\"166\":1}}],[\"构造方法不需要填写返回值\",{\"1\":{\"166\":1}}],[\"构造方法\",{\"0\":{\"166\":1},\"1\":{\"166\":1}}],[\"各种属性都是默认值\",{\"1\":{\"166\":1}}],[\"参数类型可能会多种多样\",{\"1\":{\"165\":1}}],[\"优先使用作用域最接近的\",{\"1\":{\"164\":1}}],[\"优化\",{\"1\":{\"171\":1}}],[\"优化方法\",{\"0\":{\"142\":1}}],[\"优化算法\",{\"0\":{\"136\":1}}],[\"优化目标函数的算法\",{\"1\":{\"132\":1}}],[\"优化问题\",{\"1\":{\"103\":1}}],[\"优化问题建立\",{\"0\":{\"11\":1}}],[\"出现重名时\",{\"1\":{\"164\":1}}],[\"出发的\",{\"1\":{\"82\":1,\"137\":1}}],[\"出发\",{\"1\":{\"78\":2,\"84\":1}}],[\"运算符\",{\"1\":{\"162\":1}}],[\"运用到\",{\"1\":{\"29\":1}}],[\"小明\",{\"1\":{\"162\":1,\"166\":1}}],[\"小红\",{\"1\":{\"161\":1}}],[\"岁了\",{\"1\":{\"162\":1}}],[\"今年\",{\"1\":{\"162\":1}}],[\"自我介绍需要用到当前对象的名字和年龄\",{\"1\":{\"162\":1}}],[\"自我介绍只需要完成就行\",{\"1\":{\"162\":1}}],[\"完成自我介绍需要执行的所有代码就在这个花括号中编写\",{\"1\":{\"162\":1}}],[\"没有返回值\",{\"1\":{\"162\":1}}],[\"返回值类型\",{\"1\":{\"162\":1}}],[\"果直接创建对象\",{\"1\":{\"161\":1}}],[\"变量的值就是当前对象的存放值\",{\"1\":{\"162\":1}}],[\"变量的类型就是对应的类名\",{\"1\":{\"161\":1}}],[\"变量使用之前需要先赋值\",{\"1\":{\"161\":1}}],[\"它的属性没有进行赋值\",{\"1\":{\"161\":1}}],[\"它会在\",{\"1\":{\"82\":1}}],[\"我是静态方法\",{\"1\":{\"170\":1,\"174\":1}}],[\"我是代码块\",{\"1\":{\"166\":1}}],[\"我被构造了\",{\"1\":{\"166\":1}}],[\"我叫\",{\"1\":{\"162\":1}}],[\"我任性\",{\"1\":{\"161\":1}}],[\"我们之前都是直接创建的类\",{\"1\":{\"173\":1}}],[\"我们的包就可以命名为com\",{\"1\":{\"173\":1}}],[\"我们的方法需要能够同时应对多种情况\",{\"1\":{\"165\":1}}],[\"我们在自己的包中也建一个名为string的类\",{\"1\":{\"173\":1}}],[\"我们在\",{\"1\":{\"171\":1}}],[\"我们实际上是将\",{\"1\":{\"171\":1}}],[\"我们并不会通过一个具体的对象去修改和使用静态属性\",{\"1\":{\"170\":1}}],[\"我们通过使用\",{\"1\":{\"170\":1}}],[\"我们通常将\",{\"1\":{\"27\":1}}],[\"我们前面创建对象\",{\"1\":{\"166\":1}}],[\"我们如果想要在方法中访问到当前对象的属性\",{\"1\":{\"164\":1}}],[\"我们直接使用成员变量即可\",{\"1\":{\"162\":1}}],[\"我们也可以在类中添加代码块\",{\"1\":{\"166\":1}}],[\"我们也可以为构造方法设定参数\",{\"1\":{\"166\":1}}],[\"我们也可以手动声明\",{\"1\":{\"166\":1}}],[\"我们也可以不对任何对象进行引用\",{\"1\":{\"161\":1}}],[\"我们也可以进行近似\",{\"1\":{\"156\":1}}],[\"我们便称为\",{\"1\":{\"156\":1}}],[\"我们定义\",{\"1\":{\"148\":1}}],[\"我们定义最优的策略为\",{\"1\":{\"148\":1}}],[\"我们先直接求解\",{\"1\":{\"142\":1}}],[\"我们将类放到包中\",{\"1\":{\"173\":1}}],[\"我们将变量p2赋值为p1的值\",{\"1\":{\"161\":1}}],[\"我们将\",{\"1\":{\"138\":1}}],[\"我们是无法估计的\",{\"1\":{\"136\":1}}],[\"我们是引入了\",{\"1\":{\"105\":1}}],[\"我们就可以通过\",{\"1\":{\"104\":1}}],[\"我们就可以得到\",{\"1\":{\"99\":1}}],[\"我们很难直接获得\",{\"1\":{\"101\":1}}],[\"我们有以下几种方法\",{\"1\":{\"101\":1}}],[\"我们得到的观测值是\",{\"1\":{\"99\":1}}],[\"我们只能通过\",{\"1\":{\"97\":1}}],[\"我们只需要在类名前面把完整的包名也给写上\",{\"1\":{\"173\":1}}],[\"我们只需要在当前状态下\",{\"1\":{\"57\":1}}],[\"我们只需要使用\",{\"1\":{\"162\":1}}],[\"我们只需要挑选在当前迭代下最大的\",{\"1\":{\"63\":1}}],[\"我们采用的是\",{\"1\":{\"87\":1}}],[\"我们不仅可以通过构造方法\",{\"1\":{\"166\":1}}],[\"我们不仅可以用来估计q\",{\"1\":{\"81\":1}}],[\"我们不断地与环境进行交互\",{\"1\":{\"135\":1}}],[\"我们不能直接得到随机变量的值\",{\"1\":{\"99\":1}}],[\"我们不能通过之前的方法来求出q\",{\"1\":{\"77\":1}}],[\"我们选取其\",{\"1\":{\"79\":1}}],[\"我们仍需要估计\",{\"1\":{\"79\":1}}],[\"我们这里强行初始化为vπ0​​\",{\"1\":{\"70\":1}}],[\"我们需要明确指定一下\",{\"1\":{\"173\":1}}],[\"我们需要从经验池\",{\"1\":{\"142\":1}}],[\"我们需要思考使用\",{\"1\":{\"104\":1}}],[\"我们需要保证策略是不断提升\",{\"1\":{\"68\":1}}],[\"我们需要找到一个\",{\"1\":{\"26\":1}}],[\"我们可以为成员变量\",{\"1\":{\"174\":1}}],[\"我们可以理解为是属于这个类的\",{\"1\":{\"170\":1}}],[\"我们可以使用\",{\"1\":{\"166\":1,\"173\":1}}],[\"我们可以进行一次重载\",{\"1\":{\"165\":1}}],[\"我们可以认为\",{\"1\":{\"150\":1}}],[\"我们可以写出\",{\"1\":{\"113\":1}}],[\"我们可以将方法标记为静态\",{\"1\":{\"170\":1}}],[\"我们可以将各种需要初始化的操作都在这里进行处理\",{\"1\":{\"166\":1}}],[\"我们可以将该问题定义为一个\",{\"1\":{\"110\":1}}],[\"我们可以将这个问题可以转化为一个随机变量的方法\",{\"1\":{\"105\":1}}],[\"我们可以将\",{\"1\":{\"101\":1,\"103\":1,\"104\":1}}],[\"我们可以修改为噪音\",{\"1\":{\"99\":1}}],[\"我们可以设计如下方程\",{\"1\":{\"99\":1}}],[\"我们可以确保其可以遍历所有的\",{\"1\":{\"86\":1}}],[\"我们可以生成一个\",{\"1\":{\"78\":1}}],[\"我们可以通过包的形式将这些类进行分类存放\",{\"1\":{\"173\":1}}],[\"我们可以通过对象的引用来间接操作对象\",{\"1\":{\"161\":1}}],[\"我们可以通过对应的迭代算法来求解贝尔曼最优公式\",{\"1\":{\"62\":1}}],[\"我们可以通过一些特定的算法进行求解\",{\"1\":{\"96\":1}}],[\"我们可以通过前面所引入的\",{\"1\":{\"77\":1}}],[\"我们可以很轻松的求出各个情况下的q\",{\"1\":{\"77\":1}}],[\"我们可以分析出在该状态下采取哪个\",{\"1\":{\"48\":1}}],[\"我们还需要理解其所描述的最优策略π∗\",{\"1\":{\"56\":1}}],[\"我们对无人机的发射功率有一个约束\",{\"1\":{\"9\":1}}],[\"关键字来声明一个变量或一个方法为静态的\",{\"1\":{\"170\":1}}],[\"关键元素\",{\"1\":{\"20\":1}}],[\"关于对象类型的变量\",{\"1\":{\"161\":1}}],[\"修改一个对象的属性并不会影响到其他对象\",{\"1\":{\"161\":1}}],[\"修改对象的属性\",{\"1\":{\"161\":1}}],[\"修改为\",{\"1\":{\"142\":1}}],[\"访问权限控制\",{\"0\":{\"174\":1}}],[\"访问类的静态变量\",{\"1\":{\"171\":1}}],[\"访问\",{\"1\":{\"161\":1}}],[\"性别呢\",{\"1\":{\"166\":1}}],[\"性别\",{\"1\":{\"161\":1}}],[\"年龄\",{\"1\":{\"161\":1,\"166\":1}}],[\"名字\",{\"1\":{\"161\":1}}],[\"概念上的定义\",{\"1\":{\"161\":1}}],[\"概率分布\",{\"1\":{\"150\":1}}],[\"类似于c++中的namespace\",{\"1\":{\"173\":1}}],[\"类似于c++指针的情况\",{\"1\":{\"161\":1}}],[\"类除了具有属性外\",{\"1\":{\"162\":1}}],[\"类名的首字母通常是大写的\",{\"1\":{\"161\":1}}],[\"类的创建\",{\"1\":{\"161\":1}}],[\"类\",{\"1\":{\"161\":1}}],[\"类与对象3\",{\"0\":{\"177\":1}}],[\"类与对象2\",{\"0\":{\"169\":1}}],[\"类与对象\",{\"0\":{\"161\":1}}],[\"类与对象1\",{\"0\":{\"160\":1}}],[\"梯度上升算法的本质就是最大化目标函数\",{\"1\":{\"156\":1}}],[\"梯度上升算法\",{\"0\":{\"156\":1}}],[\"梯度下降\",{\"1\":{\"136\":1}}],[\"梯度下降法\",{\"1\":{\"101\":1}}],[\"近似\",{\"1\":{\"155\":1}}],[\"严格等于\",{\"1\":{\"155\":1}}],[\"任何一种\",{\"1\":{\"155\":1}}],[\"给定的策略\",{\"1\":{\"154\":1}}],[\"给定策略\",{\"1\":{\"113\":1}}],[\"时的平均reward\",{\"1\":{\"153\":1}}],[\"时wk​→w∗\",{\"1\":{\"104\":1}}],[\"确定\",{\"1\":{\"148\":1}}],[\"确保根是存在且唯一的\",{\"1\":{\"98\":1}}],[\"经验回放\",{\"0\":{\"143\":1}}],[\"网络的输入包括\",{\"1\":{\"142\":1}}],[\"然后生成了一个\",{\"1\":{\"154\":1}}],[\"然后我们通过\",{\"1\":{\"142\":1}}],[\"然后在更新了一定次数之后\",{\"1\":{\"142\":1}}],[\"然后进行迭代\",{\"1\":{\"79\":1}}],[\"然后进行不断迭代\",{\"1\":{\"47\":1}}],[\"复制到\",{\"1\":{\"142\":1}}],[\"尝试将\",{\"1\":{\"142\":1}}],[\"固定不动\",{\"1\":{\"142\":1}}],[\"固定\",{\"1\":{\"142\":1}}],[\"目的是最小化目标函数\",{\"1\":{\"141\":1}}],[\"目前的优化算法只是在估计给定策略的\",{\"1\":{\"136\":1}}],[\"目标函数梯度求解\",{\"0\":{\"155\":1}}],[\"目标函数定义\",{\"0\":{\"149\":1}}],[\"目标\",{\"1\":{\"132\":1}}],[\"目标是获得无人机的最佳3d位置\",{\"1\":{\"13\":1}}],[\"≥0\",{\"1\":{\"135\":1,\"150\":1,\"155\":1}}],[\"≥vπ​\",{\"1\":{\"53\":1}}],[\"下的\",{\"1\":{\"135\":1,\"152\":1}}],[\"下行\",{\"1\":{\"13\":1}}],[\"认为所有状态都是同等重要的\",{\"1\":{\"134\":1}}],[\"寻找一个最优的参数w\",{\"1\":{\"132\":1}}],[\"版本\",{\"0\":{\"125\":1,\"126\":1}}],[\"|\",{\"0\":{\"121\":1}}],[\"形式下的贝尔曼最优公式\",{\"1\":{\"119\":1}}],[\"称为\",{\"1\":{\"113\":1}}],[\"称为策略评估\",{\"1\":{\"45\":1}}],[\"结合去寻找最优策略\",{\"1\":{\"111\":1}}],[\"≐e\",{\"1\":{\"155\":1}}],[\"≐g\",{\"1\":{\"110\":1}}],[\"≐w−x\",{\"1\":{\"99\":1}}],[\"≐w−e\",{\"1\":{\"99\":1}}],[\"ω=e\",{\"1\":{\"110\":1}}],[\"计算\",{\"1\":{\"110\":1}}],[\"期望的情况\",{\"1\":{\"105\":1}}],[\"另一种表达\",{\"0\":{\"151\":1,\"154\":1}}],[\"另一种问题描述方法\",{\"0\":{\"105\":1}}],[\"另一些则频繁被访问\",{\"1\":{\"134\":1}}],[\"另一分布下\",{\"1\":{\"29\":1}}],[\"∇w​f\",{\"1\":{\"104\":2}}],[\"相应的算法是\",{\"1\":{\"112\":1}}],[\"相应的\",{\"1\":{\"104\":1,\"110\":1}}],[\"让我看看\",{\"1\":{\"170\":1}}],[\"让当前对象的name变量值等于参数传入的值\",{\"1\":{\"164\":1}}],[\"让\",{\"1\":{\"104\":1}}],[\"往往可以转化为导数为\",{\"1\":{\"104\":1}}],[\"=\",{\"1\":{\"116\":1,\"120\":1}}],[\"=e\",{\"1\":{\"104\":1}}],[\"=n\",{\"1\":{\"8\":1,\"11\":1,\"13\":1}}],[\"含噪声\",{\"1\":{\"104\":1}}],[\"含噪音\",{\"1\":{\"97\":1}}],[\"视为一个特殊情况下的\",{\"1\":{\"104\":1}}],[\"视为\",{\"1\":{\"104\":1,\"142\":1}}],[\"⇓wk+1​=wk​−αk​▽w​f\",{\"1\":{\"104\":1}}],[\"到\",{\"1\":{\"104\":1}}],[\"正确性和收敛性分析\",{\"0\":{\"104\":1}}],[\"转化为\",{\"1\":{\"103\":1}}],[\"转换到\",{\"1\":{\"130\":1}}],[\"转换到状态s\",{\"1\":{\"20\":1}}],[\"转换为\",{\"1\":{\"58\":1,\"76\":1,\"77\":1,\"148\":1}}],[\"均是随机变量\",{\"1\":{\"110\":1,\"141\":1}}],[\"均值估计\",{\"1\":{\"103\":1}}],[\"均收集完\",{\"1\":{\"82\":1}}],[\"分析随机变量\",{\"1\":{\"133\":1}}],[\"分析\",{\"0\":{\"102\":1}}],[\"分别设为1\",{\"1\":{\"10\":1}}],[\"借用\",{\"1\":{\"101\":1}}],[\"▽w​q^​\",{\"1\":{\"142\":2}}],[\"▽w​qt​^​\",{\"1\":{\"139\":1,\"140\":1}}],[\"▽w​vt​^​\",{\"1\":{\"136\":1,\"137\":1,\"138\":1}}],[\"▽w​v^\",{\"1\":{\"136\":1}}],[\"▽w​\",{\"1\":{\"136\":1}}],[\"▽w​j​=e\",{\"1\":{\"142\":1}}],[\"▽w​j\",{\"1\":{\"104\":1,\"136\":1}}],[\"▽w​f\",{\"1\":{\"101\":2,\"104\":10}}],[\"▽θ​π\",{\"1\":{\"155\":1}}],[\"▽θ​lnπ\",{\"1\":{\"155\":6,\"156\":1}}],[\"▽θ​ln\",{\"1\":{\"26\":3}}],[\"▽θ​j\",{\"1\":{\"26\":1,\"155\":3}}],[\"输出为标量\",{\"1\":{\"101\":1}}],[\"输入序列\",{\"1\":{\"97\":1}}],[\"⋅▽w​q^​\",{\"1\":{\"142\":1}}],[\"⋅\",{\"1\":{\"101\":1,\"110\":1}}],[\"η\",{\"1\":{\"99\":2,\"104\":2,\"110\":1,\"155\":1}}],[\"ηk2​∣hk​\",{\"1\":{\"98\":1}}],[\"ηk​∣hk​\",{\"1\":{\"98\":1}}],[\"ηk​\",{\"1\":{\"97\":3,\"99\":1,\"104\":1,\"110\":1}}],[\"应用于\",{\"0\":{\"99\":1}}],[\"应该是无人机最终停的位置即是部署的最佳位置\",{\"1\":{\"13\":1}}],[\"<∞\",{\"1\":{\"98\":1}}],[\"保证\",{\"1\":{\"98\":2}}],[\"保证选取最大的\",{\"1\":{\"57\":1}}],[\"要给成员变量设定初始值\",{\"1\":{\"166\":1}}],[\"要在对象创建时进行处理\",{\"1\":{\"166\":1}}],[\"要求g\",{\"1\":{\"98\":1}}],[\"要证明加入baseline成立\",{\"1\":{\"26\":1}}],[\"收敛性情况\",{\"1\":{\"116\":1}}],[\"收敛性分析\",{\"0\":{\"98\":1}}],[\"收集完毕才能进行一次迭代\",{\"1\":{\"101\":1}}],[\"收益最大\",{\"1\":{\"48\":1}}],[\"次的观测值\",{\"1\":{\"97\":1}}],[\"次方程根的估计\",{\"1\":{\"97\":1}}],[\"问题\",{\"1\":{\"103\":1,\"104\":1,\"110\":1}}],[\"问题转化\",{\"0\":{\"103\":1}}],[\"问题引入\",{\"0\":{\"96\":1,\"101\":1}}],[\"问题进行研究\",{\"1\":{\"92\":1}}],[\"增量式的迭代算法\",{\"1\":{\"94\":1}}],[\"第二种\",{\"1\":{\"94\":1}}],[\"第一种\",{\"1\":{\"94\":1}}],[\"引言\",{\"0\":{\"93\":1}}],[\"引入两个网络\",{\"1\":{\"142\":1}}],[\"引入\",{\"0\":{\"88\":1,\"110\":1,\"131\":1},\"1\":{\"48\":1}}],[\"引入随机变量后对应的discounted\",{\"1\":{\"39\":1}}],[\"引入discount\",{\"1\":{\"19\":1}}],[\"针对\",{\"1\":{\"92\":1}}],[\"随着我们的程序不断变大\",{\"1\":{\"173\":1}}],[\"随着无人机与用户之间距离和发射功率的变化\",{\"1\":{\"9\":1}}],[\"随机变量\",{\"1\":{\"105\":1}}],[\"随机梯度下降\",{\"1\":{\"101\":1}}],[\"随机近似理论与随机梯度下降算法\",{\"0\":{\"92\":1}}],[\"找的是在所有可能策略中的最优策略\",{\"1\":{\"88\":1}}],[\"ϵ−greedy\",{\"1\":{\"125\":1}}],[\"ϵ∈\",{\"1\":{\"87\":1}}],[\"ϵ\",{\"1\":{\"87\":2}}],[\"足够长的情况下\",{\"1\":{\"86\":1}}],[\"条件转换掉\",{\"1\":{\"85\":1}}],[\"据目前而言\",{\"1\":{\"84\":1}}],[\"未能访问\",{\"1\":{\"84\":1}}],[\"未知时的情况\",{\"1\":{\"97\":1}}],[\"未知\",{\"1\":{\"77\":1,\"96\":1,\"166\":1}}],[\"直接使用就可以\",{\"1\":{\"174\":1}}],[\"直接使用即可\",{\"1\":{\"173\":1}}],[\"直接赋值\",{\"1\":{\"166\":1}}],[\"直接在类中定义变量\",{\"1\":{\"161\":1}}],[\"直接估计的是\",{\"1\":{\"120\":1}}],[\"直接通过\",{\"1\":{\"94\":1}}],[\"直接\",{\"1\":{\"82\":1}}],[\"直到设置的收敛条件为止\",{\"1\":{\"66\":1}}],[\"直到各个簇的成员没有太大变化\",{\"1\":{\"13\":1}}],[\"才能进行\",{\"1\":{\"82\":1}}],[\"才进行更新\",{\"1\":{\"82\":1}}],[\"等候时间过长\",{\"1\":{\"82\":1}}],[\"等均未知\",{\"1\":{\"75\":1}}],[\"缺点\",{\"1\":{\"82\":1}}],[\"估计了q\",{\"1\":{\"82\":1}}],[\"估计中\",{\"1\":{\"81\":2}}],[\"高效地更新\",{\"0\":{\"82\":1}}],[\"高度h=hn​\",{\"1\":{\"11\":1}}],[\"高度的上界是最大发射功率pmax​的函数\",{\"1\":{\"9\":1}}],[\"都是直接使用new关键字就能直接搞定了\",{\"1\":{\"166\":1}}],[\"都是最大的\",{\"1\":{\"148\":1}}],[\"都需要有多个\",{\"1\":{\"84\":1}}],[\"都记录\",{\"1\":{\"81\":1}}],[\"都有可选择的动作\",{\"1\":{\"19\":1}}],[\"记录在\",{\"1\":{\"81\":1}}],[\"仅用作估计\",{\"1\":{\"81\":1}}],[\"仅需要考虑无人机的7个移动方向即可\",{\"1\":{\"14\":1}}],[\"节所述\",{\"1\":{\"79\":1}}],[\"有着不同程度的访问限制\",{\"1\":{\"174\":1}}],[\"有点混乱\",{\"1\":{\"173\":1}}],[\"有些时候\",{\"1\":{\"165\":1}}],[\"有时候我们的方法中可能会出现一些与成员变量重名的变量\",{\"1\":{\"164\":1}}],[\"有表示\",{\"1\":{\"155\":1}}],[\"有\",{\"1\":{\"79\":1}}],[\"有关\",{\"1\":{\"48\":1,\"152\":1}}],[\"迭代\",{\"1\":{\"79\":1}}],[\"首先初始化一个随机的策略π0​\",{\"1\":{\"79\":1}}],[\"首先随机设计一个初始的策略π0​\",{\"1\":{\"66\":1}}],[\"≈▽θ​lnπ\",{\"1\":{\"155\":1}}],[\"≈vπ​\",{\"1\":{\"131\":1}}],[\"≈n1​i=1∑n​▽w​f\",{\"1\":{\"101\":1}}],[\"≈n1​i=1∑n​g\",{\"1\":{\"78\":1}}],[\"≈xˉ\",{\"1\":{\"94\":1}}],[\"≈xˉ=n1​j=1∑n​xj​\",{\"1\":{\"75\":1}}],[\"假设有\",{\"1\":{\"142\":1}}],[\"假设我们具有\",{\"1\":{\"116\":1}}],[\"假设我们可以得到有关随机变量\",{\"1\":{\"110\":1}}],[\"假设我们需要求解如下方程\",{\"1\":{\"96\":1}}],[\"假设我们有了一系列\",{\"1\":{\"78\":1}}],[\"假设\",{\"1\":{\"94\":1,\"154\":1}}],[\"假设功率q=qn​\",{\"1\":{\"11\":1}}],[\"已知的情况\",{\"1\":{\"96\":1}}],[\"已知\",{\"1\":{\"77\":1}}],[\"先根据当前策略计算出各个状态的\",{\"1\":{\"77\":1}}],[\"效率过低\",{\"1\":{\"76\":1}}],[\"样本\",{\"1\":{\"142\":1}}],[\"样本必须是独立同分布\",{\"1\":{\"75\":1}}],[\"样本采样\",{\"1\":{\"75\":1}}],[\"大数定理\",{\"1\":{\"75\":1}}],[\"若有一系列\",{\"1\":{\"75\":1}}],[\"蒙特卡洛方法\",{\"0\":{\"75\":1}}],[\"就像在这个类定义的方法一样\",{\"1\":{\"174\":1}}],[\"就像下面这样\",{\"1\":{\"170\":1}}],[\"就需要注意了\",{\"1\":{\"173\":1}}],[\"就需要新的算法进行解决\",{\"1\":{\"96\":1}}],[\"就可以表示这个是哪一个包里的类了\",{\"1\":{\"173\":1}}],[\"就可以执行定义好的方法了\",{\"1\":{\"162\":1}}],[\"就可以进行一定操作\",{\"1\":{\"161\":1}}],[\"就不太可行\",{\"1\":{\"134\":1}}],[\"就不需要是\",{\"1\":{\"125\":1}}],[\"就不能确保所选择的\",{\"1\":{\"84\":1}}],[\"就直接去更新策略\",{\"1\":{\"82\":1}}],[\"就是用于分割的\",{\"1\":{\"173\":1}}],[\"就是域名\",{\"1\":{\"173\":1}}],[\"就是要操作\",{\"1\":{\"161\":1}}],[\"就是\",{\"1\":{\"87\":1}}],[\"就是前面\",{\"1\":{\"78\":1}}],[\"就是进行迭代\",{\"1\":{\"71\":1}}],[\"就好了\",{\"1\":{\"63\":1}}],[\"⋮\",{\"1\":{\"70\":4}}],[\"方便后续比较\",{\"1\":{\"70\":1}}],[\"方法体\",{\"1\":{\"162\":1}}],[\"方法名称\",{\"1\":{\"162\":2}}],[\"方法名称同样可以随便起\",{\"1\":{\"162\":1}}],[\"方法的返回类型\",{\"1\":{\"165\":1}}],[\"方法的重载\",{\"0\":{\"165\":1}}],[\"方法的进阶使用\",{\"0\":{\"163\":1}}],[\"方法的调用\",{\"1\":{\"162\":1}}],[\"方法的定义如下\",{\"1\":{\"162\":1}}],[\"方法的创建与使用\",{\"0\":{\"162\":1}}],[\"方法是语句的集合\",{\"1\":{\"162\":1}}],[\"方法进行求解\",{\"1\":{\"136\":1}}],[\"方法2\",{\"1\":{\"82\":1}}],[\"方法1\",{\"1\":{\"82\":1}}],[\"方法\",{\"0\":{\"23\":1},\"1\":{\"77\":1,\"155\":1}}],[\"两个算法迭代过程十分类似\",{\"1\":{\"70\":1}}],[\"进行优化\",{\"1\":{\"148\":1}}],[\"进行训练\",{\"1\":{\"142\":1}}],[\"进行赋值\",{\"1\":{\"142\":1}}],[\"进行固定求解\",{\"1\":{\"142\":1}}],[\"进行生成数据\",{\"1\":{\"125\":1}}],[\"进行更新即可\",{\"1\":{\"115\":1}}],[\"进行的采样\",{\"1\":{\"84\":1}}],[\"进行估计\",{\"1\":{\"78\":1,\"94\":1}}],[\"进行了无穷多步来进行了真实的求解\",{\"1\":{\"71\":1}}],[\"进行迭代\",{\"1\":{\"70\":1,\"101\":1}}],[\"进行求解\",{\"1\":{\"47\":1,\"66\":1,\"103\":1}}],[\"内嵌迭代算法求解\",{\"1\":{\"70\":1}}],[\"之前就是直接使用的\",{\"1\":{\"173\":1}}],[\"之前介绍的方法都是\",{\"1\":{\"147\":1}}],[\"之后进行迭代\",{\"1\":{\"70\":1}}],[\"之间存在什么关系\",{\"1\":{\"68\":1}}],[\"那么我们可以通过静态导入的方式将其中的静态方法或是静态变量直接导入使用\",{\"1\":{\"174\":1}}],[\"那么我们可以对\",{\"1\":{\"78\":1}}],[\"那么此时就出现了歧义\",{\"1\":{\"173\":1}}],[\"那么此时的\",{\"1\":{\"135\":1}}],[\"那么这些类如果都放在一起的话\",{\"1\":{\"173\":1}}],[\"那么这称为\",{\"1\":{\"84\":1}}],[\"那么其他的对象读取的就是被改变的值\",{\"1\":{\"170\":1}}],[\"那么通过这个类创建的所有对象\",{\"1\":{\"170\":1}}],[\"那么能否实现在对象创建时就为其指定名字\",{\"1\":{\"166\":1}}],[\"那么默认情况下可以不使用this关键字来明确表示当前对象\",{\"1\":{\"164\":1}}],[\"那么默认是null\",{\"1\":{\"161\":1}}],[\"那么默认是统一为0\",{\"1\":{\"161\":1}}],[\"那么可以使用this关键字\",{\"1\":{\"164\":1}}],[\"那么创建对象之后能否直接访问呢\",{\"1\":{\"161\":1}}],[\"那么实际上只是传递了对象的引用\",{\"1\":{\"161\":1}}],[\"那么对象构造好之后\",{\"1\":{\"166\":1}}],[\"那么对象的属性都会存在初始值\",{\"1\":{\"161\":1}}],[\"那么对应的损失函数求解为\",{\"1\":{\"142\":1}}],[\"那么对于随机变量x的估计可以为\",{\"1\":{\"75\":1}}],[\"那么只要求解\",{\"1\":{\"99\":1}}],[\"那么\",{\"1\":{\"68\":1,\"94\":1,\"98\":1}}],[\"实际上main就是一个函数\",{\"1\":{\"162\":1}}],[\"实际不常用\",{\"1\":{\"68\":1}}],[\"实际意义的解释\",{\"1\":{\"48\":1}}],[\"实际意义是\",{\"1\":{\"48\":1}}],[\"得到含有噪音的观测值序列\",{\"1\":{\"97\":1}}],[\"得到的\",{\"1\":{\"84\":1}}],[\"得到一个样本序列x1​\",{\"1\":{\"75\":1}}],[\"得到\",{\"1\":{\"68\":1}}],[\"一旦被声明为静态\",{\"1\":{\"170\":1}}],[\"一般遇到以下情况时才会会加载类\",{\"1\":{\"171\":1}}],[\"一般情况下\",{\"1\":{\"170\":1}}],[\"一般使用驼峰命名法最规范\",{\"1\":{\"162\":1}}],[\"一般化的推广\",{\"1\":{\"69\":1}}],[\"一些特性\",{\"1\":{\"155\":1}}],[\"一些细节\",{\"1\":{\"142\":1,\"156\":1}}],[\"一些状态可能很少被访问\",{\"1\":{\"134\":1}}],[\"一些问题\",{\"0\":{\"68\":1}}],[\"一致也是可以的\",{\"1\":{\"124\":1}}],[\"一个对象改变了静态变量的值\",{\"1\":{\"170\":1}}],[\"一个类中可以包含多个同名的方法\",{\"1\":{\"165\":1}}],[\"一个\",{\"1\":{\"103\":1}}],[\"一定可以遍历所给定的\",{\"1\":{\"84\":1}}],[\"一样\",{\"1\":{\"82\":1}}],[\"一种则是有所偏向\",{\"1\":{\"152\":1}}],[\"一种是将所有状态视为同等重要\",{\"1\":{\"152\":1}}],[\"一种是通过迭代算法来求解\",{\"1\":{\"68\":1}}],[\"一种是可以直接通过矩阵求逆进行求解\",{\"1\":{\"68\":1}}],[\"一种迭代策略\",{\"1\":{\"47\":1}}],[\"jvm\",{\"1\":{\"171\":2}}],[\"java中引入了访问权限控制\",{\"1\":{\"174\":1}}],[\"java会默认导入java\",{\"1\":{\"173\":1}}],[\"java\",{\"0\":{\"160\":1,\"169\":1,\"177\":1,\"189\":1},\"1\":{\"171\":1,\"173\":1},\"2\":{\"168\":1,\"176\":1,\"183\":1}}],[\"j\",{\"1\":{\"66\":2,\"68\":1,\"78\":1,\"101\":1,\"104\":1,\"133\":1,\"134\":1,\"135\":1,\"136\":1,\"141\":1,\"142\":3,\"148\":1,\"151\":1,\"155\":1,\"156\":1}}],[\"j+1\",{\"1\":{\"66\":2,\"68\":1}}],[\"步骤类似\",{\"1\":{\"79\":1}}],[\"步骤中\",{\"1\":{\"68\":2}}],[\"步来求解\",{\"1\":{\"71\":1}}],[\"步\",{\"1\":{\"63\":1}}],[\"动态规划\",{\"0\":{\"61\":1}}],[\"动作\",{\"1\":{\"19\":1,\"142\":1}}],[\"动作空间包含两个部分\",{\"1\":{\"14\":1}}],[\"动作空间\",{\"1\":{\"13\":1}}],[\"满足f\",{\"1\":{\"58\":1}}],[\"区别于贝尔曼公式\",{\"1\":{\"56\":1}}],[\"外\",{\"1\":{\"55\":1}}],[\"作为下一步的\",{\"1\":{\"53\":1}}],[\"作用\",{\"1\":{\"26\":1}}],[\"选择对应的动作\",{\"1\":{\"84\":1}}],[\"选择移动方向和选择关联用户\",{\"1\":{\"14\":1}}],[\"选取当前状态下最大的\",{\"1\":{\"70\":2}}],[\"选取状态中最大的\",{\"1\":{\"53\":1}}],[\"唯一\",{\"1\":{\"53\":1}}],[\"存在不动点x∗\",{\"1\":{\"58\":1}}],[\"存在\",{\"1\":{\"53\":1}}],[\"存在lemma1\",{\"1\":{\"9\":1}}],[\"贝尔曼最优公式\",{\"0\":{\"52\":1},\"1\":{\"61\":1}}],[\"贝尔曼公式\",{\"0\":{\"36\":1}}],[\"总结\",{\"0\":{\"49\":1}}],[\"总用户的mos取决于无人机的发射功率\",{\"1\":{\"11\":1}}],[\"加权均值\",{\"1\":{\"48\":1}}],[\"加上\",{\"1\":{\"48\":1}}],[\"和\",{\"1\":{\"48\":1,\"82\":1,\"87\":1,\"101\":1,\"105\":1,\"122\":1,\"123\":1,\"142\":2}}],[\"和动作\",{\"1\":{\"48\":1}}],[\"和e\",{\"1\":{\"42\":1}}],[\"采取动作\",{\"1\":{\"78\":1}}],[\"采取一个指定的action可以得到的平均return\",{\"1\":{\"48\":1}}],[\"采用梯度下降\",{\"1\":{\"142\":1}}],[\"采用\",{\"1\":{\"79\":1,\"136\":1}}],[\"采用的是the\",{\"1\":{\"14\":1}}],[\"采用q\",{\"1\":{\"13\":1}}],[\"采用基于遗传算法的gak\",{\"1\":{\"13\":1}}],[\"采用mos作为用户qos衡量的标准\",{\"1\":{\"10\":1}}],[\"k→0∑k=1∞​ak​=∞\",{\"1\":{\"98\":1}}],[\"k→∞\",{\"1\":{\"47\":1,\"58\":1,\"104\":1}}],[\"k\",{\"1\":{\"97\":2}}],[\"k−1\",{\"1\":{\"94\":1}}],[\"k=2\",{\"1\":{\"94\":1}}],[\"k=1∑n​rt+k​\",{\"1\":{\"154\":1}}],[\"k=1∑n​rt+k​∣st​=s0​\",{\"1\":{\"154\":1}}],[\"k=1\",{\"1\":{\"62\":1,\"94\":1,\"97\":1}}],[\"kth\",{\"1\":{\"79\":1}}],[\"kn​∩kn\",{\"1\":{\"8\":1,\"11\":1,\"13\":1}}],[\"kn​\",{\"1\":{\"8\":1,\"9\":2}}],[\"整合\",{\"1\":{\"46\":1}}],[\"无法使用this关键字\",{\"1\":{\"170\":1}}],[\"无法获取成员变量的值\",{\"1\":{\"170\":1}}],[\"无关\",{\"1\":{\"152\":1}}],[\"无论是\",{\"1\":{\"92\":1}}],[\"无记忆性\",{\"1\":{\"44\":1}}],[\"无人机需要进行移动\",{\"1\":{\"14\":1}}],[\"无人机的动态移动设计\",{\"0\":{\"14\":1}}],[\"无人机的位置初始化也是随机部署的\",{\"1\":{\"13\":1}}],[\"无人机的3d部署\",{\"0\":{\"13\":1}}],[\"无人机3d部署算法\",{\"1\":{\"13\":1}}],[\"无人机\",{\"1\":{\"13\":1}}],[\"无人机n以可变高度悬停在用户上方\",{\"1\":{\"13\":1}}],[\"无人机n的高度需满足\",{\"1\":{\"9\":1}}],[\"无人机n与用户kn​在时间t的距离表示为\",{\"1\":{\"8\":1}}],[\"无人机往往有更高的los链接概率\",{\"1\":{\"9\":1}}],[\"用于指定当前类所处的包的\",{\"1\":{\"173\":1}}],[\"用于解释\",{\"1\":{\"76\":1}}],[\"用来描述所有状态的state\",{\"1\":{\"42\":1}}],[\"用户的速度设为\",{\"1\":{\"14\":1}}],[\"用户漫游模型\",{\"1\":{\"14\":1}}],[\"用户区域划分算法\",{\"1\":{\"13\":1}}],[\"用户关联策略\",{\"1\":{\"13\":1}}],[\"用户是保持静态的\",{\"1\":{\"13\":1}}],[\"用户rkn​​在一段时间ts​内的mos总和为\",{\"1\":{\"10\":1}}],[\"求均值的方法\",{\"0\":{\"94\":1}}],[\"求在策略πk​下所有的\",{\"1\":{\"79\":1}}],[\"求解梯度还是很好求的\",{\"1\":{\"142\":1}}],[\"求解给定策略\",{\"1\":{\"111\":1}}],[\"求解当前策略的\",{\"1\":{\"70\":1}}],[\"求解下一步的vk+1​\",{\"1\":{\"63\":1}}],[\"求解方法\",{\"1\":{\"58\":1}}],[\"求解\",{\"0\":{\"58\":1},\"1\":{\"49\":1}}],[\"求解bellman\",{\"1\":{\"45\":1}}],[\"求出其对应状态的\",{\"1\":{\"47\":1}}],[\"求\",{\"1\":{\"41\":1}}],[\"也是不同的两个类\",{\"1\":{\"173\":1}}],[\"也是尽量使用小写字母开头的单词\",{\"1\":{\"162\":1}}],[\"也只有这一个静态的变量或方法\",{\"1\":{\"170\":1}}],[\"也就是说\",{\"1\":{\"170\":1}}],[\"也可以被子类访问\",{\"1\":{\"174\":1}}],[\"也可以用来将我们的类进行分类\",{\"1\":{\"173\":1}}],[\"也可以理解为是所有对象共享的内容\",{\"1\":{\"170\":1}}],[\"也可以直接在定义时赋值\",{\"1\":{\"166\":1}}],[\"也可以不同\",{\"1\":{\"165\":1}}],[\"也可以采用基于\",{\"1\":{\"156\":1}}],[\"也可以是向量\",{\"1\":{\"101\":1}}],[\"也可以是无限长的trajectory\",{\"1\":{\"19\":1}}],[\"也称为\",{\"1\":{\"40\":1}}],[\"核心内容\",{\"0\":{\"37\":1}}],[\"核心思想\",{\"1\":{\"25\":1,\"76\":1}}],[\"伪代码\",{\"0\":{\"32\":1,\"64\":1,\"67\":1,\"124\":1,\"144\":1},\"1\":{\"116\":1}}],[\"中使用一个类之前\",{\"1\":{\"171\":1}}],[\"中取出一定数量的样本\",{\"1\":{\"142\":1}}],[\"中采用一个\",{\"1\":{\"142\":1}}],[\"中直接根据\",{\"1\":{\"115\":1}}],[\"中的\",{\"1\":{\"88\":1,\"142\":2}}],[\"中不断切换\",{\"1\":{\"82\":1}}],[\"中第一次出现的\",{\"1\":{\"81\":1}}],[\"中求解\",{\"1\":{\"68\":1}}],[\"中涉及的\",{\"1\":{\"62\":1}}],[\"中\",{\"0\":{\"99\":1},\"1\":{\"29\":1,\"71\":1,\"77\":1,\"88\":1,\"92\":1,\"137\":1,\"142\":1}}],[\"重新修改为\",{\"1\":{\"46\":1}}],[\"重要性采样\",{\"0\":{\"30\":1},\"1\":{\"29\":1}}],[\"重复步骤\",{\"1\":{\"13\":1}}],[\"3\",{\"0\":{\"28\":1,\"29\":1,\"30\":1,\"31\":1,\"32\":2,\"41\":1,\"45\":1,\"47\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":1,\"79\":1,\"83\":1,\"85\":1,\"86\":1,\"87\":1,\"88\":2,\"89\":2,\"98\":1,\"100\":1,\"101\":1,\"102\":1,\"105\":2,\"106\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":1,\"118\":2,\"124\":1,\"139\":1,\"155\":1},\"1\":{\"62\":2,\"70\":1,\"94\":1,\"97\":1,\"101\":1}}],[\"3d部署和移动问题\",{\"1\":{\"5\":1}}],[\"3d位置进行优化处理\",{\"1\":{\"5\":1}}],[\"只能被类本身和同包中的其他类访问\",{\"1\":{\"174\":1}}],[\"只有在类不在同一个包下时才需要进行导入\",{\"1\":{\"173\":1}}],[\"只有当样本全部收集完才能估计\",{\"1\":{\"94\":1}}],[\"只有当所有\",{\"1\":{\"82\":1}}],[\"只有当所有东西都是确定性的\",{\"1\":{\"41\":1}}],[\"只需要在类名前面添加包名就行了\",{\"1\":{\"173\":1}}],[\"只需要\",{\"1\":{\"162\":1}}],[\"只需要保证\",{\"1\":{\"26\":1}}],[\"只给出了梯度的公式\",{\"1\":{\"155\":1}}],[\"只不过我们此时需要遍历所有的\",{\"1\":{\"79\":1}}],[\"只不过根据区域划分\",{\"1\":{\"13\":1}}],[\"只是不用我们去写\",{\"1\":{\"166\":1}}],[\"只是进行了一步求解\",{\"1\":{\"71\":1}}],[\"只是求解各状态的\",{\"1\":{\"56\":1}}],[\"证明\",{\"1\":{\"26\":1}}],[\"θt+1​=θt​+α▽θ​lnπ\",{\"1\":{\"156\":2}}],[\"θt+1​=θt​+α▽θ​j\",{\"1\":{\"148\":1}}],[\"θt+1​​=θt​+α▽θ​j\",{\"1\":{\"156\":1}}],[\"θt​\",{\"1\":{\"148\":1,\"156\":3}}],[\"θ∈rm表示参数向量\",{\"1\":{\"148\":1}}],[\"θ\",{\"1\":{\"26\":4,\"148\":2,\"151\":1,\"155\":20,\"156\":2}}],[\"θkn​​\",{\"1\":{\"9\":1}}],[\"提高泛化能力\",{\"1\":{\"131\":1}}],[\"提升采样的效率\",{\"1\":{\"25\":1}}],[\"提出解决总用户mos最大化问题的三步骤\",{\"1\":{\"5\":1}}],[\"提出了一个理想的由qoe驱动的多无人机协助通信框架\",{\"1\":{\"5\":1}}],[\"而当我们需要使用其他包中的类时\",{\"1\":{\"173\":1}}],[\"而每一个\",{\"1\":{\"171\":1}}],[\"而非\",{\"1\":{\"166\":1}}],[\"而不是在对象创建的时候分配\",{\"1\":{\"171\":1}}],[\"而不是具体的某个对象\",{\"1\":{\"170\":1}}],[\"而不是对象本身的复制\",{\"1\":{\"161\":1}}],[\"而不是本体\",{\"1\":{\"161\":1}}],[\"而不是仅考虑吞吐量\",{\"1\":{\"4\":1}}],[\"而对应的真实梯度可以用一个估计的梯度来替代\",{\"1\":{\"156\":1}}],[\"而对应的策略梯度上升算法就是对应\",{\"1\":{\"23\":1}}],[\"而是在需要时才会去加载\",{\"1\":{\"171\":1}}],[\"而是在这之前就已经完成了\",{\"1\":{\"166\":1}}],[\"而是通过这个类去使用\",{\"1\":{\"170\":1}}],[\"而是等\",{\"1\":{\"142\":1}}],[\"而是对应的样本\",{\"1\":{\"99\":1}}],[\"而最小值问题\",{\"1\":{\"104\":1}}],[\"而言\",{\"1\":{\"70\":1}}],[\"而\",{\"1\":{\"41\":1,\"70\":1,\"71\":1}}],[\"算法是用来解决\",{\"1\":{\"119\":1}}],[\"算法是用来求解一个\",{\"1\":{\"113\":1}}],[\"算法如下来进行估计\",{\"1\":{\"116\":1}}],[\"算法同样是来求解\",{\"1\":{\"115\":1}}],[\"算法其目的是用于直接估计\",{\"1\":{\"115\":1}}],[\"算法分析\",{\"0\":{\"113\":1}}],[\"算法所需的数据\",{\"1\":{\"112\":1}}],[\"算法为\",{\"1\":{\"110\":1}}],[\"算法进行求解\",{\"1\":{\"110\":1}}],[\"算法进行求解g\",{\"1\":{\"104\":1}}],[\"算法\",{\"0\":{\"114\":1,\"157\":1},\"1\":{\"104\":2,\"105\":1,\"136\":1}}],[\"算法来进行求解\",{\"1\":{\"99\":1}}],[\"算法就可以用来求解当\",{\"1\":{\"97\":1}}],[\"算法介绍\",{\"0\":{\"97\":1}}],[\"算法流程\",{\"0\":{\"89\":1}}],[\"算法中\",{\"0\":{\"88\":1},\"1\":{\"138\":1}}],[\"算法的统一形式和总结\",{\"0\":{\"127\":1}}],[\"算法的比较\",{\"0\":{\"114\":1}}],[\"算法的问题描述中\",{\"1\":{\"105\":1}}],[\"算法的核心是\",{\"1\":{\"77\":1}}],[\"算法的基础上\",{\"1\":{\"23\":1}}],[\"算法思路\",{\"0\":{\"77\":1}}],[\"算法比较\",{\"0\":{\"70\":1}}],[\"算法描述\",{\"0\":{\"66\":1,\"112\":1}}],[\"算法迭代示意图\",{\"1\":{\"65\":1}}],[\"算法1\",{\"1\":{\"13\":1}}],[\"策略梯度法\",{\"0\":{\"147\":1}}],[\"策略梯度上升\",{\"1\":{\"23\":1}}],[\"策略了\",{\"1\":{\"125\":1}}],[\"策略求解\",{\"1\":{\"70\":2}}],[\"策略提升\",{\"1\":{\"66\":1}}],[\"策略评估\",{\"1\":{\"66\":1}}],[\"策略更新\",{\"1\":{\"29\":1,\"82\":1}}],[\"策略为贪心策略\",{\"1\":{\"13\":1}}],[\"void\",{\"1\":{\"161\":3,\"162\":2,\"164\":2,\"170\":2,\"173\":2,\"174\":2}}],[\"vˉπ0​\",{\"1\":{\"152\":1,\"155\":1}}],[\"vˉπ​=es∼d​\",{\"1\":{\"150\":1}}],[\"vˉπ​=s∈s∑​d\",{\"1\":{\"150\":1}}],[\"vˉπ​\",{\"1\":{\"150\":1,\"152\":1,\"155\":1}}],[\"v^\",{\"1\":{\"131\":1}}],[\"vt​\",{\"1\":{\"112\":3}}],[\"vt+1​\",{\"1\":{\"112\":2}}],[\"visit\",{\"1\":{\"81\":4,\"84\":2}}],[\"vu\",{\"1\":{\"70\":1}}],[\"v0​\",{\"1\":{\"70\":2}}],[\"vπ1​​\",{\"1\":{\"70\":1}}],[\"vπ1​​=rπ1​​+γpπ1​​vπ1​​\",{\"1\":{\"70\":1}}],[\"vπ0​​是通过迭代算法来求的\",{\"1\":{\"70\":1}}],[\"vπ0​​=rπ0​​+γpπ0​​vπ0​​\",{\"1\":{\"70\":1}}],[\"vπ0​​≤vπ1​​≤vπ2​​⋯≤vπk​​≤⋯≤v∗\",{\"1\":{\"68\":1}}],[\"vπk​​=\",{\"1\":{\"68\":1}}],[\"vπk​​=rπk​​+γpπk​​vπk​​policyimprovement\",{\"1\":{\"77\":1}}],[\"vπk​​=rπk​​+γpπk​​vπk​​\",{\"1\":{\"66\":1,\"70\":1}}],[\"vπk​​\",{\"1\":{\"66\":1,\"68\":1,\"77\":1}}],[\"vπk​\",{\"1\":{\"66\":2,\"68\":1}}],[\"vπ​=\",{\"1\":{\"46\":1,\"47\":1}}],[\"vπ​=rπ​+γpπ​vπ​​\",{\"1\":{\"46\":1}}],[\"vπ​\",{\"1\":{\"27\":1,\"40\":2,\"42\":1,\"44\":1,\"45\":3,\"46\":4,\"48\":5,\"49\":1,\"55\":2,\"57\":1,\"75\":1,\"112\":1,\"113\":4,\"133\":1,\"134\":2,\"135\":2,\"136\":5,\"137\":1,\"138\":1,\"150\":2}}],[\"v\",{\"1\":{\"58\":5,\"61\":1,\"63\":2,\"110\":1}}],[\"v=f\",{\"1\":{\"58\":1,\"61\":1}}],[\"v=πmax​\",{\"1\":{\"55\":1}}],[\"vk​\",{\"1\":{\"62\":1}}],[\"vk​→vπ​=\",{\"1\":{\"47\":1}}],[\"vk+1​=rπk+1​​+γpπk+1​​vk​\",{\"1\":{\"70\":1}}],[\"vk+1​=rπk+1​​+γpπk+1​​vk​这里的vk​并不是\",{\"1\":{\"63\":1}}],[\"vk+1​=rπ​+γpπ​vk​​\",{\"1\":{\"47\":1}}],[\"vk+1​=f\",{\"1\":{\"62\":1}}],[\"v2​\",{\"1\":{\"47\":1}}],[\"v1​=rπ1​​+γpπ1​​v0​\",{\"1\":{\"70\":1}}],[\"v1​\",{\"1\":{\"47\":1}}],[\"values\",{\"1\":{\"131\":2}}],[\"value后\",{\"1\":{\"77\":1}}],[\"value的关系\",{\"1\":{\"42\":1}}],[\"value\",{\"0\":{\"38\":1,\"40\":1,\"41\":1,\"47\":1,\"48\":1,\"58\":1,\"62\":1,\"70\":1,\"111\":1,\"115\":1,\"119\":1,\"130\":1,\"132\":1,\"150\":1,\"153\":1},\"1\":{\"23\":1,\"37\":1,\"40\":5,\"41\":3,\"42\":1,\"45\":1,\"47\":1,\"48\":10,\"49\":4,\"52\":1,\"53\":1,\"55\":1,\"56\":1,\"57\":2,\"62\":1,\"63\":3,\"66\":3,\"68\":2,\"69\":1,\"70\":12,\"71\":2,\"75\":2,\"77\":6,\"79\":6,\"81\":3,\"82\":2,\"84\":1,\"92\":2,\"111\":1,\"113\":2,\"115\":2,\"116\":1,\"119\":2,\"120\":1,\"130\":1,\"147\":1,\"148\":1,\"150\":1}}],[\"vector\",{\"0\":{\"46\":1},\"1\":{\"46\":1,\"49\":1,\"55\":1}}],[\"very\",{\"1\":{\"10\":1}}],[\"vehicles\",{\"1\":{\"4\":1}}],[\"vehicular\",{\"1\":{\"3\":1},\"2\":{\"16\":1}}],[\"或者为静态变量赋值\",{\"1\":{\"171\":1}}],[\"或者\",{\"1\":{\"23\":1,\"66\":1,\"131\":1,\"156\":1}}],[\"强化学习\",{\"0\":{\"188\":1},\"2\":{\"22\":1,\"35\":1,\"51\":1,\"60\":1,\"74\":1,\"91\":1,\"108\":1,\"129\":1,\"146\":1,\"159\":1}}],[\"强化学习框架图\",{\"0\":{\"18\":1}}],[\"获得reward\",{\"1\":{\"20\":1}}],[\"∣\",{\"1\":{\"87\":1}}],[\"∣−1action\",{\"1\":{\"87\":1}}],[\"∣−1\",{\"1\":{\"87\":1}}],[\"∣a\",{\"1\":{\"87\":3}}],[\"∣ϵ​\",{\"1\":{\"87\":2}}],[\"∣∣≤δ\",{\"1\":{\"66\":1}}],[\"∣∣≤γ∣∣x1​−x2​∣∣\",{\"1\":{\"58\":1}}],[\"∣∣vπk+1​\",{\"1\":{\"66\":1}}],[\"∣∣f\",{\"1\":{\"58\":1}}],[\"∣st​=s\",{\"1\":{\"119\":1}}],[\"∣s=s\",{\"1\":{\"113\":2}}],[\"∣s\",{\"1\":{\"20\":1,\"41\":1,\"44\":4,\"45\":3,\"46\":1,\"48\":2,\"55\":1,\"63\":2,\"66\":2,\"75\":1,\"77\":3,\"113\":1,\"115\":1}}],[\"∣kn​∣为第n个簇的用户总数\",{\"1\":{\"14\":1}}],[\"控制其一直待在target\",{\"1\":{\"19\":1}}],[\"会在类刚加载的时候就分配\",{\"1\":{\"171\":1}}],[\"会覆盖掉默认的那一个无参构造方法\",{\"1\":{\"166\":1}}],[\"会出现异常\",{\"1\":{\"161\":1}}],[\"会一直进行更新\",{\"1\":{\"142\":1}}],[\"会与环境一直交互下去\",{\"1\":{\"19\":1}}],[\"会将其划分为n个簇\",{\"1\":{\"8\":1}}],[\"这种情况下的\",{\"1\":{\"152\":1}}],[\"这种情况我们将\",{\"1\":{\"152\":1}}],[\"这种迭代算法称为\",{\"1\":{\"62\":1}}],[\"这是我们所求的量\",{\"1\":{\"136\":1}}],[\"这是来求解\",{\"1\":{\"79\":1}}],[\"这另一个策略会更新到最优的策略\",{\"1\":{\"123\":1}}],[\"这类算法统称为\",{\"1\":{\"82\":1}}],[\"这样就可以与\",{\"1\":{\"111\":1}}],[\"这样才能去估计相应的qπ​\",{\"1\":{\"84\":1}}],[\"这样\",{\"1\":{\"81\":1}}],[\"这样的任务称为episodic\",{\"1\":{\"19\":1}}],[\"这一条\",{\"1\":{\"81\":1}}],[\"这个仍然与之前一致\",{\"1\":{\"79\":1}}],[\"这个\",{\"1\":{\"78\":1}}],[\"这个便成为\",{\"1\":{\"23\":1}}],[\"这两个算法是一致的\",{\"1\":{\"70\":1}}],[\"这步是更新策略π\",{\"1\":{\"63\":1}}],[\"这里我们定义一个info静态变量\",{\"1\":{\"170\":1}}],[\"这里是\",{\"1\":{\"166\":1}}],[\"这里是每个无人机都有一张自己的q\",{\"1\":{\"13\":1}}],[\"这里需要特别注意\",{\"1\":{\"166\":1}}],[\"这里需要通过迭代算法来精确求出\",{\"1\":{\"70\":1}}],[\"这里没有使用this\",{\"1\":{\"164\":1}}],[\"这里实际上是将方法参数的局部变量name赋值为本身\",{\"1\":{\"164\":1}}],[\"这里编写代码跟我们之前在main中是一样的\",{\"1\":{\"162\":1}}],[\"这里定义的人类具有三个属性\",{\"1\":{\"161\":1}}],[\"这里可以用不同的方法来近似\",{\"1\":{\"156\":1}}],[\"这里的p1存放的是对象的引用\",{\"1\":{\"161\":1}}],[\"这里的a存放的是具体的某个值\",{\"1\":{\"161\":1}}],[\"这里的策略是随机性\",{\"1\":{\"155\":1}}],[\"这里的最优策略πk+1​是一个\",{\"1\":{\"63\":1}}],[\"这里在视频没有详细介绍\",{\"1\":{\"155\":1}}],[\"这里包含了一个\",{\"1\":{\"136\":1}}],[\"这里\",{\"1\":{\"66\":1}}],[\"这里有多种mobility\",{\"1\":{\"14\":1}}],[\"这里采用离散化空间坐标\",{\"1\":{\"13\":1}}],[\"通常也称为\",{\"1\":{\"135\":1}}],[\"通常是具有有限步长的trajectory\",{\"1\":{\"19\":1}}],[\"通过梯度的算法来最小化对应的损失函数\",{\"1\":{\"142\":1}}],[\"通过使用一个函数来进行拟合\",{\"1\":{\"131\":1}}],[\"通过采取\",{\"1\":{\"85\":1}}],[\"通过迭代算法求解\",{\"1\":{\"77\":1}}],[\"通过求解\",{\"1\":{\"48\":1}}],[\"通过\",{\"1\":{\"29\":1,\"45\":1,\"47\":1,\"70\":1,\"75\":1,\"82\":1}}],[\"通过k\",{\"1\":{\"13\":1}}],[\"通过gak\",{\"1\":{\"5\":1}}],[\"通过优化无人机的部署和动态移动来解决总用户mos最大化问题\",{\"1\":{\"5\":1}}],[\"更新一定次数后\",{\"1\":{\"142\":1}}],[\"更新策略的步骤就是选择此时\",{\"1\":{\"77\":1}}],[\"更加高效利用\",{\"1\":{\"80\":1}}],[\"更注重长远的reward\",{\"1\":{\"19\":1}}],[\"更远视\",{\"1\":{\"19\":1}}],[\"更多地是考虑多架无人机的二维部署或单架无人机在地面用户保持静止情况下的部署\",{\"1\":{\"4\":1}}],[\"显然是\",{\"1\":{\"150\":1}}],[\"显然是可以通过一个\",{\"1\":{\"66\":1}}],[\"显然我们仍然可以通过\",{\"1\":{\"110\":1}}],[\"显然我们可以将\",{\"1\":{\"104\":1}}],[\"显然ϵ=0\",{\"1\":{\"87\":1}}],[\"显然其核心关键就是在\",{\"1\":{\"77\":1}}],[\"显然在现实运行算法中是无法做到的\",{\"1\":{\"71\":1}}],[\"显然\",{\"1\":{\"19\":1,\"23\":1,\"63\":1,\"71\":1,\"78\":1,\"86\":1,\"96\":1,\"104\":1}}],[\"γa∑​π\",{\"1\":{\"45\":1}}],[\"γ\",{\"1\":{\"39\":1,\"48\":1,\"110\":1}}],[\"γ接近1\",{\"1\":{\"19\":1}}],[\"γ∈\",{\"1\":{\"19\":1}}],[\"γkn​\",{\"1\":{\"11\":1,\"13\":1}}],[\"γkn​​\",{\"1\":{\"9\":1}}],[\"γk0​σ2μlos​pmax​​\",{\"1\":{\"9\":1}}],[\"此时this\",{\"1\":{\"166\":1}}],[\"此时变量没有引用任何对象\",{\"1\":{\"161\":1}}],[\"此时可以分析每一个状态在这个策略下的概率\",{\"1\":{\"135\":1}}],[\"此时也可以是\",{\"1\":{\"124\":1}}],[\"此时就是随机策略\",{\"1\":{\"87\":1}}],[\"此时\",{\"1\":{\"46\":1,\"77\":1,\"125\":1}}],[\"此时对应的discountedrate=0+γ0+γ20+γ31+γ41+⋯=γ31−γ1​\",{\"1\":{\"19\":1}}],[\"此时该trajectory的return=0+0+0+1+1+⋯=∞\",{\"1\":{\"19\":1}}],[\"此时的mos模型定义如下\",{\"1\":{\"10\":1}}],[\"依赖于当前状态和所采取的动作\",{\"1\":{\"19\":1}}],[\"指导agent在当前状态下选择哪个动作\",{\"1\":{\"19\":1}}],[\"π2\",{\"1\":{\"70\":1}}],[\"π2​=argmaxπ​\",{\"1\":{\"70\":1}}],[\"π1​=argmaxπ​\",{\"1\":{\"70\":2}}],[\"π180​θkn​​−ζ\",{\"1\":{\"9\":1}}],[\"π0​\",{\"1\":{\"70\":1}}],[\"π0​pe​vπ0​​pi​π1​pe​vπ1​​pi​π2​pe​vπ2​​pi​\",{\"1\":{\"65\":1,\"70\":1}}],[\"πk​的\",{\"1\":{\"68\":1}}],[\"πk​\",{\"1\":{\"66\":2,\"78\":1}}],[\"πk+1​=πargmax​\",{\"1\":{\"66\":1,\"70\":2}}],[\"πk+1​=argmaxπ​\",{\"1\":{\"63\":1,\"77\":1}}],[\"πk+1​\",{\"1\":{\"63\":2,\"66\":2,\"68\":1,\"79\":2}}],[\"π∗\",{\"1\":{\"53\":2}}],[\"π\",{\"1\":{\"19\":1,\"20\":1,\"40\":1,\"41\":1,\"44\":1,\"45\":1,\"48\":1,\"53\":1,\"57\":1,\"87\":1,\"111\":1,\"112\":1,\"113\":2,\"135\":1,\"148\":2,\"152\":3,\"155\":1}}],[\"定义目标函数\",{\"1\":{\"132\":1}}],[\"定义\",{\"1\":{\"48\":1}}],[\"定义了agent与环境的交互行为\",{\"1\":{\"19\":1}}],[\"定义为ξ=\",{\"1\":{\"13\":1}}],[\"所处的包和对应的目录是一一对应的\",{\"1\":{\"173\":1}}],[\"所生成的数据\",{\"1\":{\"112\":1}}],[\"所求出的\",{\"1\":{\"66\":1}}],[\"所有被标记为静态的内容\",{\"1\":{\"171\":1}}],[\"所有可能动作的\",{\"1\":{\"48\":1}}],[\"所有状态的集合\",{\"1\":{\"19\":1}}],[\"所获得的均值\",{\"1\":{\"48\":1}}],[\"所得到的\",{\"1\":{\"40\":1}}],[\"所以说我们需要明确指定\",{\"1\":{\"173\":1}}],[\"所以说我们在静态方法中\",{\"1\":{\"170\":1}}],[\"所以说没有包这个概念\",{\"1\":{\"173\":1}}],[\"所以说静态内容一定会在第一个对象初始化之前完成加载\",{\"1\":{\"171\":1}}],[\"所以说\",{\"1\":{\"170\":1}}],[\"所以说直接就使用了\",{\"1\":{\"164\":1}}],[\"所以说使用void\",{\"1\":{\"162\":1}}],[\"所以\",{\"1\":{\"48\":1}}],[\"所以状态其实共有\",{\"1\":{\"13\":1}}],[\"所以在无人机辅助通信网络中我们需要考虑qoe模型\",{\"1\":{\"10\":1}}],[\"entity\",{\"1\":{\"173\":5,\"174\":1}}],[\"environment\",{\"1\":{\"19\":2}}],[\"e+γvπ​\",{\"1\":{\"113\":1}}],[\"eplison\",{\"0\":{\"85\":1}}],[\"episodestartingfrom\",{\"1\":{\"81\":4}}],[\"episodes\",{\"1\":{\"78\":1,\"82\":2,\"84\":2}}],[\"episode\",{\"0\":{\"81\":1},\"1\":{\"19\":2,\"78\":2,\"81\":6,\"82\":1,\"84\":1,\"86\":1,\"137\":1}}],[\"efficient\",{\"1\":{\"81\":1}}],[\"every\",{\"1\":{\"81\":2}}],[\"evaluation\",{\"1\":{\"23\":2,\"45\":1,\"47\":1,\"66\":1,\"70\":1,\"79\":1,\"82\":1}}],[\"estimation\",{\"0\":{\"99\":1,\"103\":1,\"132\":1},\"1\":{\"75\":2,\"77\":1,\"82\":1,\"92\":1}}],[\"es∼η\",{\"1\":{\"26\":1}}],[\"elementwise\",{\"1\":{\"49\":1,\"55\":1,\"63\":1,\"66\":2}}],[\"euqation\",{\"1\":{\"47\":1}}],[\"e\",{\"1\":{\"43\":2,\"44\":4,\"48\":2,\"75\":1,\"94\":1,\"98\":2,\"99\":3,\"101\":1,\"104\":2,\"110\":1,\"113\":1}}],[\"equation\",{\"0\":{\"42\":1,\"45\":1,\"46\":1,\"54\":1},\"1\":{\"37\":1,\"45\":1,\"46\":2,\"49\":2,\"52\":1,\"58\":1,\"66\":1,\"68\":2,\"70\":1,\"71\":1,\"77\":1,\"113\":3,\"115\":1,\"119\":1}}],[\"exploration\",{\"1\":{\"87\":1}}],[\"exploring\",{\"0\":{\"80\":1,\"83\":1,\"84\":1},\"1\":{\"80\":1,\"84\":2,\"85\":1,\"88\":1}}],[\"exploitation\",{\"1\":{\"87\":1}}],[\"exponentially\",{\"1\":{\"58\":1}}],[\"expected\",{\"0\":{\"118\":1},\"1\":{\"40\":1}}],[\"expection\",{\"1\":{\"40\":1,\"113\":1,\"133\":1,\"136\":1}}],[\"experience\",{\"0\":{\"10\":1},\"1\":{\"4\":1,\"80\":1,\"112\":1,\"116\":1}}],[\"existence\",{\"1\":{\"58\":1}}],[\"excellent\",{\"1\":{\"10\":1}}],[\"测试阶段\",{\"1\":{\"14\":1}}],[\"训练阶段\",{\"1\":{\"14\":1}}],[\"与策略\",{\"1\":{\"152\":2}}],[\"与\",{\"0\":{\"41\":1,\"70\":1,\"114\":1},\"1\":{\"41\":1,\"68\":1,\"79\":1,\"124\":1}}],[\"与基于q\",{\"1\":{\"14\":1}}],[\"与k\",{\"1\":{\"5\":1}}],[\"基于函数表示的策略\",{\"1\":{\"148\":1}}],[\"基于函数的\",{\"1\":{\"130\":1}}],[\"基于策略\",{\"1\":{\"135\":1}}],[\"基于给定策略下\",{\"1\":{\"134\":1}}],[\"基于表格的\",{\"1\":{\"130\":1}}],[\"基于q\",{\"1\":{\"14\":1}}],[\"基本思路\",{\"0\":{\"148\":1}}],[\"基本形式\",{\"0\":{\"55\":1}}],[\"基本概念\",{\"0\":{\"17\":1,\"19\":1}}],[\"基本设置\",{\"0\":{\"8\":1}}],[\"后面的\",{\"1\":{\"173\":1}}],[\"后\",{\"1\":{\"48\":1,\"82\":2}}],[\"后续可能是未知的\",{\"1\":{\"45\":1}}],[\"后四个方向\",{\"1\":{\"14\":1}}],[\"后退\",{\"1\":{\"13\":1}}],[\"前\",{\"1\":{\"14\":1}}],[\"前进\",{\"1\":{\"13\":1}}],[\"右\",{\"1\":{\"14\":1}}],[\"右转\",{\"1\":{\"13\":1}}],[\"www\",{\"1\":{\"173\":1}}],[\"wt+1​=wt​+αt​n1​i=1∑n​\",{\"1\":{\"142\":1}}],[\"wt​=w\",{\"1\":{\"142\":1}}],[\"wt​\",{\"1\":{\"136\":2,\"137\":2,\"138\":4,\"139\":3,\"140\":3,\"142\":10}}],[\"wargmin​j\",{\"1\":{\"101\":1}}],[\"walk\",{\"1\":{\"14\":2}}],[\"w−e\",{\"1\":{\"99\":1,\"110\":1}}],[\"w∗\",{\"1\":{\"98\":2}}],[\"w∈r\",{\"1\":{\"96\":1}}],[\"w\",{\"1\":{\"96\":3,\"97\":2,\"98\":3,\"99\":5,\"101\":5,\"104\":15,\"110\":4,\"131\":1,\"132\":1,\"133\":2,\"134\":3,\"135\":3,\"136\":6,\"141\":3,\"142\":21}}],[\"wk−1​\",{\"1\":{\"98\":1}}],[\"wk​−\",{\"1\":{\"110\":1}}],[\"wk​−xk​\",{\"1\":{\"94\":2,\"99\":1}}],[\"wk​\",{\"1\":{\"97\":6,\"98\":1,\"99\":1,\"101\":6,\"104\":12,\"110\":1,\"136\":1}}],[\"wk​+xk​\",{\"1\":{\"94\":1}}],[\"wk​=k−11​i=1∑k−1​xi​\",{\"1\":{\"94\":1}}],[\"wk+1​=wk​+αk​\",{\"1\":{\"136\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1}}],[\"wk+1​=wk​−αk​g~​\",{\"1\":{\"110\":1}}],[\"wk+1​=wk​−αk​g​\",{\"1\":{\"99\":1,\"104\":1}}],[\"wk+1​=wk​−αk​e\",{\"1\":{\"104\":1}}],[\"wk+1​=wk​−αk​▽w​j\",{\"1\":{\"136\":1}}],[\"wk+1​=wk​−αk​▽w​f\",{\"1\":{\"101\":1}}],[\"wk+1​=wk​−αk​▽w​e\",{\"1\":{\"101\":1}}],[\"wk+1​=wk​−αk​n1​i=1∑n​▽w​f\",{\"1\":{\"101\":1}}],[\"wk+1​=wk​−ak​g​\",{\"1\":{\"97\":1}}],[\"wk+1​=wk​−k1​\",{\"1\":{\"94\":1}}],[\"wk+1​=k1​i=1∑k​xi​\",{\"1\":{\"94\":1}}],[\"wk+1​​=k1​∑i=1k​xi​​=k1​\",{\"1\":{\"94\":1}}],[\"wk+1​可以由wk​推导出来\",{\"1\":{\"94\":1}}],[\"why\",{\"0\":{\"47\":1}}],[\"where\",{\"1\":{\"46\":1,\"58\":1}}],[\"when\",{\"1\":{\"19\":1}}],[\"with\",{\"0\":{\"137\":1,\"138\":1,\"139\":1,\"140\":1},\"1\":{\"10\":1,\"19\":2,\"98\":1}}],[\"wireless\",{\"1\":{\"4\":1}}],[\"如梯度上升算法\",{\"1\":{\"148\":1}}],[\"如\",{\"1\":{\"79\":1,\"82\":1,\"161\":1}}],[\"如何估计\",{\"0\":{\"78\":1}}],[\"如何将\",{\"1\":{\"76\":1}}],[\"如何在没有模型\",{\"1\":{\"75\":1}}],[\"如何确保策略\",{\"1\":{\"68\":1}}],[\"如何通过\",{\"1\":{\"68\":1}}],[\"如何处理等式右边的\",{\"0\":{\"57\":1}}],[\"如何求解\",{\"0\":{\"56\":1}}],[\"如何得到最优策略\",{\"1\":{\"53\":1}}],[\"如可以在target\",{\"1\":{\"19\":1}}],[\"如果某个类中存在静态方法或是静态变量\",{\"1\":{\"174\":1}}],[\"如果没有出现歧义\",{\"1\":{\"173\":1}}],[\"如果一个包中有多个类\",{\"1\":{\"173\":1}}],[\"如果方法内没有变量出现重名的情况\",{\"1\":{\"164\":1}}],[\"如果是多个单词\",{\"1\":{\"162\":1}}],[\"如果是引用类型\",{\"1\":{\"161\":1}}],[\"如果是boolean的话\",{\"1\":{\"161\":1}}],[\"如果是基本类型\",{\"1\":{\"161\":1}}],[\"如果我们强制让\",{\"1\":{\"124\":1}}],[\"如果\",{\"1\":{\"96\":1}}],[\"如果对于\",{\"1\":{\"96\":1}}],[\"如果ϵ=1\",{\"1\":{\"87\":1}}],[\"如果经过了\",{\"1\":{\"84\":1}}],[\"如果从其他状态出发\",{\"1\":{\"84\":1}}],[\"如果存在一个\",{\"1\":{\"84\":1}}],[\"如果该\",{\"1\":{\"81\":1}}],[\"如果γ接近0\",{\"1\":{\"19\":1}}],[\"如果agent在当前时刻t所执行的动作能够提高总mos\",{\"1\":{\"13\":1}}],[\"如p\",{\"1\":{\"19\":1}}],[\"如a\",{\"1\":{\"14\":1}}],[\"需考虑用户的移动性\",{\"1\":{\"14\":1}}],[\"需要先进行导入才可以\",{\"1\":{\"173\":1}}],[\"需要在类的最上面添加package关键字来指明当前类所处的包\",{\"1\":{\"173\":1}}],[\"需要通过关键字\",{\"1\":{\"173\":2}}],[\"需要通过采样解决\",{\"1\":{\"45\":1}}],[\"需要考虑怎样对状态进行平均\",{\"1\":{\"133\":1}}],[\"需要求解一个优化问题\",{\"1\":{\"101\":1}}],[\"需要从\",{\"1\":{\"77\":1}}],[\"需要初始化猜测的\",{\"1\":{\"70\":1}}],[\"需要初始化策略π0​\",{\"1\":{\"70\":1}}],[\"需要确定几件事\",{\"1\":{\"53\":1}}],[\"需要推导e\",{\"1\":{\"42\":1}}],[\"需要调整相应无人机的高度\",{\"1\":{\"9\":1}}],[\"需要合理选择无人机n的垂直高度hn​\",{\"1\":{\"9\":1}}],[\"∑s∈s​d\",{\"1\":{\"150\":1}}],[\"∑s∈s​dπ​\",{\"1\":{\"135\":1}}],[\"∑k=1∞​ak2​=∞\",{\"1\":{\"98\":1}}],[\"∑k=1∞​ak2​<∞\",{\"1\":{\"98\":1}}],[\"∑k=1∞​ak​=∞\",{\"1\":{\"98\":1}}],[\"∑kn​=1kn​​pkn​​\",{\"1\":{\"11\":1,\"13\":1}}],[\"∑i=1k−1​xi​+xk​\",{\"1\":{\"94\":1}}],[\"∑​pπ​\",{\"1\":{\"46\":1}}],[\"∑​p\",{\"1\":{\"44\":2,\"45\":2,\"48\":1,\"55\":1,\"63\":1,\"66\":2,\"77\":1,\"113\":1}}],[\"∑​vπ​\",{\"1\":{\"44\":2}}],[\"∑​e\",{\"1\":{\"44\":3}}],[\"∑n=1n​∣kn​∣是总用户数\",{\"1\":{\"14\":1}}],[\"则只是进行一次带入求解\",{\"1\":{\"70\":1}}],[\"则是对多个\",{\"1\":{\"41\":1}}],[\"则总动作空间的大小为7+2n∑n=1n​∣kn​∣会导致动作空间过大\",{\"1\":{\"14\":1}}],[\"则用户的关联动作数为2n∑n=1n​∣kn​∣\",{\"1\":{\"14\":1}}],[\"则无人机将获得正奖励\",{\"1\":{\"13\":1}}],[\"设\",{\"1\":{\"135\":1,\"137\":1}}],[\"设置为\",{\"1\":{\"27\":1}}],[\"设无人机总数为n\",{\"1\":{\"14\":1}}],[\"设计一种基于\",{\"1\":{\"5\":2}}],[\"对其进行修改\",{\"1\":{\"166\":1}}],[\"对象再多\",{\"1\":{\"170\":1}}],[\"对象在创建之后\",{\"1\":{\"166\":1}}],[\"对象创建成功之后\",{\"1\":{\"161\":1}}],[\"对象实例的创建\",{\"1\":{\"161\":1}}],[\"对象\",{\"1\":{\"161\":1}}],[\"对应多个文件夹\",{\"1\":{\"173\":1}}],[\"对应目标函数的真实梯度是\",{\"1\":{\"136\":1}}],[\"对应收敛性证明\",{\"1\":{\"104\":1}}],[\"对应一个向量\",{\"1\":{\"58\":1}}],[\"对应策略表示为\",{\"1\":{\"57\":1}}],[\"对应的vk+1​\",{\"1\":{\"63\":1}}],[\"对应的动作\",{\"1\":{\"57\":1}}],[\"对应的\",{\"1\":{\"42\":2,\"46\":1,\"66\":1,\"94\":1,\"116\":1}}],[\"对应的discounted\",{\"1\":{\"39\":1}}],[\"对应算法\",{\"0\":{\"28\":1}}],[\"对应\",{\"1\":{\"23\":2,\"154\":1}}],[\"对应状态中所有可选择的动作集合\",{\"1\":{\"19\":1}}],[\"对应s就是uav的部署位置\",{\"1\":{\"13\":1}}],[\"对于对象而言\",{\"1\":{\"161\":1}}],[\"对于策略\",{\"1\":{\"113\":1}}],[\"对于观测值g​\",{\"1\":{\"104\":1}}],[\"对于这个问题\",{\"1\":{\"101\":1}}],[\"对于函数\",{\"1\":{\"101\":1}}],[\"对于一个\",{\"1\":{\"81\":1}}],[\"对于贝尔曼最优公式而言\",{\"1\":{\"55\":1,\"56\":1}}],[\"对于\",{\"1\":{\"48\":1,\"70\":3,\"77\":2,\"79\":1,\"81\":1,\"88\":1,\"130\":1,\"142\":1}}],[\"对于所有状态s\",{\"1\":{\"46\":1}}],[\"对于不同的策略\",{\"1\":{\"40\":1}}],[\"对于每个\",{\"1\":{\"81\":1}}],[\"对于每个智能体\",{\"1\":{\"13\":1}}],[\"对于每一个状态\",{\"1\":{\"19\":1}}],[\"对于动作空间而言\",{\"1\":{\"14\":1}}],[\"对于用户kn​存在特定的信噪比目标γkn​​\",{\"1\":{\"9\":1}}],[\"对于用户kn​∈kn​\",{\"1\":{\"8\":1}}],[\"对于无人机的总发射功率也均匀地分配给每个用户\",{\"1\":{\"9\":1}}],[\"对于无人机n\",{\"1\":{\"8\":1,\"9\":1}}],[\"对于指定区域\",{\"1\":{\"8\":1}}],[\"值函数近似\",{\"0\":{\"130\":1}}],[\"值迭代和策略迭代\",{\"0\":{\"61\":1}}],[\"值的估计通过一个网络来进行描述\",{\"1\":{\"23\":1}}],[\"值\",{\"1\":{\"13\":1}}],[\"最好是一个域名的格式\",{\"1\":{\"173\":1}}],[\"最好的\",{\"0\":{\"27\":1}}],[\"最优的策略\",{\"1\":{\"148\":1}}],[\"最优策略\",{\"0\":{\"57\":1}}],[\"最优策略是\",{\"1\":{\"53\":1}}],[\"最优策略是否唯一\",{\"1\":{\"53\":1}}],[\"最优策略是否存在\",{\"1\":{\"53\":1}}],[\"最优策略的定义\",{\"1\":{\"53\":1}}],[\"最接近真实的vπ​\",{\"1\":{\"132\":1}}],[\"最大的\",{\"1\":{\"77\":1,\"79\":1}}],[\"最大的q\",{\"1\":{\"13\":1}}],[\"最简单的示例算法\",{\"1\":{\"76\":1}}],[\"最终会达到一个平稳的状态\",{\"1\":{\"135\":1}}],[\"最终优化的策略\",{\"1\":{\"121\":1}}],[\"最终可以证明\",{\"1\":{\"47\":1}}],[\"最终输出结果\",{\"1\":{\"13\":1}}],[\"最终输出的结果\",{\"1\":{\"13\":1}}],[\"当前类\",{\"1\":{\"174\":1}}],[\"当我们使用同一个包中的类时\",{\"1\":{\"173\":1}}],[\"当然\",{\"1\":{\"164\":1,\"166\":3,\"173\":2}}],[\"当策略是以函数的形式存在时\",{\"1\":{\"148\":1}}],[\"当策略是以表格的形式保存时\",{\"1\":{\"148\":1}}],[\"当a为静止时\",{\"1\":{\"13\":1}}],[\"当执行动作at​时\",{\"1\":{\"13\":1}}],[\"发现在该位置静止是最优的\",{\"1\":{\"13\":1}}],[\"发送端的时间\",{\"1\":{\"10\":1}}],[\"不同包下的类\",{\"1\":{\"174\":1}}],[\"不同包下的子类\",{\"1\":{\"174\":1}}],[\"不同的访问权限\",{\"1\":{\"174\":1}}],[\"不同的无人机agent的q\",{\"1\":{\"13\":1}}],[\"不同类的重名问题\",{\"1\":{\"173\":1}}],[\"不同对象的属性是分开独立存放的\",{\"1\":{\"161\":1}}],[\"不太一样\",{\"1\":{\"144\":1}}],[\"不用进行求导\",{\"1\":{\"142\":1}}],[\"不记录\",{\"1\":{\"81\":1}}],[\"不动点x∗是唯一的\",{\"1\":{\"58\":1}}],[\"不常用\",{\"1\":{\"47\":1}}],[\"不会影响所求的梯度\",{\"1\":{\"26\":1}}],[\"不能用同一个q\",{\"1\":{\"13\":1}}],[\"还可以定义一些方法来描述同一类的行为\",{\"1\":{\"162\":1}}],[\"还可以估计q\",{\"1\":{\"81\":1}}],[\"还需要求解最优策略π\",{\"1\":{\"55\":1}}],[\"还需要考虑所有用户的2d位置\",{\"1\":{\"14\":1}}],[\"还是\",{\"1\":{\"53\":1,\"75\":1,\"92\":1}}],[\"还是同一张q\",{\"1\":{\"13\":1}}],[\"还与los的概率有关\",{\"1\":{\"13\":1}}],[\"但还未复制\",{\"1\":{\"166\":1}}],[\"但还存在\",{\"1\":{\"156\":1}}],[\"但是同样需要有访问权限的情况下才可以\",{\"1\":{\"174\":1}}],[\"但是现在\",{\"1\":{\"173\":1}}],[\"但是静态方法是可以访问到静态变量的\",{\"1\":{\"170\":1}}],[\"但是我们发现\",{\"1\":{\"166\":1}}],[\"但是我们前面说了\",{\"1\":{\"161\":1}}],[\"但是仅返回类型不同\",{\"1\":{\"165\":1}}],[\"但是需要的形式参数不一样\",{\"1\":{\"165\":1}}],[\"但是当前作用域下只有对象属性的name变量\",{\"1\":{\"164\":1}}],[\"但是规则跟变量的命名差不多\",{\"1\":{\"162\":1}}],[\"但在发表\",{\"1\":{\"144\":1}}],[\"但对于\",{\"1\":{\"142\":1}}],[\"但这里还有一个难点\",{\"1\":{\"136\":1}}],[\"但实际情况可能并不是所有状态的概率都是一致的\",{\"1\":{\"134\":1}}],[\"但实际上我们往往是选择一个非常小的常数\",{\"1\":{\"98\":1}}],[\"但能否保证其精确度\",{\"1\":{\"101\":1}}],[\"但需要大量的\",{\"1\":{\"101\":1}}],[\"但由于\",{\"1\":{\"101\":1}}],[\"但目前无法保证\",{\"1\":{\"84\":1}}],[\"但存在一定的浪费\",{\"1\":{\"81\":1}}],[\"但现实场景中不太经常使用\",{\"1\":{\"76\":1}}],[\"但若考虑集群情况\",{\"1\":{\"14\":1}}],[\"但每个无人机所管理的用户不同\",{\"1\":{\"13\":1}}],[\"但即使仅考虑用户聚类\",{\"1\":{\"13\":1}}],[\"否则要创建这个类的对象\",{\"1\":{\"166\":1}}],[\"否则\",{\"1\":{\"13\":1}}],[\"奖励\",{\"1\":{\"13\":1,\"19\":1}}],[\"并不会在一开始就去加载它\",{\"1\":{\"171\":1}}],[\"并不是在构造方法之后\",{\"1\":{\"166\":1}}],[\"并不是一直进行更新\",{\"1\":{\"142\":1}}],[\"并非是所对应的对象本身\",{\"1\":{\"161\":1}}],[\"并非只是一个单一的轨迹\",{\"1\":{\"19\":1}}],[\"并且也导入了我们自己定义的string类\",{\"1\":{\"173\":1}}],[\"并且方法名称与类名相同\",{\"1\":{\"166\":2}}],[\"并且如果按照这样一个策略\",{\"1\":{\"86\":1}}],[\"并且与状态\",{\"1\":{\"48\":1}}],[\"并获得奖励rt​的这一过程可以用条件转移概率p\",{\"1\":{\"13\":1}}],[\"并与传统的基于遗传的学习算法进行对比\",{\"1\":{\"5\":1}}],[\"上行\",{\"1\":{\"13\":1}}],[\"左转\",{\"1\":{\"13\":1}}],[\"共两步\",{\"1\":{\"132\":1}}],[\"共分为\",{\"1\":{\"63\":1}}],[\"共考虑7个方向\",{\"1\":{\"13\":1}}],[\"共划分5个等级\",{\"1\":{\"10\":1}}],[\"按照所给定策略j来执行一个动作at​∈a从而获得奖励rt​以及下一个状态st+1​\",{\"1\":{\"13\":1}}],[\"个样本\",{\"1\":{\"142\":1}}],[\"个人推导\",{\"1\":{\"44\":1}}],[\"个人感觉是这个\",{\"1\":{\"13\":1}}],[\"个人理解\",{\"1\":{\"13\":1,\"19\":1}}],[\"个\",{\"1\":{\"13\":1}}],[\"×\",{\"1\":{\"13\":2}}],[\"0<c1​≤▽w​g\",{\"1\":{\"98\":1}}],[\"0\",{\"1\":{\"13\":17,\"14\":1,\"19\":1,\"47\":1,\"87\":1,\"104\":1,\"166\":1}}],[\"0≤t≤ts​\",{\"1\":{\"8\":2,\"11\":2}}],[\"状态s时可以得到的平均reward\",{\"1\":{\"153\":1}}],[\"状态转换\",{\"1\":{\"19\":1}}],[\"状态转换模型\",{\"1\":{\"13\":1}}],[\"状态除了要考虑无人机的3d位置外\",{\"1\":{\"14\":1}}],[\"状态空间\",{\"1\":{\"13\":1,\"19\":1}}],[\"状态\",{\"1\":{\"13\":1,\"19\":1,\"135\":1,\"142\":1}}],[\"智能体\",{\"1\":{\"13\":1}}],[\"来完成\",{\"1\":{\"166\":1}}],[\"来明确表示当前类的示例对象本身\",{\"1\":{\"164\":1}}],[\"来定义最优的策略\",{\"1\":{\"148\":1}}],[\"来更新参数值\",{\"1\":{\"142\":1}}],[\"来近似\",{\"1\":{\"137\":1,\"138\":1}}],[\"来进行求解\",{\"1\":{\"77\":1}}],[\"来进行迭代\",{\"1\":{\"13\":1}}],[\"来提升当前策略\",{\"1\":{\"66\":1}}],[\"来保证这个梯度的方差最小即可\",{\"1\":{\"26\":1}}],[\"来表示\",{\"1\":{\"13\":1}}],[\"来最大化mos总和\",{\"1\":{\"13\":1}}],[\"来考虑无人机的机动性\",{\"1\":{\"4\":1}}],[\"划分完毕\",{\"1\":{\"13\":1}}],[\"再通过这些经验来不断改进更新另一个策略\",{\"1\":{\"123\":1}}],[\"再次出现\",{\"1\":{\"81\":1}}],[\"再将\",{\"1\":{\"77\":1}}],[\"再将用户划分给距离最近的无人机\",{\"1\":{\"13\":1}}],[\"再求平均值\",{\"1\":{\"41\":1}}],[\"再根据欧几里得距离重新划分\",{\"1\":{\"13\":1}}],[\"再找到新的簇的各中心\",{\"1\":{\"13\":1}}],[\"描述如下\",{\"1\":{\"13\":1}}],[\"每次都是从对应的状态\",{\"1\":{\"84\":1}}],[\"每次迭代都会使得策略进行提升\",{\"1\":{\"68\":1}}],[\"每次无人机会根据当前状态st​∈s\",{\"1\":{\"13\":1}}],[\"每个对象都有一个自己的空间\",{\"1\":{\"161\":1}}],[\"每个用户的移动方向均匀分布在左\",{\"1\":{\"14\":1}}],[\"每个用户都需要判断是否与每个无人机关联\",{\"1\":{\"14\":1}}],[\"每个用户只能属于一个集群\",{\"1\":{\"8\":1}}],[\"每个集群中无人机的最优位置也会发生变化\",{\"1\":{\"14\":1}}],[\"每架无人机的带宽和发射功率都均匀分配给每个用户\",{\"1\":{\"13\":1}}],[\"将main类放到com\",{\"1\":{\"173\":1}}],[\"将基于表格表示的策略\",{\"1\":{\"148\":1}}],[\"将该网络的\",{\"1\":{\"142\":1}}],[\"将\",{\"1\":{\"58\":1,\"85\":1,\"142\":2}}],[\"将所有状态的\",{\"1\":{\"46\":1}}],[\"将处于\",{\"1\":{\"29\":1}}],[\"将对于\",{\"1\":{\"23\":1}}],[\"将对应的轨迹所获得的所有reward的总和\",{\"1\":{\"19\":1}}],[\"将无人机部署在每个中心内\",{\"1\":{\"13\":1}}],[\"将上述优化问题简化\",{\"1\":{\"13\":1}}],[\"将其平均分配给其∣kn​∣个关联用户\",{\"1\":{\"9\":1}}],[\"解决方案\",{\"0\":{\"12\":1}}],[\"水平位置和高度\",{\"1\":{\"11\":1}}],[\"数量和位置\",{\"1\":{\"11\":1}}],[\"数据包从发送端\",{\"1\":{\"10\":1}}],[\"因而也被称为实例\",{\"1\":{\"161\":1}}],[\"因为都直接在一个缺省的包中\",{\"1\":{\"173\":1}}],[\"因为this关键字代表的是当前的对象本身\",{\"1\":{\"170\":1}}],[\"因为静态方法属于类的\",{\"1\":{\"170\":1}}],[\"因为我们需要计算的是\",{\"1\":{\"155\":1}}],[\"因为不需要\",{\"1\":{\"125\":1}}],[\"因为在\",{\"1\":{\"92\":1}}],[\"因为在不考虑用户自由穿梭集群的情况\",{\"1\":{\"14\":1}}],[\"因为最终策略更新的核心仍然是\",{\"1\":{\"79\":1}}],[\"因为\",{\"1\":{\"77\":1,\"104\":1,\"124\":1}}],[\"因为无论是\",{\"1\":{\"75\":1}}],[\"因为其满足该理论\",{\"1\":{\"58\":1}}],[\"因为q\",{\"1\":{\"13\":1}}],[\"因为目标函数对于无人机的3d坐标是非凸的\",{\"1\":{\"11\":1}}],[\"因此该指标可以描述为\",{\"1\":{\"150\":1}}],[\"因此左侧那个类似\",{\"1\":{\"142\":1}}],[\"因此对应的损失函数的梯度可以修改为\",{\"1\":{\"142\":1}}],[\"因此对应算法为\",{\"1\":{\"138\":1}}],[\"因此对于用户kn​的在时刻t的传输速率rkn​​\",{\"1\":{\"9\":1}}],[\"因此需要用近似算法来进行替代\",{\"1\":{\"136\":1}}],[\"因此可以考虑\",{\"1\":{\"136\":1}}],[\"因此采用这种\",{\"1\":{\"134\":1}}],[\"因此这种情况下的\",{\"1\":{\"134\":1}}],[\"因此不需要进行\",{\"1\":{\"120\":1}}],[\"因此我们不需要手动指定\",{\"1\":{\"173\":1}}],[\"因此我们需要保证对于所有的\",{\"1\":{\"155\":1}}],[\"因此我们用\",{\"1\":{\"137\":1}}],[\"因此我们可以通过\",{\"1\":{\"99\":1}}],[\"因此我们将优化问题简化为区域分割问题\",{\"1\":{\"13\":1}}],[\"因此是一个确定的贪心策略\",{\"1\":{\"88\":1}}],[\"因此是2n\",{\"1\":{\"14\":1}}],[\"因此随着用户位置的变化\",{\"1\":{\"14\":1}}],[\"因此mos不仅与欧氏距离有关\",{\"1\":{\"13\":1}}],[\"因此moskn​​delay\",{\"1\":{\"10\":1}}],[\"因此gak\",{\"1\":{\"13\":1}}],[\"因此\",{\"1\":{\"9\":2,\"10\":1,\"26\":1,\"42\":1,\"48\":1,\"58\":1,\"71\":1,\"94\":1,\"101\":1,\"104\":2,\"110\":1,\"113\":1,\"124\":1}}],[\"∀\",{\"1\":{\"116\":1,\"120\":1}}],[\"∀s\",{\"1\":{\"115\":1,\"119\":1}}],[\"∀s=st​\",{\"1\":{\"112\":1}}],[\"∀s∈s​\",{\"1\":{\"55\":1}}],[\"∀s∈s=πmax​a∑​π\",{\"1\":{\"55\":1}}],[\"∀s∈s\",{\"1\":{\"45\":1,\"58\":1}}],[\"∀kn​\",{\"1\":{\"11\":3,\"13\":3}}],[\"∀t\",{\"1\":{\"11\":4,\"13\":4}}],[\"∀n\",{\"1\":{\"11\":2,\"13\":2}}],[\"具体而言\",{\"1\":{\"162\":1}}],[\"具体推导过程\",{\"1\":{\"155\":1}}],[\"具体解决如下\",{\"1\":{\"97\":1}}],[\"具体求解方法\",{\"1\":{\"79\":1}}],[\"具体算法\",{\"0\":{\"79\":1}}],[\"具体步骤\",{\"0\":{\"63\":1}}],[\"具体分两步\",{\"1\":{\"56\":1}}],[\"具体代码\",{\"1\":{\"13\":1}}],[\"具体表述如下\",{\"1\":{\"11\":1}}],[\"具体如下\",{\"1\":{\"10\":1,\"142\":1}}],[\"从这章开始时基于\",{\"1\":{\"147\":1}}],[\"从\",{\"1\":{\"104\":1}}],[\"从给定的\",{\"1\":{\"82\":1}}],[\"从状态\",{\"1\":{\"78\":1,\"137\":1}}],[\"从状态st​到st+1​\",{\"1\":{\"13\":1}}],[\"从指定的\",{\"1\":{\"78\":1}}],[\"从此可以发现\",{\"1\":{\"77\":1}}],[\"从而可以进行近似求解\",{\"1\":{\"155\":1}}],[\"从而可以在\",{\"1\":{\"115\":1}}],[\"从而方便计算\",{\"1\":{\"142\":1}}],[\"从而使得算法可行\",{\"1\":{\"136\":1}}],[\"从而生成经验数据的策略\",{\"1\":{\"121\":1}}],[\"从而引入\",{\"1\":{\"105\":1}}],[\"从而转换为一个\",{\"1\":{\"104\":1}}],[\"从而进行多次利用\",{\"1\":{\"81\":1}}],[\"从而选择每个状态下最大的\",{\"1\":{\"77\":1}}],[\"从而减小方差\",{\"1\":{\"25\":1}}],[\"从而最大化所有用户的总mos值\",{\"1\":{\"11\":1}}],[\"从无人机n到用户kn​的信道功率增益\",{\"1\":{\"9\":1}}],[\"本文目的是优化无人机在每个时隙的位置\",{\"1\":{\"11\":1}}],[\"是不允许的\",{\"1\":{\"165\":1}}],[\"是不同的\",{\"1\":{\"40\":1,\"123\":1}}],[\"是为了完成某件事情而存在的\",{\"1\":{\"162\":1}}],[\"是类的一个具体化个体\",{\"1\":{\"161\":1}}],[\"是某一类事物实际存在的每个个体\",{\"1\":{\"161\":1}}],[\"是抽象的\",{\"1\":{\"161\":1}}],[\"是对一类事物的描述\",{\"1\":{\"161\":1}}],[\"是未知的\",{\"1\":{\"156\":1}}],[\"是各个\",{\"1\":{\"150\":1}}],[\"是我们需要进行优化的\",{\"1\":{\"148\":1}}],[\"是我们不断进行更新的策略\",{\"1\":{\"121\":1}}],[\"是直接建立一个基于策略的目标函数来进行梯度上升的优化\",{\"1\":{\"147\":1}}],[\"是用来与环境进行交互\",{\"1\":{\"121\":1}}],[\"是qπ​\",{\"1\":{\"116\":1}}],[\"是关于\",{\"1\":{\"112\":1}}],[\"是常数\",{\"1\":{\"110\":1}}],[\"是否成立\",{\"1\":{\"104\":1}}],[\"是否是收敛的\",{\"0\":{\"72\":1}}],[\"是需要被优化的参数\",{\"1\":{\"101\":1}}],[\"是第\",{\"1\":{\"97\":2}}],[\"是最优的\",{\"1\":{\"84\":1}}],[\"是最大报文长度\",{\"1\":{\"10\":1}}],[\"是针对\",{\"1\":{\"80\":1}}],[\"是针对一条trajectory所求的\",{\"1\":{\"41\":1}}],[\"是优于\",{\"1\":{\"68\":1}}],[\"是已知的\",{\"1\":{\"63\":1}}],[\"是依赖于策略π的\",{\"1\":{\"48\":1}}],[\"是由环境决定的\",{\"1\":{\"45\":1}}],[\"是一个期望值\",{\"1\":{\"101\":1}}],[\"是一个随机变量\",{\"1\":{\"101\":1}}],[\"是一个\",{\"1\":{\"97\":1}}],[\"是一个黑盒\",{\"1\":{\"97\":1}}],[\"是一个必要条件\",{\"1\":{\"84\":1}}],[\"是一个contraction\",{\"1\":{\"58\":1}}],[\"是一个有关状态s的函数\",{\"1\":{\"40\":1}}],[\"是一致的\",{\"1\":{\"41\":1,\"122\":1}}],[\"是基于一个给定策略\",{\"1\":{\"40\":1}}],[\"是\",{\"1\":{\"40\":1,\"49\":1,\"86\":1,\"113\":1,\"123\":2,\"124\":1,\"150\":1}}],[\"是在基于\",{\"1\":{\"23\":1}}],[\"是根据q\",{\"1\":{\"13\":1}}],[\"是网页大小\",{\"1\":{\"10\":1}}],[\"是与传输速率有关的延迟时间\",{\"1\":{\"10\":1}}],[\"接收端\",{\"1\":{\"10\":1}}],[\"6746\",{\"1\":{\"10\":1}}],[\"取值范围从1−4\",{\"1\":{\"10\":1}}],[\"rˉπ​\",{\"1\":{\"155\":1}}],[\"rˉπ​​\",{\"1\":{\"154\":1}}],[\"rˉπ​=s∈s∑​dπ​\",{\"1\":{\"153\":1}}],[\"run\",{\"1\":{\"135\":1}}],[\"r1​\",{\"1\":{\"112\":1}}],[\"rk​+γv\",{\"1\":{\"110\":1}}],[\"rkn​​mss​\",{\"1\":{\"10\":1}}],[\"rkn​​\",{\"1\":{\"10\":3}}],[\"r→r\",{\"1\":{\"96\":1}}],[\"rm\",{\"0\":{\"95\":1},\"1\":{\"97\":1,\"99\":1,\"104\":3,\"110\":2}}],[\"robbins\",{\"0\":{\"95\":1},\"1\":{\"98\":2}}],[\"root\",{\"1\":{\"92\":1,\"98\":1,\"104\":1,\"110\":1}}],[\"r+s\",{\"1\":{\"66\":2}}],[\"r+γa∈a\",{\"1\":{\"141\":1,\"142\":3}}],[\"r+γqπ​\",{\"1\":{\"115\":1}}],[\"r+γg∣s=s\",{\"1\":{\"113\":1}}],[\"r+γv\",{\"1\":{\"110\":6}}],[\"r+γ∑s\",{\"1\":{\"48\":1}}],[\"r+γs\",{\"1\":{\"45\":1,\"48\":1,\"55\":1,\"63\":1}}],[\"rπ​+γpπ​v1​\",{\"1\":{\"70\":1}}],[\"rπ​+γpπ​vπ1​​\",{\"1\":{\"70\":1}}],[\"rπ​+γpπ​vπ0​​\",{\"1\":{\"70\":2}}],[\"rπ​+γpπ​vπk​​\",{\"1\":{\"66\":1,\"70\":1,\"77\":1}}],[\"rπ​+γpπ​vk​\",{\"1\":{\"62\":1,\"63\":1,\"70\":1}}],[\"rπ​+γpπ​v\",{\"1\":{\"55\":1,\"58\":1,\"61\":1}}],[\"rπ​\",{\"1\":{\"46\":2,\"153\":4,\"154\":1}}],[\"rπ​=\",{\"1\":{\"46\":1}}],[\"r​​+mean\",{\"1\":{\"45\":1}}],[\"r​\",{\"1\":{\"43\":1}}],[\"r∑​p\",{\"1\":{\"43\":1,\"45\":2,\"48\":1,\"55\":1,\"63\":1,\"66\":2}}],[\"rl9\",{\"0\":{\"147\":1}}],[\"rl8\",{\"0\":{\"130\":1}}],[\"rl7\",{\"0\":{\"109\":1}}],[\"rl\",{\"1\":{\"92\":1}}],[\"rl6\",{\"0\":{\"92\":1}}],[\"rl5\",{\"0\":{\"75\":1}}],[\"rl4\",{\"0\":{\"61\":1}}],[\"rl3\",{\"0\":{\"52\":1}}],[\"rl2\",{\"0\":{\"36\":1}}],[\"rl10\",{\"0\":{\"23\":1}}],[\"rl1\",{\"0\":{\"17\":1}}],[\"r∣s\",{\"1\":{\"20\":1,\"41\":1,\"43\":1,\"45\":3,\"48\":2,\"55\":1,\"63\":2,\"66\":2,\"75\":1,\"77\":3,\"153\":2}}],[\"r\",{\"1\":{\"20\":3,\"110\":3,\"141\":1,\"142\":1,\"153\":3}}],[\"rate\",{\"1\":{\"19\":1,\"39\":1,\"58\":1}}],[\"random\",{\"1\":{\"14\":2,\"42\":1}}],[\"r=1∣s1​\",{\"1\":{\"19\":1}}],[\"reinforce\",{\"0\":{\"156\":1,\"157\":1},\"1\":{\"156\":1}}],[\"reinforcement\",{\"0\":{\"3\":1}}],[\"replay\",{\"0\":{\"143\":1},\"1\":{\"142\":1}}],[\"representation\",{\"1\":{\"130\":2}}],[\"refers\",{\"1\":{\"92\":1}}],[\"resulting\",{\"1\":{\"19\":1}}],[\"respect\",{\"1\":{\"19\":1}}],[\"returns\",{\"1\":{\"41\":1}}],[\"return为\",{\"1\":{\"39\":1}}],[\"return的描述\",{\"1\":{\"39\":1}}],[\"return越短视\",{\"1\":{\"19\":1}}],[\"return\",{\"0\":{\"41\":1,\"153\":1},\"1\":{\"19\":2,\"39\":1,\"41\":3,\"42\":1,\"48\":2,\"78\":1,\"82\":1,\"113\":1,\"137\":1,\"164\":1,\"165\":2}}],[\"rewards\",{\"0\":{\"43\":1,\"44\":1},\"1\":{\"20\":1,\"45\":2,\"154\":1}}],[\"reward\",{\"1\":{\"13\":1,\"19\":2,\"20\":2,\"115\":1,\"154\":1}}],[\"rt+2​\",{\"1\":{\"154\":1}}],[\"rt+2​+γrt+3​+\",{\"1\":{\"42\":1}}],[\"rt+1​+rt+2​+⋯+rt+n​∣st​=s0​\",{\"1\":{\"154\":1}}],[\"rt+1​+γq^​\",{\"1\":{\"139\":1}}],[\"rt+1​+γqt​\",{\"1\":{\"116\":1}}],[\"rt+1​+γv^\",{\"1\":{\"138\":2}}],[\"rt+1​+γvt​\",{\"1\":{\"112\":2}}],[\"rt+1​+γa∈a\",{\"1\":{\"140\":1}}],[\"rt+1​+γa∈amax​qt​\",{\"1\":{\"120\":1}}],[\"rt+1​+γamax​q\",{\"1\":{\"119\":1}}],[\"rt+1​+γgt+1​∣st​=s\",{\"1\":{\"42\":1}}],[\"rt+1​\",{\"1\":{\"112\":2,\"116\":1,\"154\":1}}],[\"rt+1​∣st​=s\",{\"1\":{\"42\":2,\"43\":2,\"45\":1}}],[\"rt+1​∣at+1​\",{\"1\":{\"20\":1}}],[\"rt​∣st​\",{\"1\":{\"13\":1}}],[\"rtt\",{\"1\":{\"10\":1}}],[\"5\",{\"0\":{\"49\":1,\"127\":1,\"141\":1},\"1\":{\"10\":3,\"70\":1}}],[\"404\",{\"1\":{\"184\":1}}],[\"4\",{\"0\":{\"33\":1,\"46\":1,\"48\":1,\"84\":1,\"99\":1,\"106\":1,\"119\":1,\"120\":1,\"121\":1,\"124\":1,\"140\":1,\"156\":1},\"1\":{\"10\":1,\"70\":1}}],[\"根据这个式子我们就可以通过\",{\"1\":{\"155\":1}}],[\"根据梯度下降的公式\",{\"1\":{\"142\":1}}],[\"根据给定的策略\",{\"1\":{\"112\":1}}],[\"根据策略\",{\"1\":{\"78\":1}}],[\"根据策略π\",{\"1\":{\"48\":1}}],[\"根据对应的\",{\"1\":{\"66\":1}}],[\"根据\",{\"1\":{\"53\":2,\"62\":1,\"63\":1,\"66\":1,\"68\":1,\"113\":1}}],[\"根据一个\",{\"1\":{\"42\":1}}],[\"根据所给定的用户划分情况\",{\"1\":{\"13\":1}}],[\"根据遗传算法找到cn​个最优个体作为簇的中心\",{\"1\":{\"13\":1}}],[\"根据n个用户\",{\"1\":{\"13\":1}}],[\"根据mos数值\",{\"1\":{\"10\":1}}],[\"根据香农定理\",{\"1\":{\"9\":1}}],[\"可见性\",{\"1\":{\"174\":1}}],[\"可写可不写\",{\"1\":{\"173\":1}}],[\"可能会创建各种各样的类\",{\"1\":{\"173\":1}}],[\"可知\",{\"1\":{\"9\":1}}],[\"可以相同\",{\"1\":{\"165\":1}}],[\"可以是\",{\"1\":{\"155\":1}}],[\"可以是标量\",{\"1\":{\"101\":1}}],[\"可以根据对各个状态的重要程度进行选择\",{\"1\":{\"152\":1}}],[\"可以最大化一个确定的常数指标\",{\"1\":{\"148\":1}}],[\"可以最开始均初始化为\",{\"1\":{\"47\":1}}],[\"可以描述为\",{\"1\":{\"148\":1}}],[\"可以写成\",{\"1\":{\"134\":1}}],[\"可以提高存储效率\",{\"1\":{\"131\":1}}],[\"可以参考\",{\"1\":{\"104\":1}}],[\"可以平衡\",{\"1\":{\"87\":1}}],[\"可以拆分为多个\",{\"1\":{\"81\":1}}],[\"可以通过\",{\"1\":{\"66\":1}}],[\"可以通过contraction\",{\"1\":{\"58\":1}}],[\"可以通过设置将episodic\",{\"1\":{\"19\":1}}],[\"可以互相转化\",{\"1\":{\"48\":1}}],[\"可以得到一个序列v0​\",{\"1\":{\"47\":1}}],[\"可以求解\",{\"1\":{\"45\":1}}],[\"可以用来衡量一个状态的价值\",{\"1\":{\"40\":1}}],[\"可以粗步衡量一个策略的好坏\",{\"1\":{\"19\":1}}],[\"可以有限\",{\"1\":{\"19\":1}}],[\"可以忽略\",{\"1\":{\"10\":1}}],[\"可以表示为\",{\"1\":{\"9\":1,\"135\":1}}],[\"可以减轻无人机对用户接收到的干扰\",{\"1\":{\"9\":1}}],[\"≤c2​\",{\"1\":{\"98\":1}}],[\"≤pmax​\",{\"1\":{\"11\":1,\"13\":1}}],[\"≤hmax​\",{\"1\":{\"11\":1,\"13\":1}}],[\"≤hn​\",{\"1\":{\"9\":1}}],[\"≤\",{\"1\":{\"9\":1}}],[\"ζ2​是系数\",{\"1\":{\"10\":1}}],[\"ζ1​\",{\"1\":{\"10\":1}}],[\"ζ+em\",{\"1\":{\"9\":1}}],[\"ζ是由环境决定的常数\",{\"1\":{\"9\":1}}],[\"18\",{\"1\":{\"162\":1,\"166\":1}}],[\"180π​\",{\"1\":{\"9\":1}}],[\"10\",{\"1\":{\"161\":1,\"166\":1}}],[\"10​a=ak∗​\",{\"1\":{\"63\":1,\"66\":1}}],[\"10​a=a∗a=a∗​\",{\"1\":{\"57\":1}}],[\"1−∣a\",{\"1\":{\"87\":1}}],[\"1234\",{\"1\":{\"33\":1}}],[\"120和4\",{\"1\":{\"10\":1}}],[\"1\",{\"0\":{\"19\":1,\"24\":1,\"26\":1,\"30\":1,\"38\":1,\"39\":2,\"40\":1,\"41\":1,\"43\":1,\"53\":1,\"55\":1,\"57\":1,\"62\":1,\"63\":2,\"64\":1,\"66\":1,\"70\":1,\"76\":1,\"77\":2,\"78\":1,\"79\":1,\"81\":1,\"86\":1,\"93\":1,\"94\":2,\"96\":1,\"101\":1,\"110\":1,\"112\":1,\"116\":1,\"120\":1,\"131\":1,\"133\":1,\"148\":1,\"150\":1},\"1\":{\"10\":1,\"13\":7,\"14\":1,\"19\":1,\"66\":1,\"70\":1,\"79\":2,\"87\":1,\"98\":2,\"101\":1,\"112\":1,\"116\":1}}],[\"1~2\",{\"1\":{\"10\":1}}],[\"1+σ2pkn​​gkn​​\",{\"1\":{\"9\":1}}],[\"1+ns​\",{\"1\":{\"9\":1}}],[\"表示导入这个包中全部的类\",{\"1\":{\"173\":1}}],[\"表示类具有的属性\",{\"1\":{\"161\":1}}],[\"表示是\",{\"1\":{\"135\":1}}],[\"表示是一种长时间的交互行为\",{\"1\":{\"135\":1}}],[\"表示是最佳部署位置\",{\"1\":{\"13\":1}}],[\"表示一个函数\",{\"1\":{\"110\":1}}],[\"表示一个给定的策略\",{\"1\":{\"45\":1}}],[\"表示对于对应\",{\"1\":{\"84\":1}}],[\"表示对于每一个\",{\"1\":{\"84\":1}}],[\"表示状态转移矩阵\",{\"1\":{\"46\":1}}],[\"表示为\",{\"1\":{\"40\":1,\"152\":3}}],[\"表示为rkn​​\",{\"1\":{\"9\":1}}],[\"表示在单步情况下\",{\"1\":{\"153\":1}}],[\"表示在策略π下\",{\"1\":{\"153\":1}}],[\"表示在一个\",{\"1\":{\"137\":1}}],[\"表示在各状态执行各动作的概率\",{\"1\":{\"20\":1}}],[\"表示在状态s下采取动作a\",{\"1\":{\"20\":2}}],[\"表示\",{\"1\":{\"10\":1,\"135\":1,\"142\":1,\"155\":1}}],[\"表示round\",{\"1\":{\"10\":1}}],[\"表示无人机与用户之间的仰角\",{\"1\":{\"9\":1}}],[\"且需要被求解出来\",{\"1\":{\"96\":1}}],[\"且最终会收敛到最优策略v∗\",{\"1\":{\"68\":1}}],[\"且\",{\"1\":{\"53\":1,\"87\":1,\"98\":2,\"135\":1,\"144\":1}}],[\"且ζ1​+ζ2​=1\",{\"1\":{\"10\":1}}],[\"且传输率永远都不可能超过信道容量c\",{\"1\":{\"9\":1}}],[\"且无人机向关联用户的发射功率是恒定的\",{\"1\":{\"9\":1}}],[\"信道容量c=b∗log\",{\"1\":{\"9\":1}}],[\"信号模型\",{\"0\":{\"9\":1}}],[\"由无人机的位置和它们在最后时隙采取的动作决定\",{\"1\":{\"14\":1}}],[\"由用户的初始位置和运动模型决定\",{\"1\":{\"14\":1}}],[\"由于默认导入了系统自带的string类\",{\"1\":{\"173\":1}}],[\"由于πk+1​是\",{\"1\":{\"63\":1}}],[\"由于\",{\"1\":{\"48\":1,\"63\":1}}],[\"由于用户在每个时隙都处于漫游状态\",{\"1\":{\"14\":1}}],[\"由于gak\",{\"1\":{\"13\":1}}],[\"由于特定用户的mos与该用户与无人机之间的距离有关\",{\"1\":{\"13\":1}}],[\"由于不同用户对于传输速率的需求是不同的\",{\"1\":{\"10\":1}}],[\"由于不同集群的频谱不同\",{\"1\":{\"9\":1}}],[\"由此进行迭代\",{\"1\":{\"66\":1}}],[\"由此可以根据contraction\",{\"1\":{\"58\":1}}],[\"由此可以推导出一个多步的trajectory\",{\"1\":{\"39\":1}}],[\"由此\",{\"1\":{\"9\":1}}],[\"同一个包下的类\",{\"1\":{\"174\":1}}],[\"同一无人机通过fdma同时为同一集群中的多个用户提供服务\",{\"1\":{\"8\":1}}],[\"同理\",{\"1\":{\"19\":1}}],[\"同样的\",{\"1\":{\"170\":2}}],[\"同样地\",{\"1\":{\"48\":1}}],[\"同样可以用条件概率的形式进行描述\",{\"1\":{\"19\":1}}],[\"同样\",{\"1\":{\"9\":1,\"99\":1}}],[\"μnlos​\",{\"1\":{\"9\":1}}],[\"μnlos​是表示los和nlos链路的衰减因子\",{\"1\":{\"9\":1}}],[\"μlos​−μnlos​\",{\"1\":{\"9\":1}}],[\"μlos​\",{\"1\":{\"9\":1}}],[\"常数\",{\"1\":{\"9\":1}}],[\"α是表示路径损耗指数\",{\"1\":{\"9\":1}}],[\"g∣s=s\",{\"1\":{\"113\":1}}],[\"g~​\",{\"1\":{\"104\":1,\"110\":1}}],[\"gd\",{\"1\":{\"101\":1,\"104\":1}}],[\"gradient\",{\"0\":{\"100\":1,\"147\":1},\"1\":{\"101\":2,\"148\":1}}],[\"greedy\",{\"0\":{\"85\":1,\"87\":1,\"88\":1},\"1\":{\"53\":1,\"63\":2,\"66\":1,\"70\":2,\"79\":1,\"87\":3}}],[\"g​\",{\"1\":{\"97\":2,\"99\":2}}],[\"gpi\",{\"1\":{\"82\":1}}],[\"generalized\",{\"1\":{\"82\":1}}],[\"getname\",{\"1\":{\"164\":1}}],[\"get\",{\"1\":{\"48\":2}}],[\"g\",{\"1\":{\"78\":3,\"96\":4,\"97\":2,\"98\":1,\"99\":2,\"104\":1,\"110\":2,\"113\":1}}],[\"gt+1​∣st+1​=s\",{\"1\":{\"44\":2}}],[\"gt+1​∣st​=s\",{\"1\":{\"42\":2,\"44\":5,\"45\":1}}],[\"gt​−v^\",{\"1\":{\"137\":1}}],[\"gt​​=rt+1​+γrt+2​+γ2rt+3​+\",{\"1\":{\"42\":1}}],[\"gt​∣st​=s\",{\"1\":{\"40\":1,\"42\":1,\"48\":3,\"49\":2,\"75\":2,\"77\":1,\"78\":1}}],[\"gt​\",{\"1\":{\"40\":2,\"42\":1,\"78\":1,\"137\":2}}],[\"gt​也是一个随机变量\",{\"1\":{\"39\":1}}],[\"gt​=rt+1​+γrt+2​+γ2rt+3​+\",{\"1\":{\"39\":1}}],[\"gt​=e\",{\"1\":{\"13\":1}}],[\"good\",{\"1\":{\"10\":1}}],[\"gkn​​\",{\"1\":{\"9\":1}}],[\"gain\",{\"1\":{\"9\":1}}],[\"certain\",{\"1\":{\"148\":1}}],[\"class\",{\"1\":{\"92\":1,\"161\":1,\"162\":1,\"166\":4,\"170\":1,\"171\":2,\"173\":3,\"174\":2}}],[\"closed\",{\"1\":{\"47\":1}}],[\"com\",{\"1\":{\"173\":8,\"174\":1}}],[\"comments\",{\"1\":{\"70\":1}}],[\"code\",{\"0\":{\"190\":1},\"2\":{\"167\":1,\"175\":1,\"182\":1}}],[\"coefficient\",{\"1\":{\"97\":1}}],[\"core\",{\"1\":{\"52\":1}}],[\"converges\",{\"1\":{\"98\":1}}],[\"convergence\",{\"1\":{\"58\":1}}],[\"convex问题\",{\"1\":{\"11\":1}}],[\"consider\",{\"1\":{\"58\":1}}],[\"considered\",{\"1\":{\"4\":1}}],[\"contractive\",{\"1\":{\"58\":1}}],[\"contraction\",{\"1\":{\"53\":2,\"58\":2,\"62\":1}}],[\"continuing\",{\"1\":{\"19\":1}}],[\"concepts\",{\"1\":{\"52\":1}}],[\"carlo\",{\"0\":{\"75\":1,\"137\":1},\"1\":{\"75\":1,\"156\":1}}],[\"can\",{\"1\":{\"41\":1,\"48\":2}}],[\"called\",{\"1\":{\"19\":1,\"81\":1,\"86\":1}}],[\"critic\",{\"0\":{\"23\":1,\"24\":1,\"25\":1,\"29\":1,\"33\":1},\"1\":{\"23\":2}}],[\"choose\",{\"1\":{\"20\":1}}],[\"chapter\",{\"1\":{\"62\":1,\"68\":1}}],[\"chain\",{\"1\":{\"19\":1}}],[\"channel\",{\"1\":{\"9\":1}}],[\"cmax​\",{\"1\":{\"14\":1}}],[\"c\",{\"1\":{\"11\":1,\"13\":1}}],[\"cycles\",{\"1\":{\"10\":1}}],[\"c1​和c2​是通过分析web浏览应用程序的实验结果确定的常数\",{\"1\":{\"10\":1}}],[\"c是光速\",{\"1\":{\"9\":1}}],[\"c4πfc​​\",{\"1\":{\"9\":1}}],[\"为一个函数方程\",{\"1\":{\"96\":1}}],[\"为状态\",{\"1\":{\"87\":1}}],[\"为什么不去求\",{\"1\":{\"79\":1}}],[\"为什么考虑\",{\"1\":{\"75\":1}}],[\"为什么这个迭代算法最终可以找到最优策略\",{\"1\":{\"68\":1}}],[\"为discounted\",{\"1\":{\"39\":1}}],[\"为t时刻的mos评分\",{\"1\":{\"10\":1}}],[\"为了支持小数加法\",{\"1\":{\"165\":1}}],[\"为了让右边取到最大值的情况\",{\"1\":{\"57\":1}}],[\"为了进行\",{\"1\":{\"47\":1}}],[\"为了应对具有无限步的trajectory的return=∞的情况\",{\"1\":{\"19\":1}}],[\"为了保证所有用户都能连接到网络\",{\"1\":{\"9\":1}}],[\"为了满足不同用户传输速率要求\",{\"1\":{\"9\":1}}],[\"为了在los信道概率和路径损耗之间取得平衡\",{\"1\":{\"9\":1}}],[\"为\",{\"1\":{\"9\":1,\"42\":2,\"46\":1,\"78\":1,\"148\":1}}],[\"为指标\",{\"1\":{\"5\":1}}],[\"​≐a∈a∑​π\",{\"1\":{\"153\":1}}],[\"​≐g\",{\"1\":{\"99\":1}}],[\"​q^​\",{\"1\":{\"142\":3}}],[\"​tderrorδt​​\",{\"1\":{\"112\":1}}],[\"​forthegreedyaction\",{\"1\":{\"87\":1}}],[\"​vu​u2​pu​\",{\"1\":{\"70\":1}}],[\"​vu​u1​pu​π2\",{\"1\":{\"70\":1}}],[\"​p\",{\"1\":{\"48\":1}}],[\"​​−αt​\",{\"1\":{\"112\":1}}],[\"​​=currentestimatevt​\",{\"1\":{\"112\":1}}],[\"​​=a∑​qπ​\",{\"1\":{\"48\":1}}],[\"​​+η∇w​f\",{\"1\":{\"104\":1}}],[\"​​π\",{\"1\":{\"48\":1}}],[\"​​\",{\"1\":{\"45\":1,\"66\":1,\"104\":2,\"112\":1}}],[\"​=π\",{\"1\":{\"155\":1}}],[\"​=πmax​a∑​π\",{\"1\":{\"55\":1}}],[\"​=▽w​e\",{\"1\":{\"136\":1}}],[\"​=qt​\",{\"1\":{\"116\":1,\"120\":1}}],[\"​=vt​\",{\"1\":{\"112\":1}}],[\"​=∇w​f\",{\"1\":{\"104\":1}}],[\"​=w−e\",{\"1\":{\"110\":1}}],[\"​=w−x​=w−x+e\",{\"1\":{\"99\":1}}],[\"​=wk​−k1​\",{\"1\":{\"94\":1}}],[\"​=argmaxπ​\",{\"1\":{\"70\":1}}],[\"​=a∑​π\",{\"1\":{\"43\":1,\"44\":1}}],[\"​=rπk​​+γpπk​​vπk​\",{\"1\":{\"68\":1}}],[\"​=s∈s∑​d\",{\"1\":{\"155\":1}}],[\"​=s∈s∑​η\",{\"1\":{\"155\":1}}],[\"​=s\",{\"1\":{\"44\":1}}],[\"​=e\",{\"1\":{\"42\":1,\"45\":1}}],[\"​=es∼η\",{\"1\":{\"26\":1}}],[\"​=ϕ\",{\"1\":{\"8\":1,\"11\":1,\"13\":1}}],[\"​ifmosnew​>mosold​ifmosnew​=mosold​ifmosnew​<mosold​​\",{\"1\":{\"13\":1}}],[\"​≥0\",{\"1\":{\"11\":1,\"13\":1}}],[\"​≥γkn​​\",{\"1\":{\"11\":1,\"13\":1}}],[\"​−μlos​−μnlos​μnlos​​​s\",{\"1\":{\"9\":1}}],[\"​\",{\"1\":{\"9\":3,\"10\":1,\"11\":1,\"13\":1,\"26\":1,\"42\":1,\"44\":2,\"45\":1,\"46\":1,\"48\":2,\"55\":1,\"63\":1,\"66\":5,\"68\":1,\"77\":1,\"81\":2,\"87\":1,\"94\":1,\"104\":2,\"112\":1,\"116\":1,\"120\":1,\"136\":1,\"142\":1,\"153\":1,\"155\":2,\"156\":1}}],[\"​hn​\",{\"1\":{\"9\":1}}],[\"buffer\",{\"0\":{\"143\":1},\"1\":{\"142\":1}}],[\"bgd\",{\"0\":{\"106\":1},\"1\":{\"101\":1}}],[\"baidu\",{\"1\":{\"173\":3}}],[\"batch\",{\"1\":{\"101\":1,\"142\":2}}],[\"based\",{\"0\":{\"88\":1},\"1\":{\"77\":1,\"82\":1,\"147\":2,\"156\":1}}],[\"baseline\",{\"0\":{\"26\":1,\"27\":1},\"1\":{\"25\":1,\"26\":2,\"27\":1}}],[\"basic\",{\"0\":{\"76\":1},\"1\":{\"80\":1,\"81\":1,\"88\":1}}],[\"broad\",{\"1\":{\"92\":1}}],[\"boe\",{\"0\":{\"54\":1},\"1\":{\"52\":1,\"58\":1}}],[\"bootstrapping\",{\"1\":{\"45\":1}}],[\"b\",{\"1\":{\"26\":1,\"165\":4}}],[\"bit\",{\"1\":{\"10\":2}}],[\"bkn​​=bn​\",{\"1\":{\"9\":1}}],[\"b2​\",{\"1\":{\"9\":1}}],[\"b2​pnlos​=1−plos​\",{\"1\":{\"9\":1}}],[\"b1​\",{\"1\":{\"9\":2}}],[\"behavior\",{\"1\":{\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"135\":1}}],[\"bellman\",{\"0\":{\"42\":1,\"45\":1,\"46\":1,\"54\":1},\"1\":{\"37\":1,\"46\":2,\"47\":1,\"49\":2,\"52\":1,\"66\":1,\"68\":2,\"70\":1,\"71\":1,\"77\":1,\"113\":3,\"115\":1,\"119\":1}}],[\"be\",{\"1\":{\"4\":1,\"41\":1}}],[\"package\",{\"1\":{\"173\":3}}],[\"pair\",{\"1\":{\"79\":1,\"81\":6,\"82\":1,\"84\":2,\"86\":1}}],[\"private\",{\"1\":{\"174\":2}}],[\"println\",{\"1\":{\"161\":1,\"162\":1,\"166\":3,\"170\":2,\"174\":1}}],[\"protected\",{\"1\":{\"174\":2}}],[\"problems\",{\"1\":{\"92\":1}}],[\"probability\",{\"1\":{\"19\":1,\"20\":4,\"86\":1,\"98\":1,\"133\":1}}],[\"property\",{\"1\":{\"20\":1}}],[\"proposition1展示了无人机为相关用户提供可靠服务所需的高度的必要条件\",{\"1\":{\"9\":1}}],[\"proposition1\",{\"1\":{\"9\":1}}],[\"proposed\",{\"1\":{\"4\":1}}],[\"process\",{\"0\":{\"20\":1},\"1\":{\"20\":1,\"135\":1}}],[\"p2\",{\"1\":{\"161\":1}}],[\"p1\",{\"1\":{\"161\":2}}],[\"ppolicy\",{\"1\":{\"124\":1}}],[\"public\",{\"1\":{\"161\":4,\"162\":2,\"166\":4,\"170\":2,\"173\":5,\"174\":6}}],[\"pu\",{\"1\":{\"70\":1}}],[\"pi\",{\"1\":{\"66\":1,\"68\":1,\"70\":1}}],[\"person\",{\"1\":{\"161\":8,\"162\":3,\"166\":8,\"170\":3,\"173\":1,\"174\":2}}],[\"periods\",{\"1\":{\"10\":1}}],[\"pe\",{\"1\":{\"66\":2,\"68\":1,\"70\":1,\"77\":1}}],[\"pπ​\",{\"1\":{\"46\":1}}],[\"pπ​∈rn×n\",{\"1\":{\"46\":1}}],[\"p\",{\"1\":{\"19\":1,\"20\":3,\"41\":2,\"44\":1,\"45\":2,\"63\":2,\"75\":1,\"77\":4,\"98\":1,\"161\":4,\"162\":4}}],[\"pkn​\",{\"1\":{\"11\":1,\"13\":1}}],[\"pkn​​=pmax​\",{\"1\":{\"9\":1}}],[\"poicy\",{\"0\":{\"125\":1}}],[\"point\",{\"1\":{\"58\":1}}],[\"positive\",{\"1\":{\"86\":1,\"97\":1}}],[\"possible\",{\"1\":{\"41\":1}}],[\"policies\",{\"1\":{\"85\":1,\"87\":2}}],[\"policy版本\",{\"1\":{\"140\":1}}],[\"policyevaluation\",{\"1\":{\"77\":1}}],[\"policy\",{\"0\":{\"29\":1,\"31\":1,\"53\":1,\"65\":1,\"69\":1,\"70\":1,\"71\":1,\"72\":1,\"82\":1,\"86\":1,\"87\":1,\"88\":1,\"121\":2,\"122\":1,\"123\":1,\"126\":1,\"147\":1},\"1\":{\"19\":2,\"20\":1,\"23\":2,\"45\":1,\"47\":1,\"52\":1,\"53\":2,\"63\":3,\"66\":2,\"68\":1,\"69\":1,\"70\":11,\"71\":2,\"76\":1,\"77\":1,\"79\":4,\"82\":3,\"86\":2,\"87\":2,\"88\":1,\"111\":1,\"115\":1,\"120\":1,\"121\":2,\"122\":2,\"123\":4,\"124\":3,\"125\":2,\"144\":1,\"147\":2,\"148\":1}}],[\"poor\",{\"1\":{\"10\":1}}],[\"power\",{\"1\":{\"9\":1}}],[\"pm​ax​\",{\"1\":{\"9\":1}}],[\"pmax​≥γσ2k0​dkn​​α\",{\"1\":{\"9\":1}}],[\"plos​μlos​+pnlos​μnlos​\",{\"1\":{\"9\":1}}],[\"plos​\",{\"1\":{\"9\":1}}],[\"−2\",{\"1\":{\"142\":2}}],[\"−q^​\",{\"1\":{\"139\":1,\"140\":1,\"141\":1,\"142\":3}}],[\"−v^\",{\"1\":{\"133\":1,\"134\":2,\"135\":2,\"136\":4,\"138\":1}}],[\"−vπk​\",{\"1\":{\"66\":1}}],[\"−tdtargetvt​ˉ​\",{\"1\":{\"112\":1}}],[\"−αt​\",{\"1\":{\"112\":1,\"116\":1,\"120\":1}}],[\"−\",{\"1\":{\"110\":1,\"112\":1,\"116\":1,\"120\":1}}],[\"−x\",{\"1\":{\"99\":1}}],[\"−xkn​​\",{\"1\":{\"8\":1}}],[\"−e\",{\"1\":{\"99\":1,\"104\":2}}],[\"−f\",{\"1\":{\"58\":1}}],[\"−b\",{\"1\":{\"26\":1}}],[\"−0\",{\"1\":{\"13\":1}}],[\"−1rπk​​\",{\"1\":{\"68\":1}}],[\"−1rπ​\",{\"1\":{\"47\":1}}],[\"−1rπ​​\",{\"1\":{\"47\":1}}],[\"−1\",{\"1\":{\"9\":1,\"10\":2,\"13\":4}}],[\"−ykn​​\",{\"1\":{\"8\":1}}],[\"+age+\",{\"1\":{\"162\":1}}],[\"+name+\",{\"1\":{\"162\":1}}],[\"+η▽w​f\",{\"1\":{\"104\":1}}],[\"+η​\",{\"1\":{\"99\":1,\"110\":1}}],[\"+ηk​\",{\"1\":{\"97\":1}}],[\"+γs\",{\"1\":{\"46\":1,\"77\":1}}],[\"+γe\",{\"1\":{\"42\":1,\"45\":1}}],[\"+rtt−rkn​​\",{\"1\":{\"10\":1}}],[\"+c2​\",{\"1\":{\"10\":1}}],[\"+ζ2​moskn​​rate\",{\"1\":{\"10\":1}}],[\"+\",{\"1\":{\"8\":1,\"99\":1,\"110\":1,\"165\":2}}],[\"yt​\",{\"1\":{\"142\":1}}],[\"yt​≐r+γmaxa∈a\",{\"1\":{\"142\":1}}],[\"yt​−q^​\",{\"1\":{\"142\":2}}],[\"y≐r+γmaxa∈a\",{\"1\":{\"142\":1}}],[\"yuser​\",{\"1\":{\"14\":2}}],[\"yuav​\",{\"1\":{\"13\":2,\"14\":2}}],[\"yd​+1\",{\"1\":{\"13\":1}}],[\"yd​\",{\"1\":{\"13\":1}}],[\"yn​\",{\"1\":{\"8\":2}}],[\"ykn​​\",{\"1\":{\"8\":1}}],[\"xi​\",{\"1\":{\"101\":2}}],[\"xk+1​=f\",{\"1\":{\"58\":1}}],[\"xk​→x∗\",{\"1\":{\"58\":1}}],[\"xk​\",{\"1\":{\"58\":2,\"101\":1,\"104\":1,\"110\":1}}],[\"xkn​​\",{\"1\":{\"8\":1}}],[\"x∗\",{\"1\":{\"58\":1}}],[\"x=f\",{\"1\":{\"58\":1}}],[\"x2​\",{\"1\":{\"58\":1,\"75\":1}}],[\"x1​\",{\"1\":{\"58\":1}}],[\"x\",{\"1\":{\"58\":2,\"75\":1,\"94\":1,\"99\":10,\"101\":6,\"104\":17,\"110\":9}}],[\"xuser​\",{\"1\":{\"14\":2}}],[\"xuav​\",{\"1\":{\"13\":1,\"14\":2}}],[\"xt​=⎩⎨⎧​1\",{\"1\":{\"13\":1}}],[\"xd​+1\",{\"1\":{\"13\":1}}],[\"xd​\",{\"1\":{\"13\":1}}],[\"xn​\",{\"1\":{\"8\":2,\"75\":1}}],[\"=θt​+αe\",{\"1\":{\"156\":1}}],[\"====​n→∞lim​n1​e\",{\"1\":{\"154\":1}}],[\"=dtvπ​\",{\"1\":{\"150\":1}}],[\"=i=1∑n​yt​−q^​\",{\"1\":{\"142\":1}}],[\"=−2e\",{\"1\":{\"136\":1}}],[\"=−c1​ln\",{\"1\":{\"10\":1}}],[\"=∣s∣1​s∈s∑​\",{\"1\":{\"134\":1}}],[\"=qt​\",{\"1\":{\"116\":1,\"120\":1}}],[\"=vt​\",{\"1\":{\"112\":1}}],[\"=vπ0​​\",{\"1\":{\"70\":1}}],[\"=w−\",{\"1\":{\"110\":1}}],[\"=wk​−αk​▽w​f\",{\"1\":{\"104\":1}}],[\"=wk​−αk​e\",{\"1\":{\"101\":1}}],[\"=wk​−αk​\",{\"1\":{\"99\":1,\"110\":1}}],[\"=▽w​j\",{\"1\":{\"104\":1}}],[\"=g\",{\"1\":{\"97\":1,\"104\":1}}],[\"=k1​\",{\"1\":{\"94\":1}}],[\"=k0​−1dkn​​−α\",{\"1\":{\"9\":1}}],[\"=n1​∑i=1n​xi​\",{\"1\":{\"94\":1}}],[\"=1\",{\"1\":{\"79\":1,\"135\":1,\"150\":1}}],[\"=argmaxπ​∑a​π\",{\"1\":{\"79\":1}}],[\"=argmaxa​qπk​​\",{\"1\":{\"66\":1}}],[\"=argmaxa​qk​\",{\"1\":{\"63\":1}}],[\"=a∑​πk​\",{\"1\":{\"66\":1}}],[\"=a∑​π\",{\"1\":{\"43\":1,\"44\":3,\"45\":1,\"48\":1,\"113\":1}}],[\"=πargmax​a∑​πk​\",{\"1\":{\"66\":1}}],[\"=πargmax​a∑​π\",{\"1\":{\"63\":1}}],[\"=πmax​\",{\"1\":{\"61\":1,\"62\":1}}],[\"=x∗\",{\"1\":{\"58\":1}}],[\"=x\",{\"1\":{\"58\":1}}],[\"=maxa​qk​\",{\"1\":{\"63\":1}}],[\"=maxπ​\",{\"1\":{\"58\":1}}],[\"=maxπ​∑a​π\",{\"1\":{\"57\":1}}],[\"=mean\",{\"1\":{\"45\":1}}],[\"=∑a​π\",{\"1\":{\"49\":1}}],[\"=∑a​qπ​\",{\"1\":{\"48\":1}}],[\"=∑r​p\",{\"1\":{\"48\":1}}],[\"=r∑​rp\",{\"1\":{\"153\":1}}],[\"=r∑​p\",{\"1\":{\"77\":1}}],[\"=rπ​\",{\"1\":{\"46\":1}}],[\"=rt+1​+γgt+1​​\",{\"1\":{\"42\":1}}],[\"=rt+1​+γ\",{\"1\":{\"42\":1}}],[\"=s∈s∑​d\",{\"1\":{\"155\":1}}],[\"=s∈s∑​dπ​\",{\"1\":{\"135\":1}}],[\"=s\",{\"1\":{\"44\":3}}],[\"=sin−1\",{\"1\":{\"9\":1}}],[\"=es∼d\",{\"1\":{\"155\":1}}],[\"=es∼d​\",{\"1\":{\"153\":1,\"155\":1}}],[\"=es∼η\",{\"1\":{\"26\":1}}],[\"=e\",{\"1\":{\"40\":1,\"42\":2,\"48\":1,\"49\":2,\"75\":2,\"77\":1,\"78\":1,\"101\":1,\"104\":4,\"113\":3,\"115\":1,\"119\":1,\"133\":1,\"134\":1,\"135\":1,\"136\":1,\"141\":1,\"142\":3,\"151\":1,\"153\":1,\"155\":1}}],[\"=0\",{\"1\":{\"26\":1,\"96\":1,\"98\":2,\"99\":1,\"104\":2,\"110\":1}}],[\"=p\",{\"1\":{\"20\":2}}],[\"=3rtt+rkn​​\",{\"1\":{\"10\":1}}],[\"=ζ1​moskn​​delay\",{\"1\":{\"10\":1}}],[\"=γk0​σ2dkn​​α\",{\"1\":{\"9\":1}}],[\"=b2​ln\",{\"1\":{\"9\":1}}],[\"=bkn​​log2​\",{\"1\":{\"9\":1}}],[\"=b1​\",{\"1\":{\"9\":1}}],[\"=σ2pkn​​gkn​​\",{\"1\":{\"9\":1}}],[\"=\",{\"1\":{\"8\":1,\"19\":1,\"57\":1,\"63\":1,\"66\":1,\"87\":1,\"99\":1,\"110\":1,\"155\":1,\"161\":6,\"162\":3,\"164\":2,\"166\":16,\"170\":1,\"173\":1}}],[\"hello\",{\"1\":{\"162\":2}}],[\"has\",{\"1\":{\"58\":1}}],[\"hard问题\",{\"1\":{\"13\":1}}],[\"hard\",{\"1\":{\"5\":1}}],[\"hybrid\",{\"1\":{\"14\":1}}],[\"huav​\",{\"1\":{\"13\":2,\"14\":2}}],[\"hmax​−hmin​+1\",{\"1\":{\"13\":1}}],[\"hmax​mostotal​=∑n=1n​∑kn​=1kn​​moskn​​\",{\"1\":{\"13\":1}}],[\"hmax​mostotal​=∑n=1n​∑kn​=1kn​​∑t=0ts​​moskn​​\",{\"1\":{\"11\":1}}],[\"hmax​\",{\"1\":{\"8\":1,\"13\":1}}],[\"hmin​≤hn​\",{\"1\":{\"11\":1,\"13\":1}}],[\"hmin​\",{\"1\":{\"8\":1,\"13\":1}}],[\"∈\",{\"1\":{\"8\":1}}],[\"飞行速度恒定\",{\"1\":{\"8\":1}}],[\"其他的情况会在讲到反射时介绍\",{\"1\":{\"171\":1}}],[\"其他的算法\",{\"1\":{\"156\":1}}],[\"其变量名存储的是对象的引用\",{\"1\":{\"161\":1}}],[\"其求解梯度比较难求\",{\"1\":{\"142\":1}}],[\"其定义都是一个均值\",{\"1\":{\"92\":1}}],[\"其探索性就很强\",{\"1\":{\"87\":1}}],[\"其属于\",{\"1\":{\"87\":1}}],[\"其原始定义都是从期望出发的\",{\"1\":{\"75\":1}}],[\"其核心思想是\",{\"1\":{\"75\":1}}],[\"其对应的\",{\"1\":{\"63\":1}}],[\"其策略π表示的是最优策略\",{\"1\":{\"55\":1}}],[\"其目标也应该不一样\",{\"1\":{\"13\":1}}],[\"其状态为其3d坐标\",{\"1\":{\"13\":1}}],[\"其高度的下界是距离dkn​​\",{\"1\":{\"9\":1}}],[\"其每个用户带宽表示为\",{\"1\":{\"9\":1}}],[\"其可用带宽为bn​\",{\"1\":{\"9\":1}}],[\"其水平坐标表示为qn​\",{\"1\":{\"8\":1}}],[\"其垂直高度表示为hn​\",{\"1\":{\"8\":1}}],[\"其坐标表示为wkn​​=\",{\"1\":{\"8\":1}}],[\"其中的\",{\"1\":{\"173\":1}}],[\"其中st​是随机变量s的一个样本\",{\"1\":{\"136\":1}}],[\"其中w∈rm是参数向量\",{\"1\":{\"131\":1}}],[\"其中hk​=wk​\",{\"1\":{\"98\":1}}],[\"其中ak∗​=argmaxa​qπk​​\",{\"1\":{\"79\":1}}],[\"其中ak∗​\",{\"1\":{\"63\":1}}],[\"其中a∗表示在该状态下计算出来的最大\",{\"1\":{\"57\":1}}],[\"其中vk​是给定的\",{\"1\":{\"63\":1}}],[\"其中f\",{\"1\":{\"58\":1}}],[\"其中cmax​表示用户的最大速度\",{\"1\":{\"14\":1}}],[\"其中\",{\"1\":{\"9\":1,\"10\":2,\"46\":1,\"66\":1,\"87\":1,\"96\":1,\"97\":1,\"101\":1,\"110\":1,\"112\":1,\"113\":1,\"116\":1,\"135\":1,\"141\":1,\"148\":1,\"153\":1,\"155\":1}}],[\"其中σ2=bkn​​n0​\",{\"1\":{\"9\":1}}],[\"其中k0​=\",{\"1\":{\"9\":1}}],[\"其中kn​表示划分到集群n的用户\",{\"1\":{\"8\":1}}],[\"其中θkn​​\",{\"1\":{\"9\":1}}],[\"其中用户表示为k=k1​\",{\"1\":{\"8\":1}}],[\"在当前包以外的其他包中无法访问\",{\"1\":{\"174\":1}}],[\"在当前状态s下\",{\"1\":{\"48\":1}}],[\"在当前状态s下采取动作\",{\"1\":{\"48\":1}}],[\"在不同包下的类\",{\"1\":{\"173\":1}}],[\"在放入包中\",{\"1\":{\"173\":1}}],[\"在静态方法中\",{\"1\":{\"170\":1}}],[\"在成员变量初始化之后执行\",{\"1\":{\"166\":1}}],[\"在赋值之前看看是否有初始值\",{\"1\":{\"166\":1}}],[\"在我们自己定义一个构造方法之后\",{\"1\":{\"166\":1}}],[\"在创建了对象之后\",{\"1\":{\"161\":1}}],[\"在状态s采用动作a\",{\"1\":{\"153\":1}}],[\"在该策略下的所有\",{\"1\":{\"148\":1}}],[\"在该文中考虑的是网页浏览应用传输情况\",{\"1\":{\"10\":1}}],[\"在该文中\",{\"1\":{\"10\":1}}],[\"在原文是\",{\"1\":{\"144\":1}}],[\"在训练求解梯度时\",{\"1\":{\"142\":1}}],[\"在每一次迭代时\",{\"1\":{\"142\":1}}],[\"在初始化的时候是设为相同的\",{\"1\":{\"142\":1}}],[\"在初始时间假设用户处于静止下不断调整\",{\"1\":{\"5\":1}}],[\"在将\",{\"1\":{\"142\":1}}],[\"在计算\",{\"1\":{\"142\":1}}],[\"在通过经验来更新这个策略\",{\"1\":{\"122\":1}}],[\"在之前关于使用\",{\"1\":{\"105\":1}}],[\"在这里\",{\"1\":{\"87\":1}}],[\"在收集到了足够多的\",{\"1\":{\"82\":1}}],[\"在求解\",{\"1\":{\"71\":1}}],[\"在策略更新上\",{\"1\":{\"70\":1}}],[\"在策略梯度算法中引入一个\",{\"1\":{\"26\":1}}],[\"在实际情况中\",{\"1\":{\"27\":1}}],[\"在实际应用中\",{\"1\":{\"9\":1}}],[\"在\",{\"1\":{\"25\":1,\"68\":2,\"86\":1,\"138\":1,\"142\":1}}],[\"在policy是确定的情况下\",{\"1\":{\"20\":1}}],[\"在执行一个动作后获得的一个常数\",{\"1\":{\"19\":1}}],[\"在此情况下\",{\"1\":{\"14\":1}}],[\"在本文中\",{\"1\":{\"14\":1}}],[\"在本文中不考虑用户移动到其他集群的情况\",{\"1\":{\"14\":1}}],[\"在设计无人机的移动之前\",{\"1\":{\"14\":1}}],[\"在时刻t关联到无人机n的地面用户kn​的接受到的信噪比表示为\",{\"1\":{\"9\":1}}],[\"在时间t\",{\"1\":{\"9\":1}}],[\"在任意时刻t\",{\"1\":{\"8\":1}}],[\"2种方法\",{\"1\":{\"49\":1}}],[\"20240826181712\",{\"1\":{\"157\":1}}],[\"20240826181638\",{\"1\":{\"156\":1}}],[\"20240826181538\",{\"1\":{\"156\":1}}],[\"20240826181340\",{\"1\":{\"156\":1}}],[\"20240826180244\",{\"1\":{\"155\":1}}],[\"20240826173749\",{\"1\":{\"151\":1}}],[\"20240820231205\",{\"1\":{\"144\":1}}],[\"20240820231024\",{\"1\":{\"144\":1}}],[\"20240820230944\",{\"1\":{\"143\":1}}],[\"20240820230920\",{\"1\":{\"143\":1}}],[\"20240820230827\",{\"1\":{\"143\":1}}],[\"20240820184405\",{\"1\":{\"140\":1}}],[\"20240820184127\",{\"1\":{\"139\":1}}],[\"20240820181718\",{\"1\":{\"135\":1}}],[\"20240820181406\",{\"1\":{\"135\":1}}],[\"20240815234719\",{\"1\":{\"173\":1}}],[\"20240818182231\",{\"1\":{\"127\":1}}],[\"20240818182301\",{\"1\":{\"127\":1}}],[\"20240818182057\",{\"1\":{\"125\":1}}],[\"20240818181917\",{\"1\":{\"126\":1}}],[\"20240817000409\",{\"1\":{\"118\":1}}],[\"20240817000331\",{\"1\":{\"118\":1}}],[\"20240817000642\",{\"1\":{\"117\":1}}],[\"20240817000601\",{\"1\":{\"117\":1}}],[\"20240817000500\",{\"1\":{\"117\":1}}],[\"20240817000230\",{\"1\":{\"116\":1}}],[\"20240817000134\",{\"1\":{\"116\":1}}],[\"20240817000114\",{\"1\":{\"116\":1}}],[\"20240814230747\",{\"1\":{\"106\":1}}],[\"20240814014058\",{\"1\":{\"103\":1}}],[\"20240812010538\",{\"1\":{\"89\":1}}],[\"20240812011140\",{\"1\":{\"88\":1}}],[\"20240812004534\",{\"1\":{\"83\":1}}],[\"20240811233346\",{\"1\":{\"79\":1}}],[\"20240811011334\",{\"1\":{\"72\":1}}],[\"20240811010933\",{\"1\":{\"71\":1}}],[\"20240811002219\",{\"1\":{\"67\":1}}],[\"20240810190018\",{\"1\":{\"64\":1}}],[\"20240830200624\",{\"1\":{\"33\":1}}],[\"20240830200608\",{\"1\":{\"33\":1}}],[\"20240830200406\",{\"1\":{\"32\":1}}],[\"20240830200343\",{\"1\":{\"31\":1}}],[\"20240830200320\",{\"1\":{\"31\":1}}],[\"20240830200305\",{\"1\":{\"31\":1}}],[\"20240830200248\",{\"1\":{\"31\":1}}],[\"20240830200138\",{\"1\":{\"30\":1}}],[\"20240830200118\",{\"1\":{\"30\":1}}],[\"20240830200056\",{\"1\":{\"30\":1}}],[\"20240830185629\",{\"1\":{\"28\":1}}],[\"20240830185556\",{\"1\":{\"28\":1}}],[\"20240830185537\",{\"1\":{\"28\":1}}],[\"20240830185324\",{\"1\":{\"27\":1}}],[\"20240830185127\",{\"1\":{\"26\":1}}],[\"20240830184424\",{\"1\":{\"24\":1}}],[\"20240830184330\",{\"1\":{\"24\":1}}],[\"20240830184312\",{\"1\":{\"23\":1}}],[\"20240830184236\",{\"1\":{\"23\":1}}],[\"2019\",{\"1\":{\"3\":1}}],[\"2l−1\",{\"1\":{\"10\":1}}],[\"2mssfs​+1\",{\"1\":{\"10\":1}}],[\"2mss\",{\"1\":{\"10\":1}}],[\"2~3\",{\"1\":{\"10\":1}}],[\"2​\",{\"1\":{\"8\":1}}],[\"2+\",{\"1\":{\"8\":1}}],[\"2\",{\"0\":{\"20\":1,\"25\":1,\"26\":1,\"27\":2,\"28\":1,\"31\":1,\"40\":1,\"42\":1,\"43\":1,\"44\":2,\"45\":1,\"46\":1,\"54\":1,\"55\":1,\"56\":2,\"57\":2,\"58\":1,\"64\":1,\"65\":1,\"66\":1,\"67\":2,\"68\":1,\"71\":1,\"78\":1,\"80\":1,\"81\":1,\"82\":2,\"83\":1,\"84\":1,\"87\":1,\"95\":1,\"96\":1,\"97\":2,\"98\":1,\"99\":1,\"102\":1,\"111\":1,\"112\":1,\"113\":2,\"114\":1,\"117\":1,\"121\":1,\"132\":1,\"133\":1,\"136\":2,\"149\":1,\"150\":1,\"153\":2},\"1\":{\"8\":1,\"9\":1,\"13\":1,\"14\":1,\"62\":1,\"63\":1,\"66\":1,\"68\":1,\"70\":1,\"79\":2,\"94\":1,\"97\":1,\"101\":1,\"112\":1,\"116\":1,\"133\":1,\"134\":2,\"135\":2,\"136\":2,\"141\":1,\"142\":2}}],[\"即使类名相同\",{\"1\":{\"173\":1}}],[\"即使先估计了\",{\"1\":{\"79\":1}}],[\"即空指针异常\",{\"1\":{\"161\":1}}],[\"即在策略\",{\"1\":{\"152\":1}}],[\"即在状态s1​下采用动作a1​获得的奖励r=1的概率\",{\"1\":{\"19\":1}}],[\"即此时策略\",{\"1\":{\"148\":1}}],[\"即此时的discounted\",{\"1\":{\"19\":1}}],[\"即基于一个策略\",{\"1\":{\"135\":1}}],[\"即各个状态的可能性为∣s∣1​\",{\"1\":{\"134\":1}}],[\"即我通过一个策略与环境进行交互生成一系列经验\",{\"1\":{\"123\":1}}],[\"即我通过这个策略与环境进行交互生成一系列经验\",{\"1\":{\"122\":1}}],[\"即函数\",{\"1\":{\"97\":1}}],[\"即不需要完全精确地求出\",{\"1\":{\"82\":1}}],[\"即不具备terminal\",{\"1\":{\"19\":1}}],[\"即对于损失函数中的\",{\"1\":{\"133\":1}}],[\"即对于数据\",{\"1\":{\"80\":1}}],[\"即对于每个状态\",{\"1\":{\"79\":1}}],[\"即对于给定策略\",{\"1\":{\"47\":1}}],[\"即p\",{\"1\":{\"75\":1}}],[\"即j→∞\",{\"1\":{\"66\":1}}],[\"即求解右边的式子\",{\"1\":{\"63\":1}}],[\"即f\",{\"1\":{\"58\":1}}],[\"即a∗=argmaxa​q\",{\"1\":{\"57\":1}}],[\"即可以通过梯度下降\",{\"1\":{\"142\":1}}],[\"即可\",{\"1\":{\"57\":1,\"77\":1,\"162\":1}}],[\"即vπ​\",{\"1\":{\"49\":1}}],[\"即一个trajectory下的discounted\",{\"1\":{\"39\":1}}],[\"即\",{\"1\":{\"26\":1,\"63\":2,\"66\":2,\"68\":1,\"77\":1,\"78\":2,\"94\":1,\"99\":1,\"137\":1,\"161\":1}}],[\"即无记忆的特性\",{\"1\":{\"20\":1}}],[\"即无人机作为空中基站\",{\"1\":{\"8\":1}}],[\"即表示具有终止状态terminal\",{\"1\":{\"19\":1}}],[\"即状态s1​采用动作a1​转到状态s2​的概率\",{\"1\":{\"19\":1}}],[\"即ξ=\",{\"1\":{\"14\":1}}],[\"即xuav​\",{\"1\":{\"13\":1}}],[\"即γ≥γkn​​\",{\"1\":{\"9\":1}}],[\"即需要考虑地面不同用户的具体需求\",{\"1\":{\"4\":1}}],[\"lnπ\",{\"1\":{\"155\":1}}],[\"loss\",{\"1\":{\"141\":1}}],[\"long\",{\"1\":{\"135\":1}}],[\"limiting\",{\"1\":{\"135\":1}}],[\"link\",{\"1\":{\"8\":1}}],[\"lang包下的\",{\"1\":{\"173\":1}}],[\"lang\",{\"1\":{\"173\":1}}],[\"lang这个包下的所有类\",{\"1\":{\"173\":1}}],[\"large\",{\"1\":{\"75\":1}}],[\"law\",{\"1\":{\"75\":1}}],[\"l2​=log2​\",{\"1\":{\"10\":1}}],[\"l2​\",{\"1\":{\"10\":1}}],[\"l1​=log2​\",{\"1\":{\"10\":1}}],[\"l1​\",{\"1\":{\"10\":1}}],[\"l=min\",{\"1\":{\"10\":1}}],[\"learning是优化长期目标\",{\"1\":{\"13\":1}}],[\"learning的部署算法不同的是\",{\"1\":{\"14\":1}}],[\"learning的移动算法\",{\"1\":{\"14\":1}}],[\"learning的优化目标是最大化长期收益\",{\"1\":{\"13\":1}}],[\"learning的方案来解决无人机的np\",{\"1\":{\"5\":1}}],[\"learning算法\",{\"1\":{\"13\":1}}],[\"learning\",{\"0\":{\"3\":1,\"109\":1,\"111\":1,\"115\":1,\"119\":1,\"120\":1,\"124\":1,\"137\":1,\"138\":1,\"140\":1,\"141\":1},\"1\":{\"5\":2,\"119\":1,\"120\":1,\"123\":1,\"124\":1,\"141\":1}}],[\"考虑能否仅用一次\",{\"1\":{\"101\":1}}],[\"考虑一个复杂的均值估计问题\",{\"1\":{\"110\":1}}],[\"考虑一个\",{\"1\":{\"81\":1}}],[\"考虑\",{\"1\":{\"70\":2}}],[\"考虑用户在每个时隙移动的情况\",{\"1\":{\"14\":1}}],[\"考虑以下场景\",{\"1\":{\"13\":1}}],[\"考虑无人机辅助无线网络的下行链路传输\",{\"1\":{\"8\":1}}],[\"考虑qoe\",{\"1\":{\"4\":1}}],[\"系统结构\",{\"0\":{\"7\":1}}],[\"的使用\",{\"0\":{\"164\":1}}],[\"的算法\",{\"1\":{\"156\":1}}],[\"的选择\",{\"0\":{\"152\":1}}],[\"的权重或者分布\",{\"1\":{\"155\":1}}],[\"的权重\",{\"1\":{\"150\":1}}],[\"的加权平均\",{\"1\":{\"150\":1}}],[\"的加权均值\",{\"1\":{\"48\":1}}],[\"的基本步骤\",{\"1\":{\"148\":1}}],[\"的基础上来引入偏置量\",{\"1\":{\"25\":1}}],[\"的文章中\",{\"1\":{\"144\":1}}],[\"的输出是不一样的\",{\"1\":{\"144\":1}}],[\"的输出\",{\"1\":{\"142\":1}}],[\"的参数\",{\"1\":{\"142\":1}}],[\"的就不是有关\",{\"1\":{\"142\":1}}],[\"的梯度时\",{\"1\":{\"142\":1}}],[\"的分布\",{\"1\":{\"135\":1}}],[\"的新形式\",{\"1\":{\"113\":1}}],[\"的定义\",{\"1\":{\"113\":1}}],[\"的定义出发\",{\"1\":{\"77\":1}}],[\"的估计从\",{\"1\":{\"130\":1}}],[\"的估计\",{\"1\":{\"112\":1,\"116\":1}}],[\"的采样\",{\"1\":{\"110\":1}}],[\"的思想\",{\"1\":{\"101\":1}}],[\"的值\",{\"1\":{\"99\":1}}],[\"的动作数量\",{\"1\":{\"87\":1}}],[\"的高效利用\",{\"0\":{\"81\":1}}],[\"的一些改进\",{\"1\":{\"80\":1}}],[\"的一个观测值\",{\"1\":{\"104\":1}}],[\"的一个\",{\"1\":{\"78\":1}}],[\"的情况\",{\"1\":{\"76\":1,\"77\":2,\"104\":1}}],[\"的情况下进行估计\",{\"1\":{\"75\":1}}],[\"的原理\",{\"1\":{\"76\":1}}],[\"的\",{\"1\":{\"63\":1,\"66\":1,\"77\":1,\"78\":2,\"81\":1,\"84\":1,\"86\":1,\"87\":1,\"111\":1,\"113\":2,\"123\":2,\"124\":2,\"133\":1,\"155\":1}}],[\"的策略来进行选择\",{\"1\":{\"66\":1}}],[\"的策略πk+1​\",{\"1\":{\"63\":1}}],[\"的策略所采集的数据来\",{\"1\":{\"29\":1}}],[\"的形式\",{\"1\":{\"58\":1,\"99\":1}}],[\"的根据策略π加权平均\",{\"1\":{\"49\":1}}],[\"的转到下一个状态的\",{\"1\":{\"48\":1}}],[\"的过程\",{\"1\":{\"47\":1}}],[\"的计算即可\",{\"1\":{\"42\":1}}],[\"的区别\",{\"0\":{\"41\":1}}],[\"的期望\",{\"1\":{\"40\":1}}],[\"的方法进行解决\",{\"1\":{\"142\":1}}],[\"的方法\",{\"1\":{\"29\":1,\"68\":1,\"85\":1,\"147\":2}}],[\"的概率\",{\"1\":{\"20\":2}}],[\"的s是有范围的\",{\"1\":{\"13\":1}}],[\"的函数\",{\"1\":{\"9\":1,\"142\":1}}],[\"的功率谱密度\",{\"1\":{\"9\":1}}],[\"的无人机3d动态运动设计算法\",{\"1\":{\"5\":1}}],[\"的部署方法\",{\"1\":{\"5\":1}}],[\"以及是否可以到最后优化的成果\",{\"1\":{\"101\":1}}],[\"以及\",{\"1\":{\"69\":1,\"88\":1,\"155\":1}}],[\"以向用户提供可靠的服务\",{\"1\":{\"9\":1}}],[\"以\",{\"1\":{\"5\":1}}],[\"该算法中\",{\"1\":{\"122\":1,\"123\":1}}],[\"该算法是\",{\"1\":{\"69\":1}}],[\"该方法成立的数学依据是\",{\"1\":{\"75\":1}}],[\"该步骤是根据\",{\"1\":{\"66\":1}}],[\"该步骤是用来计算当前策略\",{\"1\":{\"66\":1}}],[\"该式子针对状态空间中的所有状态均成立\",{\"1\":{\"45\":1}}],[\"该论文中在精度和模型复杂型上作出平衡\",{\"1\":{\"13\":1}}],[\"该问题依然是np\",{\"1\":{\"13\":1}}],[\"该优化问题是一个non\",{\"1\":{\"11\":1}}],[\"该文中不同集群所利用的频谱是不同的\",{\"1\":{\"9\":1}}],[\"该文中表示为\",{\"1\":{\"9\":1}}],[\"该文提出的算法具较快的收敛性\",{\"1\":{\"5\":1}}],[\"该文基于q\",{\"1\":{\"5\":1}}],[\"该文设计的是3d部署\",{\"1\":{\"4\":1}}],[\"该框架将无人机部署在三维空间内\",{\"1\":{\"5\":1}}],[\"过去研究主要考虑的是2d部署\",{\"1\":{\"4\":1}}],[\"过去研究大多没有基于用户的移动\",{\"1\":{\"4\":1}}],[\"sum\",{\"1\":{\"165\":2}}],[\"supposed\",{\"1\":{\"4\":1}}],[\"system\",{\"1\":{\"161\":1,\"162\":1,\"166\":3,\"170\":2,\"174\":1}}],[\"s∣s\",{\"1\":{\"155\":1}}],[\"s∣a\",{\"1\":{\"57\":1,\"58\":1}}],[\"s∑​dπ​\",{\"1\":{\"154\":1}}],[\"scalar\",{\"1\":{\"148\":1}}],[\"score\",{\"1\":{\"5\":1}}],[\"sgdw\",{\"0\":{\"106\":1}}],[\"sgd的目标是\",{\"1\":{\"104\":1}}],[\"sgd\",{\"0\":{\"102\":1,\"104\":1,\"105\":1},\"1\":{\"101\":1,\"104\":3,\"105\":2,\"136\":1,\"155\":1}}],[\"s5​\",{\"1\":{\"81\":1}}],[\"s5​a1​​\",{\"1\":{\"81\":1}}],[\"s2​\",{\"1\":{\"81\":4}}],[\"s2​a3​​s5​a1​​\",{\"1\":{\"81\":1}}],[\"s2​a4​​s1​a2​​s2​a3​​s5​a1​​\",{\"1\":{\"81\":1}}],[\"s2​∣s1​\",{\"1\":{\"19\":1}}],[\"sex\",{\"1\":{\"161\":1,\"162\":1,\"166\":14,\"170\":1,\"174\":1}}],[\"sequence\",{\"1\":{\"58\":1}}],[\"setname\",{\"1\":{\"164\":2}}],[\"set\",{\"1\":{\"20\":3}}],[\"sets\",{\"1\":{\"20\":1}}],[\"s​=maxπ​∑a​π\",{\"1\":{\"58\":1}}],[\"soloving\",{\"1\":{\"92\":1}}],[\"solution\",{\"1\":{\"47\":2}}],[\"soft\",{\"0\":{\"86\":1},\"1\":{\"85\":1,\"86\":2,\"87\":1}}],[\"some\",{\"1\":{\"19\":1,\"116\":1}}],[\"slove\",{\"0\":{\"47\":1}}],[\"slow\",{\"1\":{\"10\":1}}],[\"sj​∣si​\",{\"1\":{\"46\":1}}],[\"sn​\",{\"1\":{\"46\":2}}],[\"sa​\",{\"1\":{\"116\":2}}],[\"sarsa\",{\"0\":{\"116\":1,\"117\":1,\"118\":1,\"139\":1},\"1\":{\"115\":3,\"116\":1,\"123\":1}}],[\"satisfying\",{\"1\":{\"98\":1}}],[\"satisfaction\",{\"1\":{\"4\":1}}],[\"sa\",{\"1\":{\"92\":2}}],[\"samples\",{\"1\":{\"101\":1,\"142\":1}}],[\"sample\",{\"1\":{\"78\":1,\"99\":1,\"101\":1}}],[\"sampling\",{\"0\":{\"30\":1}}],[\"s0​\",{\"1\":{\"20\":2,\"112\":1}}],[\"s∈s​\",{\"1\":{\"135\":1}}],[\"s∈s\",{\"1\":{\"20\":1,\"63\":1,\"66\":2,\"113\":2}}],[\"s1​a2​​s2​a3​​s5​a1​​\",{\"1\":{\"81\":1}}],[\"s1​a2​​s2​a4​​s1​a2​​s2​a3​​s5​a1​​\",{\"1\":{\"81\":2}}],[\"s1​\",{\"1\":{\"46\":2,\"81\":4,\"112\":1}}],[\"s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​r=1→​a2​​s9​r=1→​a2​​s9​\",{\"1\":{\"19\":1}}],[\"s1​r=0→​a2​​s2​r=0→​a2​​s5​r=0→​a2​​s8​r=1→​a2​​s9​\",{\"1\":{\"19\":1}}],[\"s1​→a1​s2​\",{\"1\":{\"19\":1}}],[\"simplest\",{\"0\":{\"24\":1}}],[\"si​\",{\"1\":{\"19\":2,\"142\":2}}],[\"single\",{\"1\":{\"82\":1,\"154\":1}}],[\"sin\",{\"1\":{\"9\":1}}],[\"s=\",{\"1\":{\"19\":1}}],[\"string\",{\"1\":{\"161\":5,\"162\":3,\"164\":3,\"166\":14,\"170\":4,\"173\":6,\"174\":3}}],[\"steady\",{\"1\":{\"135\":1}}],[\"step\",{\"0\":{\"117\":1},\"1\":{\"66\":2,\"79\":2,\"154\":1}}],[\"st+2​→at+3​\",{\"1\":{\"39\":1,\"42\":1}}],[\"st+2​→at+2​rt+3​\",{\"1\":{\"39\":1,\"42\":1}}],[\"st+1​=s\",{\"1\":{\"44\":2}}],[\"st+1​→at+1​rt+2​\",{\"1\":{\"39\":1,\"42\":1}}],[\"st+1​∣at+1​\",{\"1\":{\"20\":3}}],[\"st+1​\",{\"1\":{\"13\":1,\"112\":4,\"116\":2,\"119\":1,\"120\":1,\"138\":2,\"139\":1,\"140\":2}}],[\"st​→at​rt+1​\",{\"1\":{\"39\":1,\"42\":1}}],[\"st​\",{\"1\":{\"20\":4,\"112\":12,\"116\":6,\"120\":5,\"136\":4,\"137\":4,\"138\":3,\"139\":2,\"140\":2,\"156\":1}}],[\"stochastic\",{\"0\":{\"100\":1},\"1\":{\"19\":1,\"53\":1,\"86\":1,\"92\":2}}],[\"stop\",{\"1\":{\"19\":1}}],[\"static\",{\"1\":{\"161\":3,\"162\":1,\"170\":5,\"173\":2,\"174\":3}}],[\"stationary\",{\"0\":{\"135\":1},\"1\":{\"135\":2,\"152\":1}}],[\"statrts的解释\",{\"0\":{\"84\":1}}],[\"status\",{\"1\":{\"19\":1}}],[\"statevalue\",{\"1\":{\"136\":1}}],[\"states中\",{\"1\":{\"19\":1}}],[\"states中限制action\",{\"1\":{\"19\":1}}],[\"states的任务\",{\"1\":{\"19\":1}}],[\"states的trajectory\",{\"1\":{\"19\":1}}],[\"states\",{\"1\":{\"19\":1,\"20\":1}}],[\"state\",{\"0\":{\"38\":1,\"40\":1,\"41\":1,\"47\":1,\"58\":1,\"111\":1,\"132\":1,\"150\":1},\"1\":{\"13\":2,\"19\":6,\"20\":4,\"37\":1,\"40\":4,\"41\":4,\"42\":1,\"45\":1,\"47\":1,\"48\":6,\"49\":2,\"52\":1,\"55\":1,\"56\":1,\"63\":1,\"66\":2,\"68\":1,\"70\":2,\"71\":1,\"75\":1,\"77\":3,\"79\":3,\"81\":6,\"82\":1,\"84\":2,\"86\":1,\"92\":1,\"111\":1,\"113\":2,\"115\":2,\"131\":1,\"135\":1,\"148\":1,\"150\":2,\"155\":1}}],[\"starts\",{\"0\":{\"80\":1,\"83\":1},\"1\":{\"80\":1,\"84\":2,\"85\":1}}],[\"starting\",{\"1\":{\"41\":1,\"48\":2}}],[\"start\",{\"1\":{\"10\":1}}],[\"space\",{\"1\":{\"13\":2,\"19\":3}}],[\"s\",{\"0\":{\"152\":1},\"1\":{\"9\":1,\"10\":1,\"11\":1,\"13\":5,\"20\":5,\"26\":4,\"27\":1,\"40\":2,\"41\":1,\"42\":1,\"44\":10,\"45\":7,\"46\":4,\"48\":13,\"49\":4,\"53\":3,\"55\":4,\"57\":2,\"63\":11,\"66\":13,\"75\":3,\"77\":9,\"78\":8,\"79\":4,\"82\":1,\"84\":6,\"87\":6,\"112\":2,\"113\":7,\"115\":2,\"116\":3,\"119\":1,\"120\":3,\"131\":2,\"132\":2,\"133\":3,\"134\":4,\"135\":8,\"136\":7,\"141\":5,\"142\":28,\"150\":6,\"152\":1,\"153\":8,\"154\":2,\"155\":12,\"156\":4}}],[\"q^​\",{\"1\":{\"142\":5}}],[\"qt​\",{\"1\":{\"116\":2,\"120\":1,\"156\":1}}],[\"qt+1​\",{\"1\":{\"116\":2,\"120\":2}}],[\"qπk​​\",{\"1\":{\"66\":1,\"77\":2,\"78\":2,\"79\":1}}],[\"qπ​\",{\"1\":{\"26\":2,\"48\":3,\"49\":1,\"75\":1,\"115\":1,\"155\":8,\"156\":4}}],[\"qac\",{\"0\":{\"24\":1},\"1\":{\"25\":1}}],[\"q\",{\"0\":{\"120\":1,\"124\":1,\"140\":1,\"141\":1},\"1\":{\"5\":2,\"11\":1,\"13\":2,\"14\":1,\"23\":1,\"49\":1,\"55\":1,\"57\":1,\"58\":1,\"77\":1,\"79\":1,\"81\":2,\"119\":2,\"120\":1,\"123\":1,\"124\":1,\"130\":1,\"141\":1}}],[\"qoe\",{\"1\":{\"4\":1}}],[\"quality\",{\"0\":{\"10\":1},\"1\":{\"4\":1}}],[\"uses\",{\"1\":{\"82\":1}}],[\"users\",{\"1\":{\"4\":2}}],[\"u0​pu​π1\",{\"1\":{\"70\":1}}],[\"uniform\",{\"0\":{\"134\":1}}],[\"uniqueness\",{\"1\":{\"58\":1}}],[\"unmanned\",{\"1\":{\"4\":1}}],[\"update\",{\"1\":{\"23\":1,\"63\":3,\"70\":2}}],[\"uavn\",{\"1\":{\"13\":1}}],[\"uavs\",{\"1\":{\"4\":1,\"5\":1}}],[\"uav\",{\"0\":{\"3\":1,\"186\":1},\"1\":{\"4\":1},\"2\":{\"16\":1}}],[\"out\",{\"1\":{\"161\":1,\"162\":1,\"166\":3,\"170\":2,\"174\":1}}],[\"objective\",{\"1\":{\"134\":2,\"135\":1,\"141\":1,\"142\":1,\"148\":1}}],[\"obejctive\",{\"0\":{\"133\":1},\"1\":{\"136\":1}}],[\"obtained\",{\"1\":{\"41\":1}}],[\"other\",{\"1\":{\"53\":1}}],[\"optimization\",{\"0\":{\"136\":1},\"1\":{\"92\":1}}],[\"optimality\",{\"0\":{\"54\":1},\"1\":{\"52\":1,\"119\":1}}],[\"optimal\",{\"0\":{\"53\":1,\"119\":1},\"1\":{\"52\":2,\"53\":1,\"120\":1}}],[\"opinion\",{\"1\":{\"5\":1}}],[\"originalepisode\",{\"1\":{\"81\":1}}],[\"or\",{\"1\":{\"19\":1,\"40\":1,\"92\":1,\"112\":1,\"135\":1}}],[\"off\",{\"0\":{\"29\":1,\"31\":1,\"121\":1,\"123\":1,\"125\":1},\"1\":{\"123\":1,\"124\":1}}],[\"of\",{\"0\":{\"10\":1,\"43\":1,\"44\":1,\"111\":1,\"115\":1,\"119\":1,\"132\":1},\"1\":{\"4\":3,\"10\":1,\"19\":4,\"20\":3,\"40\":1,\"41\":1,\"45\":2,\"58\":1,\"75\":1,\"81\":1,\"82\":1,\"92\":1,\"99\":1,\"119\":1,\"135\":1,\"142\":1}}],[\"on\",{\"0\":{\"121\":1,\"122\":1,\"126\":1},\"1\":{\"3\":1,\"123\":1,\"124\":1,\"140\":1,\"144\":1},\"2\":{\"16\":1}}],[\"finding\",{\"1\":{\"92\":1,\"104\":1,\"110\":1}}],[\"first\",{\"1\":{\"81\":1}}],[\"fix\",{\"1\":{\"58\":1}}],[\"fast\",{\"1\":{\"58\":1}}],[\"fair\",{\"1\":{\"10\":1}}],[\"f\",{\"1\":{\"58\":4,\"101\":3,\"104\":1}}],[\"fundamental\",{\"1\":{\"52\":1}}],[\"function\",{\"0\":{\"130\":1,\"133\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1},\"1\":{\"40\":1,\"58\":1,\"130\":1,\"134\":2,\"135\":1,\"136\":1,\"141\":1,\"142\":1,\"147\":1,\"148\":1}}],[\"future\",{\"0\":{\"44\":1},\"1\":{\"45\":1}}],[\"free\",{\"0\":{\"75\":1},\"1\":{\"45\":1,\"76\":1,\"77\":1}}],[\"from\",{\"1\":{\"41\":1,\"48\":2}}],[\"framework\",{\"1\":{\"4\":1}}],[\"found\",{\"1\":{\"184\":1}}],[\"foem\",{\"1\":{\"55\":1}}],[\"following\",{\"1\":{\"19\":1}}],[\"forallw\",{\"1\":{\"98\":1}}],[\"foralls∈s\",{\"1\":{\"79\":1}}],[\"fortheother∣a\",{\"1\":{\"87\":1}}],[\"formulation\",{\"0\":{\"105\":1}}],[\"form\",{\"0\":{\"46\":1},\"1\":{\"46\":1,\"47\":1,\"49\":2,\"55\":1,\"58\":1,\"63\":1,\"66\":2}}],[\"for\",{\"1\":{\"4\":2,\"20\":1,\"53\":2,\"58\":1}}],[\"fs\",{\"1\":{\"10\":1}}],[\"fs​+l\",{\"1\":{\"10\":1}}],[\"fc​是载波频率\",{\"1\":{\"9\":1}}],[\"not\",{\"1\":{\"184\":1}}],[\"novel\",{\"1\":{\"4\":1}}],[\"null\",{\"1\":{\"161\":1}}],[\"numbers\",{\"1\":{\"75\":1}}],[\"number\",{\"1\":{\"10\":1}}],[\"name\",{\"1\":{\"161\":3,\"162\":2,\"164\":7,\"166\":14,\"170\":1,\"174\":1}}],[\"n→∞lim​n1​e\",{\"1\":{\"154\":2}}],[\"new\",{\"1\":{\"161\":3,\"162\":1,\"171\":1,\"173\":1}}],[\"newestimatevt+1​\",{\"1\":{\"112\":1}}],[\"network\",{\"1\":{\"142\":5,\"144\":1}}],[\"networks\",{\"0\":{\"3\":1},\"1\":{\"4\":1,\"142\":1}}],[\"n=0∑∞​βnrt+n​\",{\"1\":{\"13\":1}}],[\"n∈n=\",{\"1\":{\"13\":1}}],[\"n∈1\",{\"1\":{\"8\":1}}],[\"n0​为用户所在位置的加性高斯白噪声\",{\"1\":{\"9\":1}}],[\"n\",{\"0\":{\"117\":1},\"1\":{\"8\":2,\"11\":1,\"13\":2,\"70\":1,\"71\":1,\"142\":1}}],[\"t=0∑∞​γtrt+1​\",{\"1\":{\"151\":1}}],[\"t=0\",{\"1\":{\"112\":1,\"116\":1}}],[\"td\",{\"0\":{\"111\":1,\"114\":1,\"115\":1,\"119\":1,\"127\":1,\"138\":1},\"1\":{\"113\":1,\"138\":1,\"142\":1,\"156\":1}}],[\"t∈rn\",{\"1\":{\"46\":2}}],[\"t∈r2×1\",{\"1\":{\"8\":2}}],[\"tabular\",{\"1\":{\"130\":1}}],[\"table过大\",{\"1\":{\"14\":1}}],[\"table来找出对应q\",{\"1\":{\"13\":1}}],[\"table\",{\"1\":{\"13\":2}}],[\"table管理\",{\"1\":{\"13\":1}}],[\"target\",{\"1\":{\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"125\":2,\"142\":4}}],[\"take\",{\"1\":{\"86\":1}}],[\"taking\",{\"1\":{\"48\":1}}],[\"tasks转换成continuing\",{\"1\":{\"19\":1}}],[\"tasks\",{\"1\":{\"19\":3}}],[\"test这个包中\",{\"1\":{\"173\":1}}],[\"test\",{\"1\":{\"170\":1,\"173\":7,\"174\":4}}],[\"terms\",{\"1\":{\"119\":1}}],[\"terminal\",{\"1\":{\"19\":1}}],[\"temporal\",{\"0\":{\"109\":1}}],[\"technology\",{\"1\":{\"3\":1},\"2\":{\"16\":1}}],[\"time\",{\"1\":{\"10\":1,\"81\":1}}],[\"truncated\",{\"0\":{\"69\":1,\"71\":1,\"72\":1},\"1\":{\"71\":1}}],[\"trial\",{\"1\":{\"19\":2}}],[\"trip\",{\"1\":{\"10\":1}}],[\"trajectory以及对应的\",{\"1\":{\"154\":1}}],[\"trajectory是在策略给定下\",{\"1\":{\"19\":1}}],[\"trajectory\",{\"1\":{\"19\":4,\"41\":1,\"42\":1,\"154\":1}}],[\"transition\",{\"1\":{\"19\":2,\"20\":1}}],[\"transmission\",{\"1\":{\"8\":1}}],[\"transactions\",{\"1\":{\"3\":1},\"2\":{\"16\":1}}],[\"t\",{\"1\":{\"8\":11,\"9\":20,\"10\":12,\"11\":8,\"13\":6}}],[\"tool\",{\"1\":{\"52\":1}}],[\"to\",{\"0\":{\"47\":1},\"1\":{\"4\":1,\"19\":1,\"20\":1,\"82\":1,\"86\":1,\"92\":1,\"98\":1}}],[\"this\",{\"0\":{\"164\":1},\"1\":{\"154\":1,\"164\":1,\"166\":12}}],[\"that\",{\"1\":{\"41\":1,\"58\":1,\"81\":1}}],[\"then\",{\"1\":{\"58\":2}}],[\"theorem来求解贝尔曼最优公式\",{\"1\":{\"58\":1}}],[\"theorem\",{\"1\":{\"53\":2,\"58\":1,\"62\":1,\"98\":1}}],[\"the\",{\"0\":{\"24\":1,\"43\":1,\"44\":1},\"1\":{\"4\":1,\"9\":1,\"10\":1,\"19\":6,\"20\":4,\"37\":1,\"40\":1,\"41\":2,\"47\":1,\"48\":4,\"49\":2,\"52\":1,\"53\":2,\"58\":2,\"81\":1,\"82\":2,\"86\":1,\"98\":2,\"135\":1,\"142\":2}}],[\"throughput\",{\"1\":{\"4\":1}}],[\"identically\",{\"1\":{\"75\":1}}],[\"idle\",{\"1\":{\"10\":1}}],[\"iid\",{\"1\":{\"75\":1}}],[\"i\",{\"1\":{\"75\":2,\"78\":1}}],[\"i−γpπk​​\",{\"1\":{\"68\":1}}],[\"i−γpπ​\",{\"1\":{\"47\":2}}],[\"if\",{\"1\":{\"53\":1,\"58\":1,\"86\":1,\"98\":1}}],[\"ij​=pπ​\",{\"1\":{\"46\":1}}],[\"import\",{\"1\":{\"173\":3,\"174\":1}}],[\"importance\",{\"0\":{\"30\":1}}],[\"improvement\",{\"1\":{\"66\":1,\"70\":1,\"79\":1,\"82\":1,\"88\":1,\"111\":1,\"115\":1,\"120\":1}}],[\"immediate\",{\"0\":{\"43\":1},\"1\":{\"45\":1}}],[\"i=1n​\",{\"1\":{\"19\":2}}],[\"iteration\",{\"0\":{\"62\":1,\"65\":1,\"69\":1,\"70\":2,\"71\":1,\"72\":1},\"1\":{\"62\":1,\"68\":2,\"69\":2,\"70\":10,\"71\":3,\"76\":1,\"77\":1,\"79\":1,\"82\":1}}],[\"iterative\",{\"1\":{\"47\":1,\"92\":1}}],[\"it\",{\"1\":{\"4\":1,\"81\":1}}],[\"is\",{\"1\":{\"4\":3,\"19\":1,\"20\":2,\"41\":1,\"53\":1,\"58\":2,\"81\":1,\"86\":2,\"154\":1}}],[\"ieee\",{\"1\":{\"3\":1},\"2\":{\"16\":1}}],[\"info\",{\"1\":{\"170\":3}}],[\"int\",{\"1\":{\"161\":2,\"162\":1,\"165\":3,\"166\":7,\"170\":1,\"174\":1}}],[\"interacting\",{\"1\":{\"19\":1}}],[\"instance\",{\"1\":{\"161\":1}}],[\"independent\",{\"1\":{\"75\":1}}],[\"invoked\",{\"1\":{\"4\":1}}],[\"in\",{\"0\":{\"3\":1},\"1\":{\"4\":1,\"81\":1,\"98\":1,\"119\":1}}],[\"mini\",{\"1\":{\"142\":2}}],[\"minisize\",{\"1\":{\"136\":1}}],[\"minimize\",{\"1\":{\"104\":1}}],[\"mbgd\",{\"0\":{\"106\":1}}],[\"metrics\",{\"1\":{\"148\":2}}],[\"method\",{\"1\":{\"81\":2,\"101\":3,\"156\":1}}],[\"methods\",{\"1\":{\"81\":1}}],[\"means\",{\"1\":{\"92\":1}}],[\"means来划分各个无人机所管理的用户簇\",{\"1\":{\"13\":1}}],[\"means的优化目标是最小化无人机与对应集群用户的欧氏距离\",{\"1\":{\"13\":1}}],[\"means可以视为获得无人机部署的低复杂度方案\",{\"1\":{\"13\":1}}],[\"means算法\",{\"1\":{\"13\":1}}],[\"means和igk算法比具有较低的复杂度\",{\"1\":{\"5\":1}}],[\"mean算法获得初始单元划分\",{\"1\":{\"5\":1}}],[\"mean\",{\"0\":{\"43\":1,\"44\":1,\"99\":1,\"103\":1},\"1\":{\"5\":1,\"40\":1,\"41\":1,\"75\":1,\"77\":1,\"82\":1,\"92\":1}}],[\"mc\",{\"0\":{\"76\":1,\"80\":1,\"83\":1,\"85\":1,\"88\":1,\"114\":1},\"1\":{\"76\":1,\"80\":2,\"81\":1,\"82\":1,\"88\":2,\"101\":1,\"123\":1}}],[\"mdp就变为mp\",{\"1\":{\"20\":1}}],[\"mdp\",{\"0\":{\"20\":1}}],[\"main\",{\"1\":{\"142\":4,\"144\":1,\"161\":3,\"162\":1,\"170\":1,\"173\":4,\"174\":2}}],[\"maxa∈a\",{\"1\":{\"142\":1}}],[\"max​q^​\",{\"1\":{\"140\":1,\"141\":1,\"142\":3}}],[\"mapping\",{\"1\":{\"53\":2,\"58\":5,\"62\":1}}],[\"matrix\",{\"0\":{\"46\":1},\"1\":{\"46\":1,\"49\":1,\"55\":1}}],[\"markov\",{\"0\":{\"20\":1},\"1\":{\"20\":2,\"135\":1}}],[\"markovian\",{\"1\":{\"14\":1}}],[\"may\",{\"1\":{\"19\":1}}],[\"mssrkn​​rtt​+1\",{\"1\":{\"10\":1}}],[\"mss\",{\"1\":{\"10\":1}}],[\"m\",{\"1\":{\"9\":1}}],[\"monro\",{\"1\":{\"98\":2}}],[\"monto\",{\"0\":{\"95\":1}}],[\"monte\",{\"0\":{\"75\":1,\"137\":1},\"1\":{\"75\":1,\"156\":1}}],[\"moreover\",{\"1\":{\"58\":1}}],[\"mobility\",{\"1\":{\"14\":1}}],[\"modles可选择\",{\"1\":{\"14\":1}}],[\"model|environment\",{\"1\":{\"45\":1}}],[\"model\",{\"0\":{\"10\":1,\"75\":1},\"1\":{\"14\":3,\"45\":2,\"76\":1,\"77\":2}}],[\"mos主要是有关传输速率rkn​​的函数\",{\"1\":{\"13\":1}}],[\"mosrkn​​​=t=0∑ts​​moskn​​\",{\"1\":{\"10\":1}}],[\"moskn​​\",{\"1\":{\"10\":3}}],[\"mos\",{\"1\":{\"5\":1}}],[\"movement\",{\"0\":{\"3\":1},\"1\":{\"4\":2}}],[\"multiple\",{\"0\":{\"3\":1},\"1\":{\"4\":1}}],[\"args\",{\"1\":{\"161\":3,\"162\":1,\"170\":1,\"173\":2,\"174\":1}}],[\"age已经初始化完\",{\"1\":{\"166\":1}}],[\"age\",{\"1\":{\"161\":1,\"162\":2,\"166\":18,\"170\":1,\"174\":1}}],[\"agent从一个状态出发\",{\"1\":{\"48\":1}}],[\"agent从一个状态出发可以得到的平均return\",{\"1\":{\"48\":1}}],[\"agent可能走出的全部轨迹\",{\"1\":{\"19\":1}}],[\"agent将获得负奖励\",{\"1\":{\"13\":1}}],[\"agent\",{\"1\":{\"13\":1,\"19\":2,\"48\":2,\"154\":1}}],[\"a∈a∑​π\",{\"1\":{\"155\":1}}],[\"a∈a∑​▽θ​π\",{\"1\":{\"155\":2}}],[\"ak​=k1​是满足上面三个条件的\",{\"1\":{\"98\":1}}],[\"ak​→0不要过快\",{\"1\":{\"98\":1}}],[\"ak​→0\",{\"1\":{\"98\":1}}],[\"ak​\",{\"1\":{\"97\":1}}],[\"ak∗​∣s\",{\"1\":{\"79\":1}}],[\"ak∗​\",{\"1\":{\"66\":1}}],[\"a3​\",{\"1\":{\"81\":2}}],[\"a4​\",{\"1\":{\"81\":2}}],[\"a2​\",{\"1\":{\"81\":4}}],[\"a2c\",{\"0\":{\"25\":1}}],[\"approximation\",{\"0\":{\"130\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1},\"1\":{\"92\":1,\"147\":1}}],[\"approximate\",{\"1\":{\"82\":1}}],[\"approach\",{\"1\":{\"14\":2}}],[\"appears\",{\"1\":{\"81\":1}}],[\"a=ak∗​\",{\"1\":{\"63\":1,\"66\":1}}],[\"as\",{\"1\":{\"58\":1,\"104\":1}}],[\"associate\",{\"1\":{\"20\":1}}],[\"assisted\",{\"1\":{\"4\":1}}],[\"along\",{\"1\":{\"154\":1}}],[\"alogorithm\",{\"0\":{\"132\":1}}],[\"algorithms\",{\"0\":{\"136\":1},\"1\":{\"92\":1}}],[\"algorithm\",{\"0\":{\"62\":1,\"65\":1,\"69\":1,\"71\":1,\"72\":1,\"95\":1},\"1\":{\"58\":1,\"68\":2,\"70\":2,\"71\":1,\"76\":1,\"79\":1,\"98\":1}}],[\"all\",{\"1\":{\"41\":1,\"53\":1}}],[\"average\",{\"0\":{\"150\":1,\"153\":1},\"1\":{\"48\":2,\"154\":1}}],[\"a∑​π\",{\"1\":{\"45\":1,\"155\":1}}],[\"a∑​p\",{\"1\":{\"44\":1}}],[\"a∼π​\",{\"1\":{\"26\":3,\"155\":1}}],[\"advantage\",{\"0\":{\"25\":1}}],[\"a∣s\",{\"1\":{\"20\":1,\"26\":3,\"41\":1,\"43\":2,\"44\":5,\"45\":4,\"48\":3,\"49\":1,\"55\":2,\"57\":2,\"58\":1,\"63\":2,\"66\":3,\"79\":1,\"87\":1,\"113\":1,\"148\":1,\"153\":1,\"155\":14,\"156\":2}}],[\"any\",{\"1\":{\"53\":1,\"58\":1,\"86\":1}}],[\"an\",{\"1\":{\"19\":1,\"47\":1,\"48\":1}}],[\"and\",{\"0\":{\"3\":1},\"1\":{\"4\":2,\"14\":1,\"48\":1,\"49\":1,\"52\":1,\"53\":1,\"75\":1,\"142\":1}}],[\"at+1​\",{\"1\":{\"116\":2,\"139\":1,\"140\":1}}],[\"at\",{\"1\":{\"19\":1,\"20\":1}}],[\"at​∣st​\",{\"1\":{\"156\":1}}],[\"at​=a\",{\"1\":{\"43\":1,\"44\":2,\"48\":2,\"49\":1,\"75\":1,\"77\":1,\"78\":1,\"119\":1}}],[\"at​\",{\"1\":{\"13\":1,\"116\":8,\"120\":5,\"139\":2,\"140\":2,\"156\":1}}],[\"a1​\",{\"1\":{\"19\":2,\"20\":2,\"81\":1}}],[\"ai​\",{\"1\":{\"19\":1,\"142\":2}}],[\"actor\",{\"0\":{\"23\":1,\"24\":1,\"25\":1,\"29\":1,\"33\":1},\"1\":{\"23\":2}}],[\"actions\",{\"1\":{\"20\":1}}],[\"action\",{\"0\":{\"48\":1,\"115\":1,\"119\":1},\"1\":{\"13\":1,\"19\":3,\"20\":2,\"48\":8,\"49\":2,\"53\":2,\"57\":2,\"63\":1,\"66\":1,\"70\":2,\"75\":1,\"77\":5,\"79\":6,\"81\":9,\"82\":3,\"84\":4,\"86\":2,\"92\":1,\"115\":4,\"116\":1,\"119\":2,\"120\":1,\"131\":1}}],[\"academic\",{\"0\":{\"187\":1},\"2\":{\"15\":1,\"21\":1,\"34\":1,\"50\":1,\"59\":1,\"73\":1,\"90\":1,\"107\":1,\"128\":1,\"145\":1,\"158\":1}}],[\"awgn\",{\"1\":{\"9\":1}}],[\"aerial\",{\"1\":{\"4\":1}}],[\"a\",{\"1\":{\"4\":1,\"13\":3,\"14\":2,\"19\":7,\"20\":5,\"26\":2,\"41\":3,\"43\":1,\"44\":3,\"45\":6,\"48\":13,\"49\":2,\"52\":1,\"53\":1,\"55\":3,\"57\":1,\"58\":2,\"63\":6,\"66\":6,\"70\":1,\"75\":3,\"77\":11,\"78\":8,\"79\":3,\"81\":2,\"82\":2,\"84\":6,\"86\":1,\"92\":1,\"113\":1,\"115\":4,\"116\":3,\"119\":3,\"120\":4,\"135\":1,\"141\":3,\"142\":22,\"153\":5,\"155\":9,\"156\":4,\"161\":1,\"165\":4}}],[\"a+b=c\",{\"1\":{\"0\":1}}],[\"double\",{\"1\":{\"165\":3}}],[\"down\",{\"1\":{\"8\":1}}],[\"d0​\",{\"1\":{\"152\":1}}],[\"dqn\",{\"0\":{\"141\":1},\"1\":{\"142\":1,\"144\":1}}],[\"dπ​\",{\"1\":{\"135\":2,\"152\":1}}],[\"difference\",{\"0\":{\"109\":1}}],[\"distributon\",{\"0\":{\"134\":1},\"1\":{\"135\":3}}],[\"distributed\",{\"1\":{\"75\":1}}],[\"distribution\",{\"0\":{\"135\":1},\"1\":{\"20\":1,\"133\":1,\"135\":1,\"152\":1}}],[\"discounted\",{\"1\":{\"19\":1,\"42\":1,\"113\":1,\"137\":1}}],[\"data\",{\"1\":{\"81\":1}}],[\"daily\",{\"0\":{\"185\":1},\"2\":{\"1\":1}}],[\"daily1\",{\"0\":{\"0\":1}}],[\"dpg\",{\"0\":{\"33\":1}}],[\"d\",{\"0\":{\"152\":1},\"1\":{\"10\":3,\"75\":1,\"150\":2,\"152\":5}}],[\"dkn​​\",{\"1\":{\"9\":1}}],[\"dkn​​=hn2​\",{\"1\":{\"8\":1}}],[\"dkn​\",{\"1\":{\"9\":1}}],[\"dynamic\",{\"1\":{\"4\":1,\"45\":1}}],[\"driven\",{\"1\":{\"4\":1}}],[\"deep\",{\"0\":{\"141\":1},\"1\":{\"141\":1}}],[\"describes\",{\"1\":{\"135\":1}}],[\"descent\",{\"0\":{\"100\":1},\"1\":{\"101\":2}}],[\"design\",{\"0\":{\"3\":1}}],[\"decision\",{\"0\":{\"20\":1}}],[\"deterministic\",{\"0\":{\"33\":1,\"105\":1},\"1\":{\"14\":1,\"19\":1,\"53\":2}}],[\"demonstrating\",{\"1\":{\"4\":1}}],[\"deployment\",{\"0\":{\"3\":1},\"1\":{\"4\":1}}],[\"d1\",{\"2\":{\"2\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(et(t,v[s],n)):e==="search"?self.postMessage(tt(t,v[s],n)):self.postMessage({suggestions:et(t,v[s],n),results:tt(t,v[s],n)})};
//# sourceMappingURL=index.js.map
